@article{Menabrea2015SketchOT,
  title={Sketch of the Analytical Engine invented by Charles Babbage, Esq.},
  author={Lady Lovelace},
  journal={Ada's Legacy: Cultures of Computing from the Victorian to the Digital Age},
  year={1843},
  url={https://api.semanticscholar.org/CorpusID:257837181}
}

@inproceedings{babbage1864,
  title={Analytical engine},
  author={Charles Babbage},
  year={1837},
  url={https://api.semanticscholar.org/CorpusID:63432444}
}

@inproceedings{Newell1959ThePO,
  title={The Processes of Creative Thinking},
  author={Allen Newell and J. C. Shaw and Herbert A. Simon},
  year={1959},
  url={https://api.semanticscholar.org/CorpusID:121728459}
}

@article{turing1950,
    author = {Turing, A. M.},
    title = "{I.—COMPUTING MACHINERY AND INTELLIGENCE}",
    journal = {Mind},
    volume = {LIX},
    number = {236},
    pages = {433-460},
    year = {1950},
    month = {10},
    issn = {0026-4423},
    doi = {10.1093/mind/LIX.236.433},
    url = {https://doi.org/10.1093/mind/LIX.236.433},
    eprint = {https://academic.oup.com/mind/article-pdf/LIX/236/433/30123314/lix-236-433.pdf},
}

@misc{wei2022emergent,
      title={Emergent Abilities of Large Language Models}, 
      author={Jason Wei and Yi Tay and Rishi Bommasani and Colin Raffel and Barret Zoph and Sebastian Borgeaud and Dani Yogatama and Maarten Bosma and Denny Zhou and Donald Metzler and Ed H. Chi and Tatsunori Hashimoto and Oriol Vinyals and Percy Liang and Jeff Dean and William Fedus},
      year={2022},
      eprint={2206.07682},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{Meehan1977TALESPINAI,
  title={TALE-SPIN, An Interactive Program that Writes Stories},
  author={James R. Meehan},
  booktitle={International Joint Conference on Artificial Intelligence},
  year={1977},
  url={https://api.semanticscholar.org/CorpusID:2372981}
}

@inproceedings{Racter1984ThePB,
  title={The Policeman's Beard Is Half Constructed},
  author={Racter},
  year={1984},
  url={https://api.semanticscholar.org/CorpusID:146585648}
}

@inproceedings{paul-frank-2021-coins,
    title = "{COINS}: Dynamically Generating {CO}ntextualized Inference Rules for Narrative Story Completion",
    author = "Paul, Debjit  and
      Frank, Anette",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.395",
    doi = "10.18653/v1/2021.acl-long.395",
    pages = "5086--5099",
}

@inproceedings{ravi-etal-2024-small,
    title = "Small But Funny: A Feedback-Driven Approach to Humor Distillation",
    author = "Ravi, Sahithya  and
      Huber, Patrick  and
      Shrivastava, Akshat  and
      Shwartz, Vered  and
      Einolghozati, Arash",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.706",
    pages = "13078--13090"
}

@inproceedings{Lebowitz1983CreatingAS,
  title={Creating a Story-Telling Universe},
  author={Michael Lebowitz},
  booktitle={International Joint Conference on Artificial Intelligence},
  year={1983},
  url={https://api.semanticscholar.org/CorpusID:37655080}
}

@book{masterman1971,
    author = {Margaret Masterman},
    title = {'Computerized haiku', in Cybernetics, art and ideas},
    publisher = {London Studio Vista},
    year = {1971}
}

@article{Prez2001MEXICAAC,
  title={MEXICA: A computer model of a cognitive account of creative writing},
  author={Rafael P{\'e}rez y P{\'e}rez and Mike Sharples},
  journal={Journal of Experimental \& Theoretical Artificial Intelligence},
  year={2001},
  volume={13},
  pages={119 - 139},
  url={https://api.semanticscholar.org/CorpusID:18676334}
}

@inproceedings{Turner1994TheCP,
  title={The Creative Process: A Computer Model of Storytelling and Creativity},
  author={Scott R. Turner},
  year={1994},
  url={https://api.semanticscholar.org/CorpusID:260575817}
}

@inproceedings{Vaswani2017AttentionIA,
  title={Attention is All you Need},
  author={Ashish Vaswani and Noam M. Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  booktitle={Neural Information Processing Systems},
  year={2017},
  url={https://api.semanticscholar.org/CorpusID:13756489}
}

@misc{geminiteam2024gemini,
      title={Gemini: A Family of Highly Capable Multimodal Models}, 
      author={Team Gemini},
      year={2024},
      eprint={2312.11805},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{zhao2023survey,
      title={A Survey of Large Language Models}, 
      author={Wayne Xin Zhao and Kun Zhou and Junyi Li and Tianyi Tang and Xiaolei Wang and Yupeng Hou and Yingqian Min and Beichen Zhang and Junjie Zhang and Zican Dong and Yifan Du and Chen Yang and Yushuo Chen and Zhipeng Chen and Jinhao Jiang and Ruiyang Ren and Yifan Li and Xinyu Tang and Zikang Liu and Peiyu Liu and Jian-Yun Nie and Ji-Rong Wen},
      year={2023},
      eprint={2303.18223},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{bubeck2023sparks,
      title={Sparks of Artificial General Intelligence: Early experiments with GPT-4}, 
      author={Sébastien Bubeck and Varun Chandrasekaran and Ronen Eldan and Johannes Gehrke and Eric Horvitz and Ece Kamar and Peter Lee and Yin Tat Lee and Yuanzhi Li and Scott Lundberg and Harsha Nori and Hamid Palangi and Marco Tulio Ribeiro and Yi Zhang},
      year={2023},
      eprint={2303.12712},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{ho2020denoising,
      title={Denoising Diffusion Probabilistic Models}, 
      author={Jonathan Ho and Ajay Jain and Pieter Abbeel},
      year={2020},
      eprint={2006.11239},
      archivePrefix={arXiv}
}

@book{guilford1967nature,
  title={The Nature of Human Intelligence},
  author={Guilford, J.P.},
  isbn={9780070251359},
  lccn={lc67011207},
  series={McGraw-Hill series in psychology},
  url={https://books.google.ch/books?id=T-ZJAAAAMAAJ},
  year={1967},
  publisher={McGraw-Hill}
}

@book{boden2004creative,
  title={The Creative Mind: Myths and Mechanisms},
  author={Boden, M.A.},
  isbn={9780415314527},
  lccn={2003046533},
  url={https://books.google.ch/books?id=6Zkm4dz32Y4C},
  year={2004},
  publisher={Routledge}
}

@inproceedings{yang-etal-2022-re3,
    title = "Re3: Generating Longer Stories With Recursive Reprompting and Revision",
    author = "Yang, Kevin  and
      Tian, Yuandong  and
      Peng, Nanyun  and
      Klein, Dan",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.296",
    doi = "10.18653/v1/2022.emnlp-main.296",
    pages = "4393--4479",
    abstract = "We consider the problem of automatically generating longer stories of over two thousand words. Compared to prior work on shorter stories, long-range plot coherence and relevance are more central challenges here. We propose the Recursive Reprompting and Revision framework (Re3) to address these challenges by (a) prompting a general-purpose language model to construct a structured overarching plan, and (b) generating story passages by repeatedly injecting contextual information from both the plan and current story state into a language model prompt. We then revise by (c) reranking different continuations for plot coherence and premise relevance, and finally (d) editing the best continuation for factual consistency. Compared to similar-length stories generated directly from the same base model, human evaluators judged substantially more of Re3{'}s stories as having a coherent overarching plot (by 14{\%} absolute increase), and relevant to the given initial premise (by 20{\%}).",
}

@inproceedings{Ormazabal2022PoeLMAM,
  title={PoeLM: A Meter- and Rhyme-Controllable Language Model for Unsupervised Poetry Generation},
  author={Aitor Ormazabal and Mikel Artetxe and Manex Agirrezabal and Aitor Soroa Etxabe and Eneko Agirre},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2022},
  url={https://api.semanticscholar.org/CorpusID:249018166}
}

@article{Chakrabarty2022HelpMW,
  title={Help me write a Poem - Instruction Tuning as a Vehicle for Collaborative Poetry Writing},
  author={Tuhin Chakrabarty and Vishakh Padmakumar and Hengxing He},
  journal={ArXiv},
  year={2022},
  volume={abs/2210.13669},
  url={https://api.semanticscholar.org/CorpusID:253107865}
}

@article{Dhariwal2020JukeboxAG,
  title={Jukebox: A Generative Model for Music},
  author={Prafulla Dhariwal and Heewoo Jun and Christine Payne and Jong Wook Kim and Alec Radford and Ilya Sutskever},
  journal={ArXiv},
  year={2020},
  volume={abs/2005.00341},
  url={https://api.semanticscholar.org/CorpusID:218470180}
}

@inproceedings{BetkerImprovingIG,
  title={Improving Image Generation with Better Captions},
  author={James Betker and Gabriel Goh and Li Jing and TimBrooks and Jianfeng Wang and Linjie Li and Long Ouyang and Juntang Zhuang and Joyce Lee and Yufei Guo and Wesam Manassra and Prafulla Dhariwal and Casey Chu and Yunxin Jiao and Aditya Ramesh},
  year={2020},
  url={https://api.semanticscholar.org/CorpusID:264403242}
}

@article{videoworldsimulators2024,
  title={Video generation models as world simulators},
  author={Tim Brooks and Bill Peebles and Connor Holmes and Will DePue and Yufei Guo and Li Jing and David Schnurr and Joe Taylor and Troy Luhman and Eric Luhman and Clarence Ng and Ricky Wang and Aditya Ramesh},
  year={2024},
  url={https://openai.com/research/video-generation-models-as-world-simulators},
}

@article{Brown2020LanguageMA,
  title={Language Models are Few-Shot Learners},
  author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeff Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
  journal={ArXiv},
  year={2020},
  volume={abs/2005.14165},
  url={https://api.semanticscholar.org/CorpusID:218971783}
}

@article{Bender2021OnTD,
  title={On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?},
  author={Emily M. Bender and Timnit Gebru and Angelina McMillan-Major and Shmargaret Shmitchell},
  journal={Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
  year={2021},
  url={https://api.semanticscholar.org/CorpusID:262580630}
}

@article{Chakrabarty2023ArtOA,
  title={Art or Artifice? Large Language Models and the False Promise of Creativity},
  author={Tuhin Chakrabarty and Philippe Laban and Divyansh Agarwal and Smaranda Muresan and Chien-Sheng Wu},
  journal={Proceedings of the CHI Conference on Human Factors in Computing Systems},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:262826094}
}

@article{McCoy2021HowMD,
  title={How Much Do Language Models Copy From Their Training Data? Evaluating Linguistic Novelty in Text Generation Using RAVEN},
  author={R. Thomas McCoy and Paul Smolensky and Tal Linzen and Jianfeng Gao and Asli Celikyilmaz},
  journal={Transactions of the Association for Computational Linguistics},
  year={2021},
  volume={11},
  pages={652-670},
  url={https://api.semanticscholar.org/CorpusID:244345615}
}

@article{Padmakumar2023DoesWW,
  title={Does Writing with Language Models Reduce Content Diversity?},
  author={Vishakh Padmakumar and He He},
  journal={ArXiv},
  year={2023},
  volume={abs/2309.05196},
  url={https://api.semanticscholar.org/CorpusID:261682154}
}

@article{Tian2023MacGyverAL,
  title={MacGyver: Are Large Language Models Creative Problem Solvers?},
  author={Yufei Tian and Abhilasha Ravichander and Lianhui Qin and Ronan Joseph Le Bras and Raja Marjieh and Nanyun Peng and Yejin Choi and Thomas L. Griffiths and Faeze Brahman},
  journal={ArXiv},
  year={2023},
  volume={abs/2311.09682},
  url={https://api.semanticscholar.org/CorpusID:265221054}
}

@article{Dziri2023FaithAF,
  title={Faith and Fate: Limits of Transformers on Compositionality},
  author={Nouha Dziri and Ximing Lu and Melanie Sclar and Xiang Lorraine Li and Liwei Jian and Bill Yuchen Lin and Peter West and Chandra Bhagavatula and Ronan Le Bras and Jena D. Hwang and Soumya Sanyal and Sean Welleck and Xiang Ren and Allyson Ettinger and Zaid Harchaoui and Yejin Choi},
  journal={ArXiv},
  year={2023},
  volume={abs/2305.18654},
  url={https://api.semanticscholar.org/CorpusID:258967391}
}

@article{Ismayilzada2023CRoWBC,
  title={CRoW: Benchmarking Commonsense Reasoning in Real-World Tasks},
  author={Mete Ismayilzada and Debjit Paul and Syrielle Montariol and Mor Geva and Antoine Bosselut},
  journal={ArXiv},
  year={2023},
  volume={abs/2310.15239},
  url={https://api.semanticscholar.org/CorpusID:264438931}
}

@article{Marcus2020TheND,
  title={The Next Decade in AI: Four Steps Towards Robust Artificial Intelligence},
  author={Gary F. Marcus},
  journal={ArXiv},
  year={2020},
  volume={abs/2002.06177},
  url={https://api.semanticscholar.org/CorpusID:211126492}
}

@article{Hochreiter1997LongSM,
  title={Long Short-Term Memory},
  author={Sepp Hochreiter and J{\"u}rgen Schmidhuber},
  journal={Neural Computation},
  year={1997},
  volume={9},
  pages={1735-1780},
  url={https://api.semanticscholar.org/CorpusID:1915014}
}

@inproceedings{Dai2019TransformerXLAL,
  title={Transformer-XL: Attentive Language Models beyond a Fixed-Length Context},
  author={Zihang Dai and Zhilin Yang and Yiming Yang and Jaime G. Carbonell and Quoc V. Le and Ruslan Salakhutdinov},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2019},
  url={https://api.semanticscholar.org/CorpusID:57759363}
}

@inproceedings{Radford2019LanguageMA,
  title={Language Models are Unsupervised Multitask Learners},
  author={Alec Radford and Jeff Wu and Rewon Child and David Luan and Dario Amodei and Ilya Sutskever},
  year={2019},
  url={https://api.semanticscholar.org/CorpusID:160025533}
}

@inproceedings{Bender2020ClimbingTN,
  title={Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data},
  author={Emily M. Bender and Alexander Koller},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2020},
  url={https://api.semanticscholar.org/CorpusID:211029226}
}

@misc{merity2016pointer,
      title={Pointer Sentinel Mixture Models}, 
      author={Stephen Merity and Caiming Xiong and James Bradbury and Richard Socher},
      year={2016},
      eprint={1609.07843},
      archivePrefix={arXiv},
      primaryClass={id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}
}

@article{Sennrich2015NeuralMT,
  title={Neural Machine Translation of Rare Words with Subword Units},
  author={Rico Sennrich and Barry Haddow and Alexandra Birch},
  journal={ArXiv},
  year={2015},
  volume={abs/1508.07909},
  url={https://api.semanticscholar.org/CorpusID:1114678}
}

@inproceedings{Fan2018HierarchicalNS,
  title={Hierarchical Neural Story Generation},
  author={Angela Fan and Mike Lewis and Yann Dauphin},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2018},
  url={https://api.semanticscholar.org/CorpusID:44134226}
}

@article{Holtzman2019TheCC,
  title={The Curious Case of Neural Text Degeneration},
  author={Ari Holtzman and Jan Buys and Li Du and Maxwell Forbes and Yejin Choi},
  journal={ArXiv},
  year={2019},
  volume={abs/1904.09751},
  url={https://api.semanticscholar.org/CorpusID:127986954}
}

@article{Keysers2019MeasuringCG,
  title={Measuring Compositional Generalization: A Comprehensive Method on Realistic Data},
  author={Daniel Keysers and Nathanael Sch{\"a}rli and Nathan Scales and Hylke Buisman and Daniel Furrer and Sergii Kashubin and Nikola Momchev and Danila Sinopalnikov and Lukasz Stafiniak and Tibor Tihon and Dmitry Tsarkov and Xiao Wang and Marc van Zee and Olivier Bousquet},
  journal={ArXiv},
  year={2019},
  volume={abs/1912.09713},
  url={https://api.semanticscholar.org/CorpusID:209439843}
}

@article{Ouyang2022TrainingLM,
  title={Training language models to follow instructions with human feedback},
  author={Long Ouyang and Jeff Wu and Xu Jiang and Diogo Almeida and Carroll L. Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Ray and John Schulman and Jacob Hilton and Fraser Kelton and Luke E. Miller and Maddie Simens and Amanda Askell and Peter Welinder and Paul Francis Christiano and Jan Leike and Ryan J. Lowe},
  journal={ArXiv},
  year={2022},
  volume={abs/2203.02155},
  url={https://api.semanticscholar.org/CorpusID:246426909}
}

@inproceedings{Lin2004ROUGEAP,
  title={ROUGE: A Package for Automatic Evaluation of Summaries},
  author={Chin-Yew Lin},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2004},
  url={https://api.semanticscholar.org/CorpusID:964287}
}

@article{Zhang2019BERTScoreET,
  title={BERTScore: Evaluating Text Generation with BERT},
  author={Tianyi Zhang and Varsha Kishore and Felix Wu and Kilian Q. Weinberger and Yoav Artzi},
  journal={ArXiv},
  year={2019},
  volume={abs/1904.09675},
  url={https://api.semanticscholar.org/CorpusID:127986044}
}

@misc{openai2024gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2024},
      eprint={2303.08774},
      archivePrefix={arXiv}
}

@article{Bai2022TrainingAH,
  title={Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback},
  author={Yuntao Bai and Andy Jones and Kamal Ndousse and Amanda Askell and Anna Chen and Nova Dassarma and Dawn Drain and Stanislav Fort and Deep Ganguli and Tom Henighan and Nicholas Joseph and Saurav Kadavath and John Kernion and Tom Conerly and Sheer El-Showk and Nelson Elhage and Zac Hatfield-Dodds and Danny Hernandez and Tristan Hume and Scott Johnston and Shauna Kravec and Liane Lovitt and Neel Nanda and Catherine Olsson and Dario Amodei and Tom B. Brown and Jack Clark and Sam McCandlish and Christopher Olah and Benjamin Mann and Jared Kaplan},
  journal={ArXiv},
  year={2022},
  volume={abs/2204.05862},
  url={https://api.semanticscholar.org/CorpusID:248118878}
}

@misc{touvron2023llama,
      title={Llama 2: Open Foundation and Fine-Tuned Chat Models}, 
      author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
      year={2023},
      eprint={2307.09288},
      archivePrefix={arXiv},
      primaryClass={id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}
}

@article{Anil2023PaLM2T,
  title={PaLM 2 Technical Report},
  author={Rohan Anil and Andrew M. Dai and Orhan Firat and Melvin Johnson and Dmitry Lepikhin and Alexandre Tachard Passos and Siamak Shakeri and Emanuel Taropa and Paige Bailey and Z. Chen and Eric Chu and J. Clark and Laurent El Shafey and Yanping Huang and Kathleen S. Meier-Hellstern and Gaurav Mishra and Erica Moreira and Mark Omernick and Kevin Robinson and Sebastian Ruder and Yi Tay and Kefan Xiao and Yuanzhong Xu and Yujing Zhang and Gustavo Hern{\'a}ndez Abrego and Junwhan Ahn and Jacob Austin and Paul Barham and Jan A. Botha and James Bradbury and Siddhartha Brahma and Kevin Michael Brooks and Michele Catasta and Yongzhou Cheng and Colin Cherry and Christopher A. Choquette-Choo and Aakanksha Chowdhery and C Cr{\'e}py and Shachi Dave and Mostafa Dehghani and Sunipa Dev and Jacob Devlin and M. C. D'iaz and Nan Du and Ethan Dyer and Vladimir Feinberg and Fan Feng and Vlad Fienber and Markus Freitag and Xavier Garc{\'i}a and Sebastian Gehrmann and Lucas Gonz{\'a}lez and Guy Gur-Ari and Steven Hand and Hadi Hashemi and Le Hou and Joshua Howland and An Ren Hu and Jeffrey Hui and Jeremy Hurwitz and Michael Isard and Abe Ittycheriah and Matthew Jagielski and Wen Hao Jia and Kathleen Kenealy and Maxim Krikun and Sneha Kudugunta and Chang Lan and Katherine Lee and Benjamin Lee and Eric Li and Mu-Li Li and Wei Li and Yaguang Li and Jun Yu Li and Hyeontaek Lim and Han Lin and Zhong-Zhong Liu and Frederick Liu and Marcello Maggioni and Aroma Mahendru and Joshua Maynez and Vedant Misra and Maysam Moussalem and Zachary Nado and John Nham and Eric Ni and Andrew Nystrom and Alicia Parrish and Marie Pellat and Martin Polacek and Oleksandr Polozov and Reiner Pope and Siyuan Qiao and Emily Reif and Bryan Richter and Parker Riley and Alexandra Ros and Aurko Roy and Brennan Saeta and Rajkumar Samuel and Renee Marie Shelby and Ambrose Slone and Daniel Smilkov and David R. So and Daniela Sohn and Simon Tokumine and Dasha Valter and Vijay Vasudevan and Kiran Vodrahalli and Xuezhi Wang and Pidong Wang and Zirui Wang and Tao Wang and John Wieting and Yuhuai Wu and Ke Xu and Yunhan Xu and Lin Wu Xue and Pengcheng Yin and Jiahui Yu and Qiaoling Zhang and Steven Zheng and Ce Zheng and Wei Zhou and Denny Zhou and Slav Petrov and Yonghui Wu},
  journal={ArXiv},
  year={2023},
  volume={abs/2305.10403},
  url={https://api.semanticscholar.org/CorpusID:258740735}
}

@article{claude2023,
    author = {Anthropic},
    title = {Model Card and Evaluations for Claude Models},
    journal = {},
    year = {2023}
}

@misc{wang2024ai,
      title={Can AI Be as Creative as Humans?}, 
      author={Haonan Wang and James Zou and Michael Mozer and Anirudh Goyal and Alex Lamb and Linjun Zhang and Weijie J Su and Zhun Deng and Michael Qizhe Xie and Hannah Brown and Kenji Kawaguchi},
      year={2024},
      eprint={2401.01623},
      archivePrefix={arXiv},
      primaryClass={id='cs.AI' full_name='Artificial Intelligence' is_active=True alt_name=None in_archive='cs' is_general=False description='Covers all areas of AI except Vision, Robotics, Machine Learning, Multiagent Systems, and Computation and Language (Natural Language Processing), which have separate subject areas. In particular, includes Expert Systems, Theorem Proving (although this may overlap with Logic in Computer Science), Knowledge Representation, Planning, and Uncertainty in AI. Roughly includes material in ACM Subject Classes I.2.0, I.2.1, I.2.3, I.2.4, I.2.8, and I.2.11.'}
}

@article{gaut2010,
author = {Gaut, Berys},
title = {The Philosophy of Creativity},
journal = {Philosophy Compass},
volume = {5},
number = {12},
pages = {1034-1046},
doi = {https://doi.org/10.1111/j.1747-9991.2010.00351.x},
url = {https://compass.onlinelibrary.wiley.com/doi/abs/10.1111/j.1747-9991.2010.00351.x},
eprint = {https://compass.onlinelibrary.wiley.com/doi/pdf/10.1111/j.1747-9991.2010.00351.x},
abstract = {Abstract This paper surveys some of the central issues in the philosophy of creativity and argues that an adequate treatment of them requires attention to the rich psychological literature on creativity. It also shows that the range of interesting philosophical questions to be raised about creativity is much wider than concerns its role in art. Issues covered include the definition of ‘creativity’; the relation of creativity to imagination; whether the creative process is rational; whether it is teleological; the relation of creativity to knowledge; whether creativity can be explained; computational and Darwinian theories of creativity; whether creativity is a virtue; the relation of creativity to tradition; the aesthetic value of creativity; and whether creative activity is different in science and art.},
year = {2010}
}

@article{kronfeldner2009,
    author = {Kronfeldner, Maria E.},
    title = "{Creativity Naturalized}",
    journal = {The Philosophical Quarterly},
    volume = {59},
    number = {237},
    pages = {577-592},
    year = {2009},
    month = {07},
    abstract = "{I argue that creativity is compatible with determinism and therefore with naturalistic explanation. I explore different kinds of novelty, corresponding with four distinct concepts of creativity – anthropological, historical, psychological and metaphysical. Psychological creativity incorporates originality and spontaneity. Taken together, these point to the independence of the creative mind from social learning, experience and previously acquired knowledge. This independence is nevertheless compatible with determinism. Creativity is opposed to specific causal factors, but it does not exclude causal determination as such. So creativity can be naturalized.}",
    issn = {0031-8094},
    doi = {10.1111/j.1467-9213.2009.637.x},
    url = {https://doi.org/10.1111/j.1467-9213.2009.637.x},
    eprint = {https://academic.oup.com/pq/article-pdf/59/237/577/4299738/pq59-0577.pdf},
}

@inproceedings{Chakrabarty2023ISA,
  title={I Spy a Metaphor: Large Language Models and Diffusion Models Co-Create Visual Metaphors},
  author={Tuhin Chakrabarty and Arkadiy Saakyan and Olivia Winn and Artemis Panagopoulou and Yue Yang and Marianna Apidianaki and Smaranda Muresan},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:258865878}
}

@misc{hessel2023androids,
      title={Do Androids Laugh at Electric Sheep? Humor "Understanding" Benchmarks from The New Yorker Caption Contest}, 
      author={Jack Hessel and Ana Marasović and Jena D. Hwang and Lillian Lee and Jeff Da and Rowan Zellers and Robert Mankoff and Yejin Choi},
      year={2023},
      eprint={2209.06293},
      archivePrefix={arXiv}
}

@article{Horvitz2024GettingSA,
  title={Getting Serious about Humor: Crafting Humor Datasets with Unfunny Large Language Models},
  author={Zachary Horvitz and Jingru Chen and Rahul Aditya and Harshvardhan Srivastava and Robert West and Zhou Yu and Kathleen McKeown},
  journal={ArXiv},
  year={2024},
  volume={abs/2403.00794},
  url={https://api.semanticscholar.org/CorpusID:268230695}
}

@book{veale2019computational,
  title={Computational Creativity: The Philosophy and Engineering of Autonomously Creative Systems},
  author={Veale, T. and Cardoso, F.A.},
  isbn={9783319436098},
  series={Computational synthesis and creative systems},
  url={https://books.google.ch/books?id=EBY1ygEACAAJ},
  year={2019},
  publisher={Springer}
}

@article{kohn2009,
author = {Kohn, Nicholas and Smith, Steven},
year = {2009},
month = {06},
pages = {102-118},
title = {Partly versus Completely Out of Your Mind: Effects of Incubation and Distraction on Resolving Fixation},
volume = {43},
journal = {The Journal of Creative Behavior},
doi = {10.1002/j.2162-6057.2009.tb01309.x}
}

@article{Wiley1998ExpertiseAM,
  title={Expertise as mental set: The effects of domain knowledge in creative problem solving},
  author={Jennifer Wiley},
  journal={Memory \& Cognition},
  year={1998},
  volume={26},
  pages={716-730},
  url={https://api.semanticscholar.org/CorpusID:45775509}
}

@article{Barber1960RigidityOB,
  title={Rigidity of Behavior. A Variational Approach to the Effect of Einstellung.Abraham S. Luchins , Edith Hirsch Luchins},
  author={Theodore Xenophon Barber},
  journal={The Quarterly Review of Biology},
  year={1960},
  volume={35},
  pages={255-255},
  url={https://api.semanticscholar.org/CorpusID:85022531}
}

@article{smith2013,
author = {Smith, Steven and Blankenship, Steven},
year = {2013},
month = {04},
pages = {311-314},
title = {Incubation Effects},
volume = {27},
journal = {Bulletin of the Psychonomic Society},
doi = {10.3758/BF03334612}
}

@article{Smith1991IncubationAT,
  title={Incubation and the persistence of fixation in problem solving.},
  author={Steven M. Smith and Steven E. Blankenship},
  journal={The American journal of psychology},
  year={1991},
  volume={104 1},
  pages={61-87},
  url={https://api.semanticscholar.org/CorpusID:10359632}
}

@article{Wood1973-WOOEAR,
	author = {Gordon Wood and Joyce Pennington},
	doi = {10.1037/h0034636},
	journal = {Journal of Experimental Psychology},
	number = {2},
	pages = {243},
	title = {Encoding and Retrieval From Long-Term Storage},
	volume = {99},
	year = {1973}
}

@misc{naeini2023large,
      title={Large Language Models are Fixated by Red Herrings: Exploring Creative Problem Solving and Einstellung Effect using the Only Connect Wall Dataset}, 
      author={Saeid Naeini and Raeid Saqur and Mozhgan Saeidi and John Giorgi and Babak Taati},
      year={2023},
      eprint={2306.11167},
      archivePrefix={arXiv}
}

@article{Franceschelli2024CreativeBS,
  title={Creative Beam Search: LLM-as-a-Judge For Improving Response Generation},
  author={Giorgio Franceschelli and Mirco Musolesi},
  journal={ArXiv},
  year={2024},
  volume={abs/2405.00099},
  url={https://api.semanticscholar.org/CorpusID:269484318}
}

@inproceedings{elgammal2017can,
  title={CAN: Creative Adversarial Networks, Generating "Art" by Learning About Styles and Deviating from Style Norms},
  author={A. Elgammal and Bingchen Liu and Mohamed Elhoseiny and Marian Mazzone},
  booktitle={International Conference on Innovative Computing and Cloud Computing},
  year={2017},
  url={https://api.semanticscholar.org/CorpusID:24986117}
}

@book{aleinikov2000creating,
  title={Creating Creativity: 101 Definitions (what Webster Never Told You)},
  author={Aleinikov, A.G. and Kackmeister, S. and Koenig, R.},
  isbn={9780970311009},
  url={https://books.google.ch/books?id=M2QpAAAACAAJ},
  year={2000},
  publisher={Alden B. Dow Creativity Center Press}
}

@article{runco2012std,
author = {Mark A. Runco and Garrett J. Jaeger},
title = {The Standard Definition of Creativity},
journal = {Creativity Research Journal},
volume = {24},
number = {1},
pages = {92-96},
year = {2012},
publisher = {Routledge},
doi = {10.1080/10400419.2012.650092},
URL = {https://doi.org/10.1080/10400419.2012.650092},
eprint = {https://doi.org/10.1080/10400419.2012.650092}
}

@article{barron1955disposition,
  title={The disposition toward originality.},
  author={Frank Barron},
  journal={Journal of abnormal psychology},
  year={1955},
  volume={51 3},
  pages={478-85},
  url={https://api.semanticscholar.org/CorpusID:30102436}
}

@article{stein1953creativity,
  title={Creativity and Culture},
  author={Morris Isaac Stein},
  journal={Creativity in Art, Religion, and Culture},
  year={1953},
  url={https://api.semanticscholar.org/CorpusID:144504562}
}

@article{rudolph2001-cc,
  title={What it means to be creative},
  author={Rudolf Arnheim},
  journal={British Journal of Aesthetics},
  year={2001},
  volume={41},
  pages={24-25},
  url={https://api.semanticscholar.org/CorpusID:191484364}
}

@article{weisberg2015value,
  title={On the Usefulness of “Value” in the Definition of Creativity},
  author={Robert W. Weisberg},
  journal={Creativity Research Journal},
  year={2015},
  volume={27},
  pages={111 - 124},
  url={https://api.semanticscholar.org/CorpusID:146831140}
}

@inproceedings{anderson2024,
author = {Anderson, Barrett R and Shah, Jash Hemant and Kreminski, Max},
title = {Homogenization Effects of Large Language Models on Human Creative Ideation},
year = {2024},
isbn = {9798400704857},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3635636.3656204},
doi = {10.1145/3635636.3656204},
booktitle = {Proceedings of the 16th Conference on Creativity and Cognition},
pages = {413–425},
numpages = {13},
keywords = {creativity support tools, divergent ideation, large language models, user study},
location = {Chicago, IL, USA}
}

@article{std-def-creativity,
author = {Mark A. Runco and Garrett J. Jaeger},
title = {The Standard Definition of Creativity},
journal = {Creativity Research Journal},
volume = {24},
number = {1},
pages = {92-96},
year = {2012},
publisher = {Routledge},
doi = {10.1080/10400419.2012.650092},
URL = {https://doi.org/10.1080/10400419.2012.650092},
eprint = {https://doi.org/10.1080/10400419.2012.650092}
}

@book{kant1790critique,
  title={Critique of Judgment},
  author={Kant, I.},
  isbn={9781602065420},
  series={Cosimo Classics},
  url={https://books.google.ch/books?id=yBFsrJqNtygC},
  year={1790},
  publisher={Cosimo}
}

@article{stokes2011,
 ISSN = {00261068, 14679973},
 URL = {http://www.jstor.org/stable/24439973},
 abstract = {Creativity has received, and continues to receive, comparatively little analysis in philosophy and the brain and behavioural sciences. This is in spite of the importance of creative thought and action, and the many and varied resources of theories of mind. Here an alternative approach to analyzing creativity is suggested: start from the bottom up with minimally creative thought. Minimally creative thought depends non-accidentally upon agency, is novel relative to the acting agent, and could not have been tokened before the time it is in fact tokened, relative to the agent in question. Thoughts that meet these three conditions—agency, psychological novelty, and modal—are what may be called cognitive breakthroughs. Even if such breakthroughs are not necessary to or definitive of richer creativity, they are indeed central to much of creativity. The minimal analysis provides a more workable explanandum for theories of creativity of varied motivation and method.},
 author = {Stokes, Dustin},
 journal = {Metaphilosophy},
 number = {5},
 pages = {658--681},
 publisher = {Wiley},
 title = {Minimally Creative Thought},
 urldate = {2024-01-24},
 volume = {42},
 year = {2011}
}

@article{novitz1999,
author = {David Novitz},
title = {Creativity and Constraint},
journal = {Australasian Journal of Philosophy},
volume = {77},
number = {1},
pages = {67-82},
year = {1999},
publisher = {Routledge},
doi = {10.1080/00048409912348811},
URL = {https://doi.org/10.1080/00048409912348811},
eprint = {https://doi.org/10.1080/00048409912348811}
}

@InCollection{sep-creativity,
author       =	{Paul, Elliot Samuel and Stokes, Dustin},
title        =	{{Creativity}},
booktitle    =	{The {Stanford} Encyclopedia of Philosophy},
editor       =	{Edward N. Zalta and Uri Nodelman},
howpublished =	{\url{https://plato.stanford.edu/archives/spr2023/entries/creativity/}},
year         =	{2023},
edition      =	{{S}pring 2023},
publisher    =	{Metaphysics Research Lab, Stanford University}
}

@incollection{Livingston2018-LIVEQ,
author = {Paisley Livingston},
booktitle = {Routledge Handbook on Creativity and Philosophy},
editor = {Berys Gaut and Matthew Kieran},
pages = {108--123},
publisher = {Routledge},
title = {'Explicating "Creativity"},
year = {2018}
}

@book{wallas1926art,
  title={The Art of Thought},
  author={Wallas, G.},
  isbn={9781910146057},
  url={https://books.google.ch/books?id=JR6boAEACAAJ},
  year={1926},
  publisher={Solis Press}
}

@article{rhodes1961fourp,
 ISSN = {00317217},
 URL = {http://www.jstor.org/stable/20342603},
 author = {Mel Rhodes},
 journal = {The Phi Delta Kappan},
 number = {7},
 pages = {305--310},
 publisher = {Phi Delta Kappa International},
 title = {An Analysis of Creativity},
 urldate = {2024-01-24},
 volume = {42},
 year = {1961}
}

@article{Mednick1962TheAB,
  title={The associative basis of the creative process.},
  author={Sarnoff A. Mednick},
  journal={Psychological review},
  year={1962},
  volume={69},
  pages={220-32},
  url={https://api.semanticscholar.org/CorpusID:6702759}
}

@article{Glveanu2013RewritingTL,
  title={Rewriting the Language of Creativity: The Five A's Framework},
  author={Vlad Petre Glăveanu},
  journal={Review of General Psychology},
  year={2013},
  volume={17},
  pages={69 - 81},
  url={https://api.semanticscholar.org/CorpusID:143404705}
}

@book{amabile1996creativity,
  title={Creativity In Context: Update To The Social Psychology Of Creativity},
  author={Amabile, T.M.},
  isbn={9780813345499},
  url={https://books.google.ch/books?id=pZI4DgAAQBAJ},
  year={1996},
  publisher={Avalon Publishing}
}

@article{sternberg1991invest,
 ISSN = {0018716X, 14230054},
 URL = {http://www.jstor.org/stable/26767348},
 abstract = {This article presents an investment theory of creativity. The theory comprises 6 resources for creativity – intellectual processes, knowledge, intellectual style, personality, motivation, and environmental context. Creative performance results from a confluence of these elements. Main features of each resource are explained and the manner in which the 6 resources combine is discussed. Then a preliminary empirical study that tests aspects of the investment theory is briefly presented. Next, the development of creativity in terms of the 6 resources is described. Finally, potential criticisms of the investment theory are addressed. The goal of the theory is to understand in a cohesive way the foundations of creativity. To the extent that true creativity seems rare, it may be because many people are not willing to invest in it and because so many resources must converge in order to generate it.},
 author = {Robert J. Sternberg and Todd I. Lubart},
 journal = {Human Development},
 number = {1},
 pages = {1--31},
 publisher = {S. Karger AG},
 title = {An Investment Theory of Creativity and Its Development},
 urldate = {2024-01-25},
 volume = {34},
 year = {1991}
}

@article{sternberg2016triang,
author = {Sternberg, Robert},
year = {2016},
month = {12},
pages = {},
title = {A Triangular Theory of Creativity},
volume = {12},
journal = {Psychology of Aesthetics, Creativity, and the Arts},
doi = {10.1037/aca0000095}
}

@inbook{beghetto2022theories,
author = {Beghetto, Ronald and Kaufman, James},
year = {2022},
month = {02},
pages = {23-36},
title = {Theories of Creativity},
isbn = {9781003233923},
doi = {10.4324/9781003233923-3}
}

@article{schmidhuber2010fun,
  author={Schmidhuber, Jürgen},
  journal={IEEE Transactions on Autonomous Mental Development}, 
  title={Formal Theory of Creativity, Fun, and Intrinsic Motivation (1990–2010)}, 
  year={2010},
  volume={2},
  number={3},
  pages={230-247},
  keywords={Art;Psychology;Pediatrics;Predictive models;Data compression;Computational intelligence;Intelligent robots;Feedback;Eyes;Fingers;Active learning;aesthetics theory;art;attention;developmental psychology;formal theory of creativity;fun;humor;limited computational resources;music;novel patterns;novelty;science;surprise;typology of intrinsic motivation},
  doi={10.1109/TAMD.2010.2056368}
}

@article{Rowe1993CreativityAS,
  title={Creativity: a survey of AI approaches},
  author={Jon Rowe and Derek Partridge},
  journal={Artificial Intelligence Review},
  year={1993},
  volume={7},
  pages={43-70},
  url={https://api.semanticscholar.org/CorpusID:13433112}
}

@article{Franceschelli2021CreativityAM,
  title={Creativity and Machine Learning: A Survey},
  author={Giorgio Franceschelli and Mirco Musolesi},
  journal={ArXiv},
  year={2021},
  volume={abs/2104.02726},
  url={https://api.semanticscholar.org/CorpusID:233168627}
}

@book{chomsky1965aspects,
 ISBN = {9780262527408},
 URL = {http://www.jstor.org/stable/j.ctt17kk81z},
 abstract = {Noam Chomsky'sAspects of the Theory of Syntax, published in 1965, was a landmark work in generative grammar that introduced certain technical innovations still drawn upon in contemporary work. The fiftieth anniversary edition of this influential book includes a new preface by the author that identifies proposals that seem to be of lasting significance, reviews changes and improvements in the formulation and implementation of basic ideas, and addresses some of the controversies that arose over the general framework.Beginning in the mid-fifties and emanating largely from MIT, linguists developed an approach to linguistic theory and to the study of the structure of particular languages that diverged in many respects from conventional modern linguistics. Although the new approach was connected to the traditional study of languages, it differed enough in its specific conclusions about the structure of language to warrant a name, "generative grammar." Various deficiencies were discovered in the first attempts to formulate a theory of transformational generative grammar and in the descriptive analysis of particular languages that motivated these formulations. At the same time, it became apparent that these formulations can be extended and deepened. In this book, Chomsky reviews these developments and proposes a reformulation of the theory of transformational generative grammar that takes them into account. The emphasis in this study is syntax; semantic and phonological aspects of the language structure are discussed only insofar as they bear on syntactic theory.},
 author = {Noam Chomsky},
 edition = {50},
 publisher = {The MIT Press},
 title = {Aspects of the Theory of Syntax},
 urldate = {2024-01-25},
 year = {1965}
}

@article{bergs2019ling,
author = {Alexander Bergs},
doi = {doi:10.2478/gth-2019-0017},
url = {https://doi.org/10.2478/gth-2019-0017},
title = {What, If Anything, Is Linguistic Creativity?},
journal = {Gestalt Theory},
number = {2},
volume = {41},
year = {2019},
pages = {173--183}
}

@book{sampson2017ling,
 ISBN = {9781781796061},
 URL = {https://www.equinoxpub.com/home/linguistics-delusion/},
 author = {Geoffrey Sampson},
 publisher = {Equinox Publishing},
 title = {The Linguistics Delusion},
 urldate = {2017-09-14},
 year = {2017}
}

@article{zawada2006ling,
author = {Britta Zawada},
title = {Linguistic creativity from a cognitive perspective},
journal = {Southern African Linguistics and Applied Language Studies},
volume = {24},
number = {2},
pages = {235-254},
year = {2006},
publisher = {Routledge},
doi = {10.2989/16073610609486419},
URL = {https://doi.org/10.2989/16073610609486419},
eprint = {https://doi.org/10.2989/16073610609486419}
}

@article{lebowitz1984creating,
title = {Creating characters in a story-telling universe},
journal = {Poetics},
volume = {13},
number = {3},
pages = {171-194},
year = {1984},
issn = {0304-422X},
doi = {https://doi.org/10.1016/0304-422X(84)90001-9},
url = {https://www.sciencedirect.com/science/article/pii/0304422X84900019},
author = {Michael Lebowitz},
abstract = {Extended story generation, i.e., the creation of continuing serials, presents difficult and interesting problems for Artificial Intelligence. We present here the first phase of the development of a program, UNIVERSE, that will ultimately tell extended stories. In particular, after describing our overall model of story telling, we present a method for creating universes of characters appropriate for extended story generation. This method concentrates on the need to keep story-telling universes consistent and coherent. We also describe the information that must be maintained for characters and interpersonal relationships, and the use of stereotypical information about people to help motivate trait values. The use of historical events for motivation is also described. Finally, we present an example of a character generated by UNIVERSE.}
}

@inproceedings{yao2019plan,
  title = {Plan-And-Write: Towards Better Automatic Storytelling},
  author = {Yao, Lili and Peng, Nanyun and Ralph, Weischedel and Knight, Kevin and Zhao, Dongyan and Yan, Rui},
  booktitle = {The Thirty-Third AAAI Conference on Artificial Intelligence (AAAI-19)},
  year = {2019}
}

@inproceedings{goldfarb2020content,
  title = {Content Planning for Neural Story Generation with Aristotelian Rescoring},
  author = {Goldfarb-Tarrant, Seraphina and Chakrabarty, Tuhin and Weischedel, Ralph and Peng, Nanyun},
  booktitle = {the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages = {4319--4338},
  slideslive_id = {38939240},
  year = {2020}
}

@inproceedings{chakrabarty-etal-2023-creative,
    title = "Creative Natural Language Generation",
    author = "Chakrabarty, Tuhin  and
      Padmakumar, Vishakh  and
      He, He  and
      Peng, Nanyun",
    editor = "Zhang, Qi  and
      Sajjad, Hassan",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-tutorial.6",
    doi = "10.18653/v1/2023.emnlp-tutorial.6",
    pages = "34--40",
    abstract = "Large language models such as GPT-3, GPT4, Claude etc., have advanced the state of the art in several natural language generation tasks such as text summarization and machine translation. However when it comes to open-ended tasks with a focus on creativity such as generating stories, poetry, or various forms of figurative language, these state-of-the-art language models are often found to be inadequate. This tutorial aims to bring awareness of the important and emerging research area of open-domain creative generation, with a focus on language generation while also touching on multi-modal generation (e.g., image captioning, visual metaphors). It targets natural language processing (NLP) and artificial intelligence (AI) researchers as well as creative writing practitioners who are interested in building systems that are capable of emulating as well as augmenting human creativity. In particular, we will review recent studies on creative language generation both at the sentence level as well as longer forms of text. We will provide the audiences with a holistic view of 1) the importance and challenges of building creative language generation systems; 2) how we incorporate content planning, domain knowledge and creativity specific heuristics for different forms of creative language generation such as story, poetry, humor, metaphors etc 3) how can we build better evaluation methods for creative text generation? In particular, how could the recent advancement of AI shape the future workforce for creativity? We will conclude the tutorial by outlining future research directions in this area.",
}

@misc{chakrabarty2023art,
      title={Art or Artifice? Large Language Models and the False Promise of Creativity}, 
      author={Tuhin Chakrabarty and Philippe Laban and Divyansh Agarwal and Smaranda Muresan and Chien-Sheng Wu},
      year={2023},
      eprint={2309.14556},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{manurung2000flexible,
author = {Manurung, Ruli and Ritchie, Graeme and Thompson, Henry},
year = {2000},
month = {07},
pages = {},
title = {A Flexible Integrated Architecture For Generating Poetic Texts}
}

@inproceedings{Ren2017NeuralJG,
  title={Neural Joke Generation},
  author={He Ren and Quan Yang},
  year={2017},
  url={https://api.semanticscholar.org/CorpusID:19647037}
}

@book{veale2016metaphor,
  title={Metaphor: A Computational Perspective},
  author={Veale, T. and Shutova, E. and Klebanov, B.},
  isbn={9781627058513},
  series={Synthesis Lectures on Human Language Technologies},
  url={https://books.google.ch/books?id=_ripCwAAQBAJ},
  year={2016},
  publisher={Morgan \& Claypool Publishers}
}

@article{Paul1970-PAUFL,
	author = {Anthony M. Paul},
	journal = {Philosophy and Rhetoric},
	number = {4},
	pages = {225--248},
	publisher = {Pennsylvania State University Press},
	title = {Figurative Language},
	volume = {3},
	year = {1970}
}

@misc{dhar2019learning,
      title={Learning to Predict Novel Noun-Noun Compounds}, 
      author={Prajit Dhar and Lonneke van der Plas},
      year={2019},
      eprint={1906.03634},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{lencione2022nameling,
author = {Gabriel R Lencione and Rodrigo F Nogueira and Paula Y Pasqualini},
year = {2022},
month = {06},
title = {Nameling: Creative Neologism Generation with Transfer Learning},
journal = {International Conference on Computational Creativity},
}


@article{roberts1994people,
  title={Why do people use figurative language?},
  author={Roberts, Richard M and Kreuz, Roger J},
  journal={Psychological science},
  volume={5},
  number={3},
  pages={159--163},
  year={1994},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}

@article{Suzuki9468,
	author = {Suzuki, Wendy A. and Feli{\'u}-M{\'o}jer, M{\'o}nica I. and Hasson, Uri and Yehuda, Rachel and Zarate, Jean Mary},
	title = {Dialogues: The Science and Power of Storytelling},
	volume = {38},
	number = {44},
	pages = {9468--9470},
	year = {2018},
	doi = {10.1523/JNEUROSCI.1942-18.2018},
	publisher = {Society for Neuroscience},
	abstract = {Skillful storytelling helps listeners understand the essence of complex concepts and ideas in meaningful and often personal ways. For this reason, storytelling is being embraced by scientists who not only want to connect more authentically with their audiences, but also want to understand how the brain processes this powerful form of communication. Here we present part of a conversation between a group of scientists actively engaged with the practice and/or the science of storytelling. We highlight the brain networks involved in the telling and hearing of stories and show how storytelling is being used well beyond the realm of public communication to add a deeper dimension to communication with our students and colleagues, as well as helping to make our profession more inclusive.},
	issn = {0270-6474},
	URL = {https://www.jneurosci.org/content/38/44/9468},
	eprint = {https://www.jneurosci.org/content/38/44/9468.full.pdf},
	journal = {Journal of Neuroscience}
}


@inproceedings{weissweiler-etal-2023-counting,
    title = "Counting the Bugs in {C}hat{GPT}{'}s Wugs: A Multilingual Investigation into the Morphological Capabilities of a Large Language Model",
    author = "Weissweiler, Leonie  and
      Hofmann, Valentin  and
      Kantharuban, Anjali  and
      Cai, Anna  and
      Dutt, Ritam  and
      Hengle, Amey  and
      Kabra, Anubha  and
      Kulkarni, Atharva  and
      Vijayakumar, Abhishek  and
      Yu, Haofei  and
      Schuetze, Hinrich  and
      Oflazer, Kemal  and
      Mortensen, David",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.401",
    doi = "10.18653/v1/2023.emnlp-main.401",
    pages = "6508--6524",
    abstract = "Large language models (LLMs) have recently reached an impressive level of linguistic capability, prompting comparisons with human language skills. However, there have been relatively few systematic inquiries into the linguistic capabilities of the latest generation of LLMs, and those studies that do exist (i) ignore the remarkable ability of humans to generalize, (ii) focus only on English, and (iii) investigate syntax or semantics and overlook other capabilities that lie at the heart of human language, like morphology. Here, we close these gaps by conducting the first rigorous analysis of the morphological capabilities of ChatGPT in four typologically varied languages (specifically, English, German, Tamil, and Turkish). We apply a version of Berko{'}s (1958) wug test to ChatGPT, using novel, uncontaminated datasets for the four examined languages. We find that ChatGPT massively underperforms purpose-built systems, particularly in English. Overall, our results{---}through the lens of morphology{---}cast a new light on the linguistic capabilities of ChatGPT, suggesting that claims of human-like language skills are premature and misleading.",
}

@book{duncker1948problem,
  title={On Problem-solving},
  author={Duncker, K. and Lees, L.S.},
  series={Psychological monographs},
  url={https://books.google.ch/books?id=g888tAEACAAJ},
  year={1948},
  publisher={American Psychological Ass.}
}

@misc{davidson_gureckis_lake_2022,
 title={Creativity, Compositionality, and Common Sense in Human Goal Generation},
 url={osf.io/preprints/psyarxiv/byzs5},
 DOI={10.31234/osf.io/byzs5},
 publisher={PsyArXiv},
 author={Davidson, Guy and Gureckis, Todd M and Lake, Brenden M},
 year={2022},
 month={Mar}
}

@misc{tian2023macgyver,
      title={MacGyver: Are Large Language Models Creative Problem Solvers?}, 
      author={Yufei Tian and Abhilasha Ravichander and Lianhui Qin and Ronan Le Bras and Raja Marjieh and Nanyun Peng and Yejin Choi and Thomas L. Griffiths and Faeze Brahman},
      year={2023},
      eprint={2311.09682},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{pease2002evaluate,
author = {Pease, Alison and Winterstein, Daniel and Colton, Simon},
year = {2002},
month = {10},
pages = {},
title = {Evaluating Machine Creativity}
}

@inproceedings{Colton2011ComputationalCT,
  title={Computational Creativity Theory: The FACE and IDEA Descriptive Models},
  author={Simon Colton and John William Charnley and Alison Pease},
  booktitle={International Conference on Innovative Computing and Cloud Computing},
  year={2011},
  url={https://api.semanticscholar.org/CorpusID:522774}
}

@inproceedings{Ritchie2001AssessingC,
  title={Assessing Creativity},
  author={Graeme D. Ritchie},
  year={2001},
  url={https://api.semanticscholar.org/CorpusID:925777}
}

@article{ritchie2007some,
author = {Ritchie, Graeme},
year = {2007},
month = {07},
pages = {67-99},
title = {Some Empirical Criteria for Attributing Creativity to a Computer Program},
volume = {17},
journal = {Minds and Machines},
doi = {10.1007/s11023-007-9066-2}
}

@article{bringsjord2000creat,
author = {Bringsjord, Selmer and Bello, Paul and Ferrucci, David},
year = {2000},
month = {06},
pages = {},
title = {Creativity, the Turing Test, and the (Better) Lovelace Test},
volume = {11},
isbn = {978-1-4020-1205-1},
journal = {Minds and Machines},
doi = {10.1023/A:1011206622741}
}

@article{Jordanous2012ASP,
  title={A Standardised Procedure for Evaluating Creative Systems: Computational Creativity Evaluation Based on What it is to be Creative},
  author={Anna Jordanous},
  journal={Cognitive Computation},
  year={2012},
  volume={4},
  pages={246 - 279},
  url={https://api.semanticscholar.org/CorpusID:311189}
}

@article{webb2017once,
author = {Webb, Margaret and Little, Daniel and Cropper, Simon},
year = {2017},
month = {10},
pages = {},
title = {Once more with feeling: Normative data for the aha experience in insight and noninsight problems},
volume = {50},
journal = {Behavior Research Methods},
doi = {10.3758/s13428-017-0972-9}
}

@book{torrance1974torrance,
  title={Torrance Tests of Creative Thinking: Verbal Tests, Forms A and B, Figural Tests, Forms A and B},
  author={Torrance, E.P.},
  isbn={9780663292523},
  series={Norms-technical manual},
  url={https://books.google.ch/books?id=_4dUYAAACAAJ},
  year={1974},
  publisher={Xerox}
}

@misc{stevenson2022putting,
      title={Putting GPT-3's Creativity to the (Alternative Uses) Test}, 
      author={Claire Stevenson and Iris Smal and Matthijs Baas and Raoul Grasman and Han van der Maas},
      year={2022},
      eprint={2206.08932},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{openai2023gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{openai2022chatgpt,
      title={ChatGT: Optimizing language models for dialogue}, 
      author={OpenAI},
      year={2022},
      url={https://openai.com/blog/chatgpt/}
}

@misc{brown2020language,
      title={Language Models are Few-Shot Learners}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{anthropic2022claude,
      title={Introducing Claude}, 
      author={Anthropic},
      year={2022},
      url={https://www.anthropic.com/index/introducing-claude}
}

@inproceedings{hendrickx-etal-2013-semeval,
    title = "{S}em{E}val-2013 Task 4: Free Paraphrases of Noun Compounds",
    author = "Hendrickx, Iris  and
      Kozareva, Zornitsa  and
      Nakov, Preslav  and
      {\'O} S{\'e}aghdha, Diarmuid  and
      Szpakowicz, Stan  and
      Veale, Tony",
    editor = "Manandhar, Suresh  and
      Yuret, Deniz",
    booktitle = "Second Joint Conference on Lexical and Computational Semantics (*{SEM}), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation ({S}em{E}val 2013)",
    month = jun,
    year = "2013",
    address = "Atlanta, Georgia, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/S13-2025",
    pages = "138--143",
}

@inproceedings{butnariu-etal-2009-semeval,
    title = "{S}em{E}val-2010 Task 9: The Interpretation of Noun Compounds Using Paraphrasing Verbs and Prepositions",
    author = "Butnariu, Cristina  and
      Kim, Su Nam  and
      Nakov, Preslav  and
      {\'O} S{\'e}aghdha, Diarmuid  and
      Szpakowicz, Stan  and
      Veale, Tony",
    editor = "Agirre, Eneko  and
      M{\`a}rquez, Llu{\'\i}s  and
      Wicentowski, Richard",
    booktitle = "Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions ({SEW}-2009)",
    month = jun,
    year = "2009",
    address = "Boulder, Colorado",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W09-2416",
    pages = "100--105",
}

@article{vecchi2017,
author = {Vecchi, Eva M. and Marelli, Marco and Zamparelli, Roberto and Baroni, Marco},
title = {Spicy Adjectives and Nominal Donkeys: Capturing Semantic Deviance Using Compositionality in Distributional Spaces},
journal = {Cognitive Science},
volume = {41},
number = {1},
pages = {102-136},
keywords = {Distributional models, Semantic spaces, Compositionality, Meaning representation, Semantic deviance},
doi = {https://doi.org/10.1111/cogs.12330},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cogs.12330},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/cogs.12330},
abstract = {Abstract Sophisticated senator and legislative onion. Whether or not you have ever heard of these things, we all have some intuition that one of them makes much less sense than the other. In this paper, we introduce a large dataset of human judgments about novel adjective-noun phrases. We use these data to test an approach to semantic deviance based on phrase representations derived with compositional distributional semantic methods, that is, methods that derive word meanings from contextual information, and approximate phrase meanings by combining word meanings. We present several simple measures extracted from distributional representations of words and phrases, and we show that they have a significant impact on predicting the acceptability of novel adjective-noun phrases even when a number of alternative measures classically employed in studies of compound processing and bigram plausibility are taken into account. Our results show that the extent to which an attributive adjective alters the distributional representation of the noun is the most significant factor in modeling the distinction between acceptable and deviant phrases. Our study extends current applications of compositional distributional semantic methods to linguistically and cognitively interesting problems, and it offers a new, quantitatively precise approach to the challenge of predicting when humans will find novel linguistic expressions acceptable and when they will not.},
year = {2017}
}

@inproceedings{jiayang-etal-2023-storyanalogy,
    title = "{S}tory{A}nalogy: Deriving Story-level Analogies from Large Language Models to Unlock Analogical Understanding",
    author = "Jiayang, Cheng  and
      Qiu, Lin  and
      Chan, Tsz  and
      Fang, Tianqing  and
      Wang, Weiqi  and
      Chan, Chunkit  and
      Ru, Dongyu  and
      Guo, Qipeng  and
      Zhang, Hongming  and
      Song, Yangqiu  and
      Zhang, Yue  and
      Zhang, Zheng",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.706",
    doi = "10.18653/v1/2023.emnlp-main.706",
    pages = "11518--11537",
    abstract = "Analogy-making between narratives is crucial for human reasoning. In this paper, we evaluate the ability to identify and generate analogies by constructing a first-of-its-kind large-scale story-level analogy corpus, StoryAnalogy, which contains 24K story pairs from diverse domains with human annotations on two similarities from the extended Structure-Mapping Theory. We design a set of tests on StoryAnalogy, presenting the first evaluation of story-level analogy identification and generation. Interestingly, we find that the analogy identification tasks are incredibly difficult not only for sentence embedding models but also for the recent large language models (LLMs) such as ChatGPT and LLaMa. ChatGPT, for example, only achieved around 30{\%} accuracy in multiple-choice questions (compared to over 85{\%} accuracy for humans). Furthermore, we observe that the data in StoryAnalogy can improve the quality of analogy generation in LLMs, where a fine-tuned FlanT5-xxl model achieves comparable performance to zero-shot ChatGPT.",
}

@misc{nagarajah2022understandingnarrativesdimensionsanalogy,
      title={Understanding Narratives through Dimensions of Analogy}, 
      author={Thiloshon Nagarajah and Filip Ilievski and Jay Pujara},
      year={2022},
      eprint={2206.07167},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2206.07167}, 
}

@misc{bitton2022vasrvisualanalogiessituation,
      title={VASR: Visual Analogies of Situation Recognition}, 
      author={Yonatan Bitton and Ron Yosef and Eli Strugo and Dafna Shahaf and Roy Schwartz and Gabriel Stanovsky},
      year={2022},
      eprint={2212.04542},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2212.04542}, 
}

@inproceedings{gladkova-etal-2016-analogy,
    title = "Analogy-based detection of morphological and semantic relations with word embeddings: what works and what doesn{'}t.",
    author = "Gladkova, Anna  and
      Drozd, Aleksandr  and
      Matsuoka, Satoshi",
    editor = "Andreas, Jacob  and
      Choi, Eunsol  and
      Lazaridou, Angeliki",
    booktitle = "Proceedings of the {NAACL} Student Research Workshop",
    month = jun,
    year = "2016",
    address = "San Diego, California",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N16-2002",
    doi = "10.18653/v1/N16-2002",
    pages = "8--15",
}

@inproceedings{czinczoll-etal-2022-scientific,
    title = "Scientific and Creative Analogies in Pretrained Language Models",
    author = "Czinczoll, Tamara  and
      Yannakoudakis, Helen  and
      Mishra, Pushkar  and
      Shutova, Ekaterina",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-emnlp.153",
    doi = "10.18653/v1/2022.findings-emnlp.153",
    pages = "2094--2100",
    abstract = "This paper examines the encoding of analogy in large-scale pretrained language models, such as BERT and GPT-2. Existing analogy datasets typically focus on a limited set of analogical relations, with a high similarity of the two domains between which the analogy holds. As a more realistic setup, we introduce the Scientific and Creative Analogy dataset (SCAN), a novel analogy dataset containing systematic mappings of multiple attributes and relational structures across dissimilar domains. Using this dataset, we test the analogical reasoning capabilities of several widely-used pretrained language models (LMs). We find that state-of-the-art LMs achieve low performance on these complex analogy tasks, highlighting the challenges still posed by analogy understanding.",
}

@inproceedings{sultan-shahaf-2022-life,
    title = "Life is a Circus and We are the Clowns: Automatically Finding Analogies between Situations and Processes",
    author = "Sultan, Oren  and
      Shahaf, Dafna",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.232",
    doi = "10.18653/v1/2022.emnlp-main.232",
    pages = "3547--3562",
    abstract = "Analogy-making gives rise to reasoning, abstraction, flexible categorization and counterfactual inference {--} abilities lacking in even the best AI systems today. Much research has suggested that analogies are key to non-brittle systems that can adapt to new domains. Despite their importance, analogies received little attention in the NLP community, with most research focusing on simple word analogies. Work that tackled more complex analogies relied heavily on manually constructed, hard-to-scale input representations.In this work, we explore a more realistic, challenging setup: our input is a pair of natural language procedural texts, describing a situation or a process (e.g., how the heart works/how a pump works). Our goal is to automatically extract entities and their relations from the text and find a mapping between the different domains based on relational similarity (e.g., blood is mapped to water). We develop an interpretable, scalable algorithm and demonstrate that it identifies the correct mappings 87{\%} of the time for procedural texts and 94{\%} for stories from cognitive-psychology literature. We show it can extract analogies from a large dataset of procedural texts, achieving 79{\%} precision (analogy prevalence in data: 3{\%}). Lastly, we demonstrate that our algorithm is robust to paraphrasing the input texts",
}

@inproceedings{zhu-de-melo-2020-sentence,
    title = "Sentence Analogies: Linguistic Regularities in Sentence Embeddings",
    author = "Zhu, Xunjie  and
      de Melo, Gerard",
    editor = "Scott, Donia  and
      Bel, Nuria  and
      Zong, Chengqing",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2020.coling-main.300",
    doi = "10.18653/v1/2020.coling-main.300",
    pages = "3389--3400",
    abstract = "While important properties of word vector representations have been studied extensively, far less is known about the properties of sentence vector representations. Word vectors are often evaluated by assessing to what degree they exhibit regularities with regard to relationships of the sort considered in word analogies. In this paper, we investigate to what extent commonly used sentence vector representation spaces as well reflect certain kinds of regularities. We propose a number of schemes to induce evaluation data, based on lexical analogy data as well as semantic relationships between sentences. Our experiments consider a wide range of sentence embedding methods, including ones based on BERT-style contextual embeddings. We find that different models differ substantially in their ability to reflect such regularities.",
}

@inproceedings{ushio-etal-2021-bert,
    title = "{BERT} is to {NLP} what {A}lex{N}et is to {CV}: Can Pre-Trained Language Models Identify Analogies?",
    author = "Ushio, Asahi  and
      Espinosa Anke, Luis  and
      Schockaert, Steven  and
      Camacho-Collados, Jose",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.280",
    doi = "10.18653/v1/2021.acl-long.280",
    pages = "3609--3624",
    abstract = "Analogies play a central role in human commonsense reasoning. The ability to recognize analogies such as {``}eye is to seeing what ear is to hearing{''}, sometimes referred to as analogical proportions, shape how we structure knowledge and understand language. Surprisingly, however, the task of identifying such analogies has not yet received much attention in the language model era. In this paper, we analyze the capabilities of transformer-based language models on this unsupervised task, using benchmarks obtained from educational settings, as well as more commonly used datasets. We find that off-the-shelf language models can identify analogies to a certain extent, but struggle with abstract and complex relations, and results are highly sensitive to model architecture and hyperparameters. Overall the best results were obtained with GPT-2 and RoBERTa, while configurations using BERT were not able to outperform word embedding models. Our results raise important questions for future work about how, and to what extent, pre-trained language models capture knowledge about abstract semantic relations.",
}

@inproceedings{Petersen2023CanLM,
  title={Can language models learn analogical reasoning? Investigating training objectives and comparisons to human performance},
  author={Molly R. Petersen and Lonneke van der Plas},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:263828777}
}

@inproceedings{NIPS2013_9aa42b31,
 author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Distributed Representations of Words and Phrases and their Compositionality},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf},
 volume = {26},
 year = {2013}
}

@inproceedings{Hu2023InContextAR,
  title={In-Context Analogical Reasoning with Pre-Trained Language Models},
  author={Xiaoyang Hu and Shane Storks and Richard L. Lewis and Joyce Yue Chai},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:258959097}
}

@article{Yasunaga2023LargeLM,
  title={Large Language Models as Analogical Reasoners},
  author={Michihiro Yasunaga and Xinyun Chen and Yujia Li and Panupong Pasupat and Jure Leskovec and Percy Liang and Ed Huai-hsin Chi and Denny Zhou},
  journal={ArXiv},
  year={2023},
  volume={abs/2310.01714},
  url={https://api.semanticscholar.org/CorpusID:263608847}
}

@article{Lewis2024UsingCT,
  title={Using Counterfactual Tasks to Evaluate the Generality of Analogical Reasoning in Large Language Models},
  author={Martha Lewis and Melanie Mitchell},
  journal={ArXiv},
  year={2024},
  volume={abs/2402.08955},
  url={https://api.semanticscholar.org/CorpusID:267657861}
}

@article{Opielka2024DoLL,
  title={Do Large Language Models Solve ARC Visual Analogies Like People Do?},
  author={Gustaw Opielka and Hannes Rosenbusch and Veerle Vijverberg and Claire E. Stevenson},
  journal={ArXiv},
  year={2024},
  volume={abs/2403.09734},
  url={https://api.semanticscholar.org/CorpusID:268510601}
}

@inproceedings{Musker2024SemanticSI,
  title={Semantic Structure-Mapping in LLM and Human Analogical Reasoning},
  author={Sam Musker and Alex Duchnowski and Raphael Milliere and Ellie Pavlick},
  year={2024},
  url={https://api.semanticscholar.org/CorpusID:270619799}
}

@inproceedings{Marquer2022TransferringLM,
  title={Transferring Learned Models of Morphological Analogy},
  author={Esteban Marquer and Pierre-Alexandre Murena and Miguel Couceiro},
  year={2022},
  url={https://api.semanticscholar.org/CorpusID:258688324}
}

@article{Webb2022EmergentAR,
  title={Emergent analogical reasoning in large language models},
  author={Taylor W. Webb and Keith J. Holyoak and Hongjing Lu},
  journal={Nature Human Behaviour},
  year={2022},
  volume={7},
  pages={1526 - 1541},
  url={https://api.semanticscholar.org/CorpusID:254854575}
}

@inproceedings{Yuan2023BeneathSS,
  title={Beneath Surface Similarity: Large Language Models Make Reasonable Scientific Analogies after Structure Abduction},
  author={Siyu Yuan and Jiangjie Chen and Xuyang Ge and Yanghua Xiao and Deqing Yang},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:258833317}
}

@article{Wijesiriwardene2023ANALOGICALA,
  title={ANALOGICAL - A Novel Benchmark for Long Text Analogy Evaluation in Large Language Models},
  author={Thilini Wijesiriwardene and Ruwan Wickramarachchi and Bimal Gajera and Shreeyash Mukul Gowaikar and Chandan Gupta and Aman Chadha and Aishwarya N. Reganti and Amit P. Sheth and Amitava Das},
  journal={ArXiv},
  year={2023},
  volume={abs/2305.05050},
  url={https://api.semanticscholar.org/CorpusID:258947824}
}

@article{Olson2021NamingUW,
  title={Naming unrelated words predicts creativity},
  author={Jay A. Olson and Johnny Nahas and Denis Chmoulevitch and Simon J. Cropper and Margaret E. Webb},
  journal={Proceedings of the National Academy of Sciences of the United States of America},
  year={2021},
  volume={118},
  url={https://api.semanticscholar.org/CorpusID:235472442}
}

@inproceedings{Chen2023ProbingTC,
  title={Probing the Creativity of Large Language Models: Can models produce divergent semantic association?},
  author={Honghua Chen and Nai Ding},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:264172665}
}

@article{forster2009creat,
author = {Forster, Eve and Ca, Eve and Dunbar, Kevin and Ca, Dunbar@utsc},
year = {2009},
month = {01},
pages = {},
title = {Creativity Evaluation through Latent Semantic Analysis},
journal = {Proceedings of the Annual Conference of the Cognitive Science Society}
}

@article{heinen2017semantic,
author = {Heinen, David and Johnson, Dan},
year = {2017},
month = {04},
pages = {},
title = {Semantic Distance: An Automated Measure of Creativity That Is Novel and Appropriate},
volume = {12},
journal = {Psychology of Aesthetics, Creativity, and the Arts},
doi = {10.1037/aca0000125}
}

@article{Franceschelli2022DeepCreativityMC,
  title={DeepCreativity: Measuring Creativity with Deep Learning Techniques},
  author={Giorgio Franceschelli and Mirco Musolesi},
  journal={Intelligenza Artificiale},
  year={2022},
  volume={16},
  pages={151-163},
  url={https://api.semanticscholar.org/CorpusID:246015719}
}

@misc{goodfellow2014generative,
      title={Generative Adversarial Networks}, 
      author={Ian J. Goodfellow and Jean Pouget-Abadie and Mehdi Mirza and Bing Xu and David Warde-Farley and Sherjil Ozair and Aaron Courville and Yoshua Bengio},
      year={2014},
      eprint={1406.2661},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@article{Baldi2010OfBA,
  title={Of bits and wows: A Bayesian theory of surprise with applications to attention},
  author={Pierre Baldi and Laurent Itti},
  journal={Neural networks : the official journal of the International Neural Network Society},
  year={2010},
  volume={23 5},
  pages={649-66},
  url={https://api.semanticscholar.org/CorpusID:2070585}
}

@article{Hale2006UncertaintyAT,
  title={Uncertainty About the Rest of the Sentence},
  author={John Hale},
  journal={Cognitive science},
  year={2006},
  volume={30 4},
  pages={643-72},
  url={https://api.semanticscholar.org/CorpusID:15922633}
}

@inproceedings{Frank2010UncertaintyRA,
  title={Uncertainty Reduction as a Measure of Cognitive Processing Effort},
  author={S. Frank},
  booktitle={CMCL@ACL},
  year={2010},
  url={https://api.semanticscholar.org/CorpusID:3097133}
}

@inproceedings{Grace2014WhatTE,
  title={What to expect when you're expecting: The role of unexpectedness in computationally evaluating creativity},
  author={Kazjon Grace and Mary Lou Maher},
  booktitle={International Conference on Innovative Computing and Cloud Computing},
  year={2014},
  url={https://api.semanticscholar.org/CorpusID:18330744}
}

@inproceedings{xie-etal-2023-deltascore,
    title = "{D}elta{S}core: Fine-Grained Story Evaluation with Perturbations",
    author = "Xie, Zhuohan  and
      Li, Miao  and
      Cohn, Trevor  and
      Lau, Jey",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.353",
    doi = "10.18653/v1/2023.findings-emnlp.353",
    pages = "5317--5331",
    abstract = "Numerous evaluation metrics have been developed for natural language generation tasks, but their effectiveness in evaluating stories is limited as they are not specifically tailored to assess intricate aspects of storytelling, such as fluency and interestingness. In this paper, we introduce DeltaScore, a novel methodology that uses perturbation techniques for the evaluation of nuanced story aspects. We posit that the extent to which a story excels in a specific aspect (e.g., fluency) correlates with the magnitude of its susceptibility to particular perturbations (e.g., the introduction of typos). Given this, we measure the quality of an aspect by calculating the likelihood difference between pre- and post-perturbation states using pre-trained language models. We compare DeltaScore with existing metrics on storytelling datasets from two domains in five fine-grained story aspects: fluency, coherence, relatedness, logicality, and interestingness. DeltaScore demonstrates strong performance, revealing a surprising finding that one specific perturbation proves highly effective in capturing multiple aspects. Source code is available on our GitHub repository.",
}

@misc{vaswani2017attention,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2017},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{Yu2020HowNS,
  title={How nouns surface as verbs: Inference and generation in word class conversion},
  author={Lei Yu and Lana El Sanyoura and Yang Xu},
  booktitle={Annual Meeting of the Cognitive Science Society},
  year={2020},
  url={https://api.semanticscholar.org/CorpusID:221020946}
}

@inproceedings{watson2021coin,
  title={Coin it up: generalization of creative constructions in the wild},
  author={Julia Watson and Farhan Samir and Suzanne Stevenson and Barend Beekhuizen},
  booktitle={Proceedings of the Annual Meeting of the Cognitive Science Society},
  year={2021},
}

@book{fowler1986linguistic,
  title={Linguistic Criticism},
  author={Fowler, R.},
  isbn={9780192191250},
  lccn={85015418},
  series={An OPUS book},
  url={https://books.google.ch/books?id=0vm2AAAAIAAJ},
  year={1986},
  publisher={Oxford University Press}
}

@misc{skalicky2022fork,
  title={“A fork is a food stabber”: Linguistic creativity in English L1 and L2 speakers},
  author={Skalicky, Stephen and Bell, Nancy and Dascalu, Mihai and Crossley, Scott},
  year={2022},
  publisher={Proceedings of the Annual Meeting of the Cognitive Science Society}
}

@article{Brennan1996ConceptualPA,
  title={Conceptual pacts and lexical choice in conversation.},
  author={Susan Brennan and Herbert H. Clark},
  journal={Journal of experimental psychology. Learning, memory, and cognition},
  year={1996},
  volume={22 6},
  pages={1482-93},
  url={https://api.semanticscholar.org/CorpusID:15405799}
}

@inproceedings{liu-etal-2023-vera,
    title = "Vera: A General-Purpose Plausibility Estimation Model for Commonsense Statements",
    author = "Liu, Jiacheng  and
      Wang, Wenya  and
      Wang, Dianzhuo  and
      Smith, Noah  and
      Choi, Yejin  and
      Hajishirzi, Hannaneh",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.81",
    doi = "10.18653/v1/2023.emnlp-main.81",
    pages = "1264--1287",
    abstract = "Today{'}s language models can be remarkably intelligent yet still produce text that contains trivial commonsense errors. Therefore, we seek a retrospective verification approach that can reflect on the commonsense plausibility of the machine text, and introduce Vera, a general-purpose model that learns to estimate the commonsense plausibility of declarative statements. To support diverse commonsense domains, Vera is trained on {\textasciitilde}7M commonsense statements that are automatically converted from 19 QA datasets and two commonsense knowledge bases, and using a combination of three training objectives. When applied to solving commonsense problems in the verification format, Vera substantially outperforms existing models that can be repurposed for commonsense verification, even including GPT-3.5/ChatGPT/GPT-4, and it further exhibits generalization capabilities to unseen tasks and provides well-calibrated outputs. We find that Vera excels at filtering machine-generated commonsense knowledge and is useful in detecting erroneous commonsense statements generated by models like ChatGPT in real-world settings.",
}

@article{ravfogel2023retrieving,
      title={Retrieving Texts based on Abstract Descriptions}, 
      author={Shauli Ravfogel and Valentina Pyatkin and Amir DN Cohen and Avshalom Manevich and Yoav Goldberg},
      year={2023},
      eprint={2305.12517},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{Zhu2009HowCI,
  title={How creative is your writing? A linguistic creativity measure from computer science and cognitive psychology perspectives},
  author={Xiaojin Zhu and Zhiting Xu and Tushar Khot},
  year={2009},
  url={https://api.semanticscholar.org/CorpusID:17340001}
}

@inproceedings{maguire2004evidence,
author = {Maguire, Rebecca},
year = {2004},
month = {08},
pages = {},
title = {Evidence of muddy knowledge in reaching for the stars: Creating novel endings to event sequences.}
}

@inproceedings{Lynott2005FamiliarityAC,
  title={Familiarity and creativity in novel compound production.},
  author={Dermot Lynott and Bruno G. Bara and Lawrence W. Barsalou and Monica Bucciarelli},
  year={2005},
  url={https://api.semanticscholar.org/CorpusID:9947884}
}

@book{smith1995creative,
  title={The Creative Cognition Approach},
  author={Smith, S.M. and Ward, T.B. and Finke, R.A.},
  isbn={9780262193542},
  lccn={lc94021947},
  series={Bradford book},
  url={https://books.google.ch/books?id=iEMWI4Z1rF8C},
  year={1995},
  publisher={BRADFORD BOOK}
}

@misc{li2023large,
      title={Large Language Models Understand and Can be Enhanced by Emotional Stimuli}, 
      author={Cheng Li and Jindong Wang and Yixuan Zhang and Kaijie Zhu and Wenxin Hou and Jianxun Lian and Fang Luo and Qiang Yang and Xing Xie},
      year={2023},
      eprint={2307.11760},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{SummersStay2023BrainstormTS,
  title={Brainstorm, then Select: a Generative Language Model Improves Its Creativity Score},
  author={Douglas Summers-Stay and Stephanie M. Lukin and Clare R. Voss},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:259305709}
}

@misc{riley2019evidence,
      title={Evidence that Threatening Situations Enhance Creativity}, 
      author={Sean N. Riley and Liane Gabora},
      year={2019},
      eprint={1308.4245},
      archivePrefix={arXiv},
      primaryClass={q-bio.NC}
}

@article{Liu2020FeelingOC,
  title={Feeling of Competence Affects Children's Curiosity and Creativity},
  author={Rongzhi Liu and Fei Xu},
  journal={Cognitive Science},
  year={2020},
  url={https://api.semanticscholar.org/CorpusID:220039894}
}

@misc{beck2023psych,
      title={Psychological distance and children's innovative problem solving}, 
      author={Beck, Sarah R and Whalley, Clare Louise and Lagaha, Sabbi and APPERLY, Ian and Chappell, Jackie M and Taylor Bunce, Louise},
      year={2023},
      journal={Cognitive Science},
}

@misc{meister2023locally,
      title={Locally Typical Sampling}, 
      author={Clara Meister and Tiago Pimentel and Gian Wiher and Ryan Cotterell},
      year={2023},
      eprint={2202.00666},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{li2023contrastive,
      title={Contrastive Decoding: Open-ended Text Generation as Optimization}, 
      author={Xiang Lisa Li and Ari Holtzman and Daniel Fried and Percy Liang and Jason Eisner and Tatsunori Hashimoto and Luke Zettlemoyer and Mike Lewis},
      year={2023},
      eprint={2210.15097},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{Guimaraes2017ObjectiveReinforcedGA,
  title={Objective-Reinforced Generative Adversarial Networks (ORGAN) for Sequence Generation Models},
  author={Gabriel Lima Guimaraes and Benjam{\'i}n S{\'a}nchez-Lengeling and Pedro Luis Cunha Farias and Al{\'a}n Aspuru-Guzik},
  journal={ArXiv},
  year={2017},
  volume={abs/1705.10843},
  url={https://api.semanticscholar.org/CorpusID:35911567}
}

@article{Broad2021ActiveDW,
  title={Active Divergence with Generative Deep Learning - A Survey and Taxonomy},
  author={Terence Broad and Sebastian Berns and Simon Colton and Mick Grierson},
  journal={ArXiv},
  year={2021},
  volume={abs/2107.05599},
  url={https://api.semanticscholar.org/CorpusID:235794880}
}

@inproceedings{Bunescu2019LearningTS,
  title={Learning to Surprise: A Composer-Audience Architecture},
  author={Razvan C. Bunescu and Oseremen O. Uduehi},
  booktitle={International Conference on Innovative Computing and Cloud Computing},
  year={2019},
  url={https://api.semanticscholar.org/CorpusID:218606180}
}

@article{Franceschelli2023OnTC,
  title={On the Creativity of Large Language Models},
  author={Giorgio Franceschelli and Mirco Musolesi},
  journal={ArXiv},
  year={2023},
  volume={abs/2304.00008},
  url={https://api.semanticscholar.org/CorpusID:257913327}
}

@inproceedings{kuznetsova-etal-2013-understanding,
    title = "Understanding and Quantifying Creativity in Lexical Composition",
    author = "Kuznetsova, Polina  and
      Chen, Jianfu  and
      Choi, Yejin",
    editor = "Yarowsky, David  and
      Baldwin, Timothy  and
      Korhonen, Anna  and
      Livescu, Karen  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing",
    month = oct,
    year = "2013",
    address = "Seattle, Washington, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D13-1124",
    pages = "1246--1258",
}

@inproceedings{bunescu-uduehi-2022-distribution,
    title = "Distribution-Based Measures of Surprise for Creative Language: Experiments with Humor and Metaphor",
    author = "Bunescu, Razvan C.  and
      Uduehi, Oseremen O.",
    editor = "Ghosh, Debanjan  and
      Beigman Klebanov, Beata  and
      Muresan, Smaranda  and
      Feldman, Anna  and
      Poria, Soujanya  and
      Chakrabarty, Tuhin",
    booktitle = "Proceedings of the 3rd Workshop on Figurative Language Processing (FLP)",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates (Hybrid)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.flp-1.10",
    doi = "10.18653/v1/2022.flp-1.10",
    pages = "68--78",
    abstract = "Novelty or surprise is a fundamental attribute of creative output. As such, we postulate that a writer{'}s creative use of language leads to word choices and, more importantly, corresponding semantic structures that are unexpected for the reader. In this paper we investigate measures of surprise that rely solely on word distributions computed by language models and show empirically that creative language such as humor and metaphor is strongly correlated with surprise. Surprisingly at first, information content is observed to be at least as good a predictor of creative language as any of the surprise measures investigated. However, the best prediction performance is obtained when information and surprise measures are combined, showing that surprise measures capture an aspect of creative language that goes beyond information content.",
}

@inproceedings{he-etal-2023-hauser,
    title = "{HAUSER}: Towards Holistic and Automatic Evaluation of Simile Generation",
    author = "He, Qianyu  and
      Zhang, Yikai  and
      Liang, Jiaqing  and
      Huang, Yuncheng  and
      Xiao, Yanghua  and
      Chen, Yunwen",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.702",
    doi = "10.18653/v1/2023.acl-long.702",
    pages = "12557--12572",
    abstract = "Similes play an imperative role in creative writing such as story and dialogue generation. Proper evaluation metrics are like a beacon guiding the research of simile generation (SG). However, it remains under-explored as to what criteria should be considered, how to quantify each criterion into metrics, and whether the metrics are effective for comprehensive, efficient, and reliable SG evaluation. To address the issues, we establish HAUSER, a holistic and automatic evaluation system for the SG task, which consists of five criteria from three perspectives and automatic metrics for each criterion. Through extensive experiments, we verify that our metrics are significantly more correlated with human ratings from each perspective compared with prior automatic metrics. Resources of HAUSER are publicly available at \url{https://github.com/Abbey4799/HAUSER}.",
}

@article{Kim2006CanWT,
  title={Can We Trust Creativity Tests? A Review of the Torrance Tests of Creative Thinking (TTCT)},
  author={Kyung H Kim},
  journal={Creativity Research Journal},
  year={2006},
  volume={18},
  pages={14 - 3},
  url={https://api.semanticscholar.org/CorpusID:17636888}
}

@article{Amabile1983TheSP,
  title={The social psychology of creativity: A componential conceptualization.},
  author={T. M. Amabile},
  journal={Journal of Personality and Social Psychology},
  year={1983},
  volume={45},
  pages={357-376},
  url={https://api.semanticscholar.org/CorpusID:145719543}
}

@article{Prabhakaran2013ThinSO,
  title={Thin slices of creativity: Using single-word utterances to assess creative cognition},
  author={Ranjani Prabhakaran and Adam E. Green and Jeremy R. Gray},
  journal={Behavior Research Methods},
  year={2013},
  volume={46},
  pages={641 - 659},
  url={https://api.semanticscholar.org/CorpusID:2159817}
}

@inproceedings{Dunbar2009CreativityET,
  title={Creativity Evaluation through Latent Semantic Analysis},
  author={Kevin Dunbar and Eve Forster},
  year={2009},
  url={https://api.semanticscholar.org/CorpusID:18209155}
}

@article{Harbison2014AutomatedSO,
  title={Automated scoring of originality using semantic representations},
  author={J. Isaiah Harbison and Henk J. Haarmann},
  journal={Cognitive Science},
  year={2014},
  volume={36},
  url={https://api.semanticscholar.org/CorpusID:2645346}
}

@article{Beaty2020AutomatingCA,
  title={Automating creativity assessment with SemDis: An open platform for computing semantic distance},
  author={Roger E. Beaty and Dan Richard Johnson},
  journal={Behavior Research Methods},
  year={2020},
  volume={53},
  pages={757 - 780},
  url={https://api.semanticscholar.org/CorpusID:221403286}
}

@article{Johnson2022DivergentSI,
  title={Divergent semantic integration (DSI): Extracting creativity from narratives with distributional semantic modeling},
  author={Dan Richard Johnson and J. Kaufman and Brendan S. Baker and John D. Patterson and Baptiste Barbot and Adam E. Green and Janet G. van Hell and Evan S. Kennedy and Grace F Sullivan and Christa L. Taylor and Thomas Ward and Roger E. Beaty},
  journal={Behavior Research Methods},
  year={2022},
  volume={55},
  pages={3726 - 3759},
  url={https://api.semanticscholar.org/CorpusID:252969336}
}

@inproceedings{amin-burghardt-2020-survey,
    title = "A Survey on Approaches to Computational Humor Generation",
    author = "Amin, Miriam  and
      Burghardt, Manuel",
    editor = "DeGaetano, Stefania  and
      Kazantseva, Anna  and
      Reiter, Nils  and
      Szpakowicz, Stan",
    booktitle = "Proceedings of the 4th Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature",
    month = dec,
    year = "2020",
    address = "Online",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2020.latechclfl-1.4",
    pages = "29--41",
    abstract = "We provide a comprehensive overview of existing systems for the computational generation of verbal humor in the form of jokes and short humorous texts. Considering linguistic humor theories, we analyze the systematic strengths and drawbacks of the different approaches. In addition, we show how the systems have been evaluated so far and propose two evaluation criteria: humorousness and complexity. From our analysis of the field, we conclude new directions for the advancement of computational humor generation.",
}

@inproceedings{he-etal-2019-pun,
    title = "Pun Generation with Surprise",
    author = "He, He  and
      Peng, Nanyun  and
      Liang, Percy",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1172",
    doi = "10.18653/v1/N19-1172",
    pages = "1734--1744",
    abstract = "We tackle the problem of generating a pun sentence given a pair of homophones (e.g., {``}died{''} and {``}dyed{''}). Puns are by their very nature statistically anomalous and not amenable to most text generation methods that are supervised by a large corpus. In this paper, we propose an unsupervised approach to pun generation based on lots of raw (unhumorous) text and a surprisal principle. Specifically, we posit that in a pun sentence, there is a strong association between the pun word (e.g., {``}dyed{''}) and the distant context, but a strong association between the alternative word (e.g., {``}died{''}) and the immediate context. We instantiate the surprisal principle in two ways: (i) as a measure based on the ratio of probabilities given by a language model, and (ii) a retrieve-and-edit approach based on words suggested by a skip-gram model. Based on human evaluation, our retrieve-and-edit approach generates puns successfully 30{\%} of the time, doubling the success rate of a neural generation baseline.",
}

@inproceedings{mittal-etal-2022-ambipun,
    title = "{A}mbi{P}un: Generating Humorous Puns with Ambiguous Context",
    author = "Mittal, Anirudh  and
      Tian, Yufei  and
      Peng, Nanyun",
    editor = "Carpuat, Marine  and
      de Marneffe, Marie-Catherine  and
      Meza Ruiz, Ivan Vladimir",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.77",
    doi = "10.18653/v1/2022.naacl-main.77",
    pages = "1053--1062",
    abstract = "In this paper, we propose a simple yet effective way to generate pun sentences that does not require any training on existing puns. Our approach is inspired by humor theories that ambiguity comes from the context rather than the pun word itself. Given a pair of definitions of a pun word, our model first produces a list of related concepts through a reverse dictionary. We then utilize one-shot GPT3 to generate context words and then generate puns incorporating context words from both concepts. Human evaluation shows that our method successfully generates pun 52{\%} of the time, outperforming well-crafted baselines and the state-of-the-art models by a large margin.",
}

@inproceedings{chakrabarty-etal-2020-r,
    title = "{R}{\^{}}3: Reverse, Retrieve, and Rank for Sarcasm Generation with Commonsense Knowledge",
    author = "Chakrabarty, Tuhin  and
      Ghosh, Debanjan  and
      Muresan, Smaranda  and
      Peng, Nanyun",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.711",
    doi = "10.18653/v1/2020.acl-main.711",
    pages = "7976--7986",
    abstract = "We propose an unsupervised approach for sarcasm generation based on a non-sarcastic input sentence. Our method employs a retrieve-and-edit framework to instantiate two major characteristics of sarcasm: reversal of valence and semantic incongruity with the context, which could include shared commonsense or world knowledge between the speaker and the listener. While prior works on sarcasm generation predominantly focus on context incongruity, we show that combining valence reversal and semantic incongruity based on the commonsense knowledge generates sarcasm of higher quality. Human evaluation shows that our system generates sarcasm better than humans 34{\%} of the time, and better than a reinforced hybrid baseline 90{\%} of the time.",
}

@inproceedings{hessel-etal-2023-androids,
    title = "Do Androids Laugh at Electric Sheep? Humor {``}Understanding{''} Benchmarks from The New Yorker Caption Contest",
    author = "Hessel, Jack  and
      Marasovic, Ana  and
      Hwang, Jena D.  and
      Lee, Lillian  and
      Da, Jeff  and
      Zellers, Rowan  and
      Mankoff, Robert  and
      Choi, Yejin",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.41",
    doi = "10.18653/v1/2023.acl-long.41",
    pages = "688--714",
    abstract = "Large neural networks can now generate jokes, but do they really {``}understand{''} humor? We challenge AI models with three tasks derived from the New Yorker Cartoon Caption Contest: matching a joke to a cartoon, identifying a winning caption, and explaining why a winning caption is funny. These tasks encapsulate progressively more sophisticated aspects of {``}understanding{''} a cartoon; key elements are the complex, often surprising relationships between images and captions and the frequent inclusion of indirect and playful allusions to human experience and culture. We investigate both multimodal and language-only models: the former are challenged with the cartoon images directly, while the latter are given multifaceted descriptions of the visual scene to simulate human-level visual understanding. We find that both types of models struggle at all three tasks. For example, our best multimodal models fall 30 accuracy points behind human performance on the matching task, and, even when provided ground-truth visual scene descriptors, human-authored explanations are preferred head-to-head over the best machine-authored ones (few-shot GPT-4) in more than 2/3 of cases. We release models, code, leaderboard, and corpus, which includes newly-gathered annotations describing the image{'}s locations/entities, what{'}s unusual in the scene, and an explanation of the joke.",
}

@inproceedings{jentzsch-kersting-2023-chatgpt,
    title = "{C}hat{GPT} is fun, but it is not funny! Humor is still challenging Large Language Models",
    author = "Jentzsch, Sophie  and
      Kersting, Kristian",
    editor = "Barnes, Jeremy  and
      De Clercq, Orph{\'e}e  and
      Klinger, Roman",
    booktitle = "Proceedings of the 13th Workshop on Computational Approaches to Subjectivity, Sentiment, {\&} Social Media Analysis",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.wassa-1.29",
    doi = "10.18653/v1/2023.wassa-1.29",
    pages = "325--340",
    abstract = "Humor is a central aspect of human communication that has not been solved for artificial agents so far. Large language models (LLMs) are increasingly able to capture implicit and contextual information. Especially, OpenAI{'}s ChatGPT recently gained immense public attention. The GPT3-based model almost seems to communicate on a human level and can even tell jokes. Humor is an essential component of human communication. But is ChatGPT really funny?We put ChatGPT{'}s sense of humor to the test. In a series of exploratory experiments around jokes, i.e., generation, explanation, and detection, we seek to understand ChatGPT{'}s capability to grasp and reproduce human humor. Since the model itself is not accessible, we applied prompt-based experiments. Our empirical evidence indicates that jokes are not hard-coded but mostly also not newly generated by the model. Over 90{\%} of 1008 generated jokes were the same 25 Jokes. The system accurately explains valid jokes but also comes up with fictional explanations for invalid jokes. Joke-typical characteristics can mislead ChatGPT in the classification of jokes. ChatGPT has not solved computational humor yet but it can be a big leap toward {``}funny{''} machines.",
}

@inproceedings{chakrabarty-etal-2020-generating,
    title = "Generating similes effortlessly like a Pro: A Style Transfer Approach for Simile Generation",
    author = "Chakrabarty, Tuhin  and
      Muresan, Smaranda  and
      Peng, Nanyun",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.524",
    doi = "10.18653/v1/2020.emnlp-main.524",
    pages = "6455--6469",
    abstract = "Literary tropes, from poetry to stories, are at the crux of human imagination and communication. Figurative language such as a simile go beyond plain expressions to give readers new insights and inspirations. In this paper, we tackle the problem of simile generation. Generating a simile requires proper understanding for effective mapping of properties between two concepts. To this end, we first propose a method to automatically construct a parallel corpus by transforming a large number of similes collected from Reddit to their literal counterpart using structured common sense knowledge. We then propose to fine-tune a pre-trained sequence to sequence model, BART (Lewis et al 2019), on the literal-simile pairs to gain generalizability, so that we can generate novel similes given a literal sentence. Experiments show that our approach generates 88{\%} novel similes that do not share properties with the training data. Human evaluation on an independent set of literal statements shows that our model generates similes better than two literary experts 37{\%} of the time when compared pairwise. We also show how replacing literal sentences with similes from our best model in machine-generated stories improves evocativeness and leads to better acceptance by human judges.",
}

@article{Chakrabarty2021ItsNR,
  title={It’s not Rocket Science: Interpreting Figurative Language in Narratives},
  author={Tuhin Chakrabarty and Yejin Choi and Vered Shwartz},
  journal={Transactions of the Association for Computational Linguistics},
  year={2021},
  volume={10},
  pages={589-606},
  url={https://api.semanticscholar.org/CorpusID:237371845}
}

@inproceedings{Chakrabarty2022FLUTEFL,
  title={FLUTE: Figurative Language Understanding through Textual Explanations},
  author={Tuhin Chakrabarty and Arkadiy Saakyan and Debanjan Ghosh and Smaranda Muresan},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2022},
  url={https://api.semanticscholar.org/CorpusID:252780978}
}

@inproceedings{Mohammad2016MetaphorAA,
  title={Metaphor as a Medium for Emotion: An Empirical Study},
  author={Saif M. Mohammad and Ekaterina Shutova and Peter D. Turney},
  booktitle={International Workshop on Semantic Evaluation},
  year={2016},
  url={https://api.semanticscholar.org/CorpusID:989439}
}

@article{Stowe2020MetaphoricPG,
  title={Metaphoric Paraphrase Generation},
  author={Kevin Stowe and Leonardo Ribeiro and Iryna Gurevych},
  journal={ArXiv},
  year={2020},
  volume={abs/2002.12854},
  url={https://api.semanticscholar.org/CorpusID:211572789}
}

@article{Stowe2021MetaphorGW,
  title={Metaphor Generation with Conceptual Mappings},
  author={Kevin Stowe and Tuhin Chakrabarty and Nanyun Peng and Smaranda Muresan and Iryna Gurevych},
  journal={ArXiv},
  year={2021},
  volume={abs/2106.01228},
  url={https://api.semanticscholar.org/CorpusID:235294009}
}

@inproceedings{mohler-etal-2016-introducing,
    title = "Introducing the {LCC} Metaphor Datasets",
    author = "Mohler, Michael  and
      Brunson, Mary  and
      Rink, Bryan  and
      Tomlinson, Marc",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Grobelnik, Marko  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, Helene  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16)",
    month = may,
    year = "2016",
    address = "Portoro{\v{z}}, Slovenia",
    publisher = "European Language Resources Association (ELRA)",
    url = "https://aclanthology.org/L16-1668",
    pages = "4221--4227",
    abstract = "In this work, we present the Language Computer Corporation (LCC) annotated metaphor datasets, which represent the largest and most comprehensive resource for metaphor research to date. These datasets were produced over the course of three years by a staff of nine annotators working in four languages (English, Spanish, Russian, and Farsi). As part of these datasets, we provide (1) metaphoricity ratings for within-sentence word pairs on a four-point scale, (2) scored links to our repository of 114 source concept domains and 32 target concept domains, and (3) ratings for the affective polarity and intensity of each pair. Altogether, we provide 188,741 annotations in English (for 80,100 pairs), 159,915 annotations in Spanish (for 63,188 pairs), 99,740 annotations in Russian (for 44,632 pairs), and 137,186 annotations in Farsi (for 57,239 pairs). In addition, we are providing a large set of likely metaphors which have been independently extracted by our two state-of-the-art metaphor detection systems but which have not been analyzed by our team of annotators.",
}

@article{jacobs2018,
AUTHOR={Jacobs, Arthur M. },
TITLE={The Gutenberg English Poetry Corpus: Exemplary Quantitative Narrative Analyses},
JOURNAL={Frontiers in Digital Humanities},
VOLUME={5},
YEAR={2018},
URL={https://www.frontiersin.org/journals/digital-humanities/articles/10.3389/fdigh.2018.00005},
DOI={10.3389/fdigh.2018.00005},
ISSN={2297-2668},
ABSTRACT={<p>This paper describes a corpus of about 3,000 English literary texts with about 250 million words extracted from the Gutenberg project that span a range of genres from both fiction and non-fiction written by more than 130 authors (e.g., Darwin, Dickens, Shakespeare). Quantitative narrative analysis (QNA) is used to explore a cleaned subcorpus, the <italic>Gutenberg English Poetry Corpus</italic> (GEPC), which comprises over 100 poetic texts with around two million words from about 50 authors (e.g., Keats, Joyce, Wordsworth). Some exemplary QNA studies show author similarities based on latent semantic analysis, significant topics for each author or various text-analytic metrics for George Eliot’s poem “How Lisa Loved the King” and James Joyce’s “Chamber Music,” concerning, e.g., lexical diversity or sentiment analysis. The GEPC is particularly suited for research in Digital Humanities, Computational Stylistics, or Neurocognitive Poetics, e.g., as training and test corpus for stimulus development and control in empirical studies.</p>}
}

@inproceedings{chakrabarty-etal-2021-mermaid,
    title = "{MERMAID}: Metaphor Generation with Symbolism and Discriminative Decoding",
    author = "Chakrabarty, Tuhin  and
      Zhang, Xurui  and
      Muresan, Smaranda  and
      Peng, Nanyun",
    editor = "Toutanova, Kristina  and
      Rumshisky, Anna  and
      Zettlemoyer, Luke  and
      Hakkani-Tur, Dilek  and
      Beltagy, Iz  and
      Bethard, Steven  and
      Cotterell, Ryan  and
      Chakraborty, Tanmoy  and
      Zhou, Yichao",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.336",
    doi = "10.18653/v1/2021.naacl-main.336",
    pages = "4250--4261",
    abstract = "Generating metaphors is a challenging task as it requires a proper understanding of abstract concepts, making connections between unrelated concepts, and deviating from the literal meaning. In this paper, we aim to generate a metaphoric sentence given a literal expression by replacing relevant verbs. Based on a theoretically-grounded connection between metaphors and symbols, we propose a method to automatically construct a parallel corpus by transforming a large number of metaphorical sentences from the Gutenberg Poetry corpus (CITATION) to their literal counterpart using recent advances in masked language modeling coupled with commonsense inference. For the generation task, we incorporate a metaphor discriminator to guide the decoding of a sequence to sequence model fine-tuned on our parallel data to generate high-quality metaphors. Human evaluation on an independent test set of literal statements shows that our best model generates metaphors better than three well-crafted baselines 66{\%} of the time on average. A task-based evaluation shows that human-written poems enhanced with metaphors proposed by our model are preferred 68{\%} of the time compared to poems without metaphors.",
}

@inproceedings{chakrabarty-etal-2023-spy,
    title = "{I} Spy a Metaphor: Large Language Models and Diffusion Models Co-Create Visual Metaphors",
    author = "Chakrabarty, Tuhin  and
      Saakyan, Arkadiy  and
      Winn, Olivia  and
      Panagopoulou, Artemis  and
      Yang, Yue  and
      Apidianaki, Marianna  and
      Muresan, Smaranda",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.465",
    doi = "10.18653/v1/2023.findings-acl.465",
    pages = "7370--7388",
    abstract = "Visual metaphors are powerful rhetorical devices used to persuade or communicate creative ideas through images. Similar to linguistic metaphors, they convey meaning implicitly through symbolism and juxtaposition of the symbols. We propose a new task of generating visual metaphors from linguistic metaphors. This is a challenging task for diffusion-based text-to-image models, such as DALL$\cdot$E 2, since it requires the ability to model implicit meaning and compositionality. We propose to solve the task through the collaboration between Large Language Models (LLMs) and Diffusion Models: Instruct GPT-3 (davinci-002) with Chain-of-Thought prompting generates text that represents a visual elaboration of the linguistic metaphor containing the implicit meaning and relevant objects, which is then used as input to the diffusion-based text-to-image models. Using a human-AI collaboration framework, where humans interact both with the LLM and the top-performing diffusion model, we create a high-quality dataset containing 6,476 visual metaphors for 1,540 linguistic metaphors and their associated visual elaborations. Evaluation by professional illustrators shows the promise of LLM-Diffusion Model collaboration for this task.To evaluate the utility of our Human-AI collaboration framework and the quality of our dataset, we perform both an intrinsic human-based evaluation and an extrinsic evaluation using visual entailment as a downstream task.",
}

@inproceedings{ghazvininejad-etal-2016-generating,
    title = "Generating Topical Poetry",
    author = "Ghazvininejad, Marjan  and
      Shi, Xing  and
      Choi, Yejin  and
      Knight, Kevin",
    editor = "Su, Jian  and
      Duh, Kevin  and
      Carreras, Xavier",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D16-1126",
    doi = "10.18653/v1/D16-1126",
    pages = "1183--1191",
}

@inproceedings{van-de-cruys-2020-automatic,
    title = "Automatic Poetry Generation from Prosaic Text",
    author = "Van de Cruys, Tim",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.223",
    doi = "10.18653/v1/2020.acl-main.223",
    pages = "2471--2480",
    abstract = "In the last few years, a number of successful approaches have emerged that are able to adequately model various aspects of natural language. In particular, language models based on neural networks have improved the state of the art with regard to predictive language modeling, while topic models are successful at capturing clear-cut, semantic dimensions. In this paper, we will explore how these approaches can be adapted and combined to model the linguistic and literary aspects needed for poetry generation. The system is exclusively trained on standard, non-poetic text, and its output is constrained in order to confer a poetic character to the generated verse. The framework is applied to the generation of poems in both English and French, and is equally evaluated for both languages. Even though it only uses standard, non-poetic text as input, the system yields state of the art results for poetry generation.",
}

@inproceedings{chakrabarty-etal-2022-help,
    title = "Help me write a Poem - Instruction Tuning as a Vehicle for Collaborative Poetry Writing",
    author = "Chakrabarty, Tuhin  and
      Padmakumar, Vishakh  and
      He, He",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.460",
    doi = "10.18653/v1/2022.emnlp-main.460",
    pages = "6848--6863",
    abstract = "Recent work in training large language models (LLMs) to follow natural language instructions has opened up exciting opportunities for natural language interface design. Building on the prior success of large language models in the realm of computer assisted creativity, in this work, we present \textit{CoPoet}, a collaborative poetry writing system, with the goal of to study if LLM{'}s actually improve the quality of the generated content. In contrast to auto-completing a user{'}s text, CoPoet is controlled by user instructions that specify the attributes of the desired text, such as \textit{Write a sentence about {`}love{'}} or \textit{Write a sentence ending in {`}fly{'}}. The core component of our system is a language model fine-tuned on a diverse collection of instructions for poetry writing. Our model is not only competitive to publicly available LLMs trained on instructions (InstructGPT), but also capable of satisfying unseen compositional instructions. A study with 15 qualified crowdworkers shows that users successfully write poems with CoPoet on diverse topics ranging from \textit{Monarchy} to \textit{Climate change}, which are preferred by third-party evaluators over poems written without the system.",
}

@inproceedings{ormazabal-etal-2022-poelm,
    title = "{P}oe{LM}: A Meter- and Rhyme-Controllable Language Model for Unsupervised Poetry Generation",
    author = "Ormazabal, Aitor  and
      Artetxe, Mikel  and
      Agirrezabal, Manex  and
      Soroa, Aitor  and
      Agirre, Eneko",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-emnlp.268",
    doi = "10.18653/v1/2022.findings-emnlp.268",
    pages = "3655--3670",
    abstract = "Formal verse poetry imposes strict constraints on the meter and rhyme scheme of poems. Most prior work on generating this type of poetry uses existing poems for supervision, which are difficult to obtain for most languages and poetic forms. In this work, we propose an unsupervised approach to generate poems that follow any given meter and rhyme scheme, without requiring any poetic text for training. Our method works by splitting a regular, non-poetic corpus into phrases, prepending control codes that describe the length and end rhyme of each phrase, and training a transformer language model in the augmented corpus. The transformer learns to link the structure descriptor with the control codes to the number of lines, their length and their end rhyme. During inference, we build control codes for the desired meter and rhyme scheme, and condition our language model on them to generate formal verse poetry. Experiments in Spanish and Basque show that our approach is able to generate valid poems, which are often comparable in quality to those written by humans.",
}

@inproceedings{tian-peng-2022-zero,
    title = "Zero-shot Sonnet Generation with Discourse-level Planning and Aesthetics Features",
    author = "Tian, Yufei  and
      Peng, Nanyun",
    editor = "Carpuat, Marine  and
      de Marneffe, Marie-Catherine  and
      Meza Ruiz, Ivan Vladimir",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.262",
    doi = "10.18653/v1/2022.naacl-main.262",
    pages = "3587--3597",
    abstract = "Poetry generation, and creative language generation in general, usually suffers from the lack of large training data. In this paper, we present a novel framework to generate sonnets that does not require training on poems. We design a hierarchical framework which plans the poem sketch before decoding. Specifically, a content planning module is trained on non-poetic texts to obtain discourse-level coherence; then a rhyme module generates rhyme words and a polishing module introduces imagery and similes for aesthetics purposes. Finally, we design a constrained decoding algorithm to impose the meter-and-rhyme constraint of the generated sonnets. Automatic and human evaluation show that our multi-stage approach without training on poem corpora generates more coherent, poetic, and creative sonnets than several strong baselines.",
}

@inproceedings{coil-shwartz-2023-chocolate,
    title = "From chocolate bunny to chocolate crocodile: Do Language Models Understand Noun Compounds?",
    author = "Coil, Albert  and
      Shwartz, Vered",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.169",
    doi = "10.18653/v1/2023.findings-acl.169",
    pages = "2698--2710",
    abstract = "Noun compound interpretation is the task of expressing a noun compound (e.g. chocolate bunny) in a free-text paraphrase that makes the relationship between the constituent nouns explicit (e.g. bunny-shaped chocolate). We propose modifications to the data and evaluation setup of the standard task (Hendrickx et al., 2013), and show that GPT-3 solves it almost perfectly. We then investigate the task of noun compound conceptualization, i.e. paraphrasing a novel or rare noun compound. E.g., chocolate crocodile is a crocodile-shaped chocolate. This task requires creativity, commonsense, and the ability to generalize knowledge about similar concepts. While GPT-3{'}s performance is not perfect, it is better than that of humans{---}likely thanks to its access to vast amounts of knowledge, and because conceptual processing is effortful for people (Connell and Lynott, 2012). Finally, we estimate the extent to which GPT-3 is reasoning about the world vs. parroting its training data. We find that the outputs from GPT-3 often have significant overlap with a large web corpus, but that the parroting strategy is less beneficial for novel noun compounds.",
}

@inproceedings{das-ghosh-2017-neuramanteau,
    title = "{N}euramanteau: A Neural Network Ensemble Model for Lexical Blends",
    author = "Das, Kollol  and
      Ghosh, Shaona",
    editor = "Kondrak, Greg  and
      Watanabe, Taro",
    booktitle = "Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = nov,
    year = "2017",
    address = "Taipei, Taiwan",
    publisher = "Asian Federation of Natural Language Processing",
    url = "https://aclanthology.org/I17-1058",
    pages = "576--583",
    abstract = "The problem of blend formation in generative linguistics is interesting in the context of neologism, their quick adoption in modern life and the creative generative process guiding their formation. Blend quality depends on multitude of factors with high degrees of uncertainty. In this work, we investigate if the modern neural network models can sufficiently capture and recognize the creative blend composition process. We propose recurrent neural network sequence-to-sequence models, that are evaluated on multiple blend datasets available in the literature. We propose an ensemble neural and hybrid model that outperforms most of the baselines and heuristic models upon evaluation on test data.",
}

@article{Dathathri2019PlugAP,
  title={Plug and Play Language Models: A Simple Approach to Controlled Text Generation},
  author={Sumanth Dathathri and Andrea Madotto and Janice Lan and Jane Hung and Eric Frank and Piero Molino and Jason Yosinski and Rosanne Liu},
  journal={ArXiv},
  year={2019},
  volume={abs/1912.02164},
  url={https://api.semanticscholar.org/CorpusID:208617790}
}

@article{Pascual2021APM,
  title={A Plug-and-Play Method for Controlled Text Generation},
  author={Damian Pascual and B{\'e}ni Egressy and Clara Meister and Ryan Cotterell and Roger Wattenhofer},
  journal={ArXiv},
  year={2021},
  volume={abs/2109.09707},
  url={https://api.semanticscholar.org/CorpusID:237571784}
}

@inproceedings{Tambwekar2018ControllableNS,
  title={Controllable Neural Story Plot Generation via Reward Shaping},
  author={Pradyumna Tambwekar and Murtaza Dhuliawala and Lara J. Martin and Animesh Mehta and Brent Harrison and Mark O. Riedl},
  booktitle={International Joint Conference on Artificial Intelligence},
  year={2018},
  url={https://api.semanticscholar.org/CorpusID:199465680}
}

@article{Chung2022TaleBrushSS,
  title={TaleBrush: Sketching Stories with Generative Pretrained Language Models},
  author={John Joon Young Chung and Wooseok Kim and Kang Min Yoo and Hwaran Lee and Eytan Adar and Minsuk Chang},
  journal={Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
  year={2022},
  url={https://api.semanticscholar.org/CorpusID:247625751}
}

@article{Rashkin2020PlotMachinesOG,
  title={PlotMachines: Outline-Conditioned Generation with Dynamic Plot State Tracking},
  author={Hannah Rashkin and Asli Celikyilmaz and Yejin Choi and Jianfeng Gao},
  journal={ArXiv},
  year={2020},
  volume={abs/2004.14967},
  url={https://api.semanticscholar.org/CorpusID:216868683}
}

@inproceedings{sun-etal-2023-evaluating,
    title = "Evaluating Large Language Models on Controlled Generation Tasks",
    author = "Sun, Jiao  and
      Tian, Yufei  and
      Zhou, Wangchunshu  and
      Xu, Nan  and
      Hu, Qian  and
      Gupta, Rahul  and
      Wieting, John  and
      Peng, Nanyun  and
      Ma, Xuezhe",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.190",
    doi = "10.18653/v1/2023.emnlp-main.190",
    pages = "3155--3168",
    abstract = "While recent studies have looked into the abilities of large language models in various benchmark tasks, including question generation, reading comprehension, multilingual and etc, there have been few studies looking into the controllability of large language models on generation tasks. We present an extensive analysis of various benchmarks including a sentence planning benchmark with different granularities. After comparing large language models against state-of-the-start finetuned smaller models, we present a spectrum showing large language models falling behind, are comparable, or exceed the ability of smaller models. We conclude that *large language models struggle at meeting fine-grained hard constraints*.",
}

@article{Elazar2021MeasuringAI,
  title={Measuring and Improving Consistency in Pretrained Language Models},
  author={Yanai Elazar and Nora Kassner and Shauli Ravfogel and Abhilasha Ravichander and Eduard H. Hovy and Hinrich Sch{\"u}tze and Yoav Goldberg},
  journal={Transactions of the Association for Computational Linguistics},
  year={2021},
  volume={9},
  pages={1012-1031},
  url={https://api.semanticscholar.org/CorpusID:231740560}
}

@article{Zhang2023HowLM,
  title={How Language Model Hallucinations Can Snowball},
  author={Muru Zhang and Ofir Press and William Merrill and Alisa Liu and Noah A. Smith},
  journal={ArXiv},
  year={2023},
  volume={abs/2305.13534},
  url={https://api.semanticscholar.org/CorpusID:258841857}
}

@inproceedings{Tam2022EvaluatingTF,
  title={Evaluating the Factual Consistency of Large Language Models Through News Summarization},
  author={Derek Tam and Anisha Mascarenhas and Shiyue Zhang and Sarah Kwan and Mohit Bansal and Colin Raffel},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2022},
  url={https://api.semanticscholar.org/CorpusID:253523092}
}

@article{Anderson2024HomogenizationEO,
  title={Homogenization Effects of Large Language Models on Human Creative Ideation},
  author={Barrett R Anderson and Jash Hemant Shah and Max Kreminski},
  journal={Proceedings of the 16th Conference on Creativity \& Cognition},
  year={2024},
  url={https://api.semanticscholar.org/CorpusID:267406608}
}

@article{Mirowski2022CoWritingSA,
  title={Co-Writing Screenplays and Theatre Scripts with Language Models: Evaluation by Industry Professionals},
  author={Piotr Wojciech Mirowski and Kory Wallace Mathewson and Jaylen Pittman and Richard Evans},
  journal={Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
  year={2022},
  url={https://api.semanticscholar.org/CorpusID:252596159}
}

@article{Schick2022PEERAC,
  title={PEER: A Collaborative Language Model},
  author={Timo Schick and Jane Dwivedi-Yu and Zhengbao Jiang and Fabio Petroni and Patrick Lewis and Gautier Izacard and Qingfei You and Christoforos Nalmpantis and Edouard Grave and Sebastian Riedel},
  journal={ArXiv},
  year={2022},
  volume={abs/2208.11663},
  url={https://api.semanticscholar.org/CorpusID:251765117}
}

@article{Chakrabarty2023CreativitySI,
  title={Creativity Support in the Age of Large Language Models: An Empirical Study Involving Emerging Writers},
  author={Tuhin Chakrabarty and Vishakh Padmakumar and Faeze Brahman and Smaranda Muresan},
  journal={ArXiv},
  year={2023},
  volume={abs/2309.12570},
  url={https://api.semanticscholar.org/CorpusID:262217523}
}

@article{Guan2020UNIONAU,
  title={UNION: An Unreferenced Metric for Evaluating Open-ended Story Generation},
  author={Jian Guan and Minlie Huang},
  journal={ArXiv},
  year={2020},
  volume={abs/2009.07602},
  url={https://api.semanticscholar.org/CorpusID:221739231}
}

@article{Chen2022StoryERAS,
  title={StoryER: Automatic Story Evaluation via Ranking, Rating and Reasoning},
  author={Hong Chen and Duc Minh Vo and Hiroya Takamura and Yusuke Miyao and Hideki Nakayama},
  journal={ArXiv},
  year={2022},
  volume={abs/2210.08459},
  url={https://api.semanticscholar.org/CorpusID:252918409}
}

@inproceedings{Jiang2023BRAINTEASERLT,
  title={BRAINTEASER: Lateral Thinking Puzzles for Large Language Models},
  author={Yifan Jiang and Filip Ilievski and Kaixin Ma},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:263830212}
}

@inproceedings{Lin2021RiddleSenseRA,
  title={RiddleSense: Reasoning about Riddle Questions Featuring Linguistic Creativity and Commonsense Knowledge},
  author={Bill Yuchen Lin and Ziyi Wu and Yichi Yang and Dong-Ho Lee and Xiang Ren},
  booktitle={Findings},
  year={2021},
  url={https://api.semanticscholar.org/CorpusID:235731975}
}

@article{Zhao2023UNcommonsenseRA,
  title={UNcommonsense Reasoning: Abductive Reasoning about Uncommon Situations},
  author={Wenting Zhao and Justin T Chiu and Jena D. Hwang and Faeze Brahman and Jack Hessel and Sanjiban Choudhury and Yejin Choi and Xiang Lorraine Li and Alane Suhr},
  journal={ArXiv},
  year={2023},
  volume={abs/2311.08469},
  url={https://api.semanticscholar.org/CorpusID:265213087}
}

@misc{samadarshi2024connectingdotsevaluatingabstract,
      title={Connecting the Dots: Evaluating Abstract Reasoning Capabilities of LLMs Using the New York Times Connections Word Game}, 
      author={Prisha Samadarshi and Mariam Mustafa and Anushka Kulkarni and Raven Rothkopf and Tuhin Chakrabarty and Smaranda Muresan},
      year={2024},
      eprint={2406.11012},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.11012}, 
}

@inproceedings{akoury-etal-2020-storium,
    title = "{STORIUM}: {A} {D}ataset and {E}valuation {P}latform for {M}achine-in-the-{L}oop {S}tory {G}eneration",
    author = "Akoury, Nader  and
      Wang, Shufan  and
      Whiting, Josh  and
      Hood, Stephen  and
      Peng, Nanyun  and
      Iyyer, Mohit",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.525",
    doi = "10.18653/v1/2020.emnlp-main.525",
    pages = "6470--6484",
    abstract = "Systems for story generation are asked to produce plausible and enjoyable stories given an input context. This task is underspecified, as a vast number of diverse stories can originate from a single input. The large output space makes it difficult to build and evaluate story generation models, as (1) existing datasets lack rich enough contexts to meaningfully guide models, and (2) existing evaluations (both crowdsourced and automatic) are unreliable for assessing long-form creative text. To address these issues, we introduce a dataset and evaluation platform built from STORIUM, an online collaborative storytelling community. Our author-generated dataset contains 6K lengthy stories (125M tokens) with fine-grained natural language annotations (e.g., character goals and attributes) interspersed throughout each narrative, forming a robust source for guiding models. We evaluate language models fine-tuned on our dataset by integrating them onto STORIUM, where real authors can query a model for suggested story continuations and then edit them. Automatic metrics computed over these edits correlate well with both user ratings of generated stories and qualitative feedback from semi-structured user interviews. We release both the STORIUM dataset and evaluation platform to spur more principled research into story generation.",
}

@misc{agarwal2020acrosticpoemgeneration,
      title={Acrostic Poem Generation}, 
      author={Rajat Agarwal and Katharina Kann},
      year={2020},
      eprint={2010.02239},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2010.02239}, 
}

@inproceedings{Liu2018beyond, 
   series={MM ’18},
   title={Beyond Narrative Description: Generating Poetry from Images by Multi-Adversarial Training},
   url={http://dx.doi.org/10.1145/3240508.3240587},
   DOI={10.1145/3240508.3240587},
   booktitle={Proceedings of the 26th ACM international conference on Multimedia},
   publisher={ACM},
   author={Liu, Bei and Fu, Jianlong and Kato, Makoto P. and Yoshikawa, Masatoshi},
   year={2018},
   month=oct, collection={MM ’18} }

@inproceedings{du-chilton-2023-storywars,
    title = "{S}tory{W}ars: A Dataset and Instruction Tuning Baselines for Collaborative Story Understanding and Generation",
    author = "Du, Yulun  and
      Chilton, Lydia",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.171",
    doi = "10.18653/v1/2023.acl-long.171",
    pages = "3044--3062",
    abstract = "Collaborative stories, which are texts created through the collaborative efforts of multiple authors with different writing styles and intentions, pose unique challenges for NLP models. Understanding and generating such stories remains an underexplored area due to the lack of open-domain corpora. To address this, we introduce StoryWars, a new dataset of over 40,000 collaborative stories written by 9,400 different authors from an online platform. We design 12 task types, comprising 7 understanding and 5 generation task types, on {pasted macro {`}STORYWARS{'}}, deriving 101 diverse story-related tasks in total as a multi-task benchmark covering all fully-supervised, few-shot, and zero-shot scenarios. Furthermore, we present our instruction-tuned model, InstructStory, for the story tasks showing that instruction tuning, in addition to achieving superior results in zero-shot and few-shot scenarios, can also obtain the best performance on the fully-supervised tasks in StoryWars, establishing strong multi-task benchmark performances on StoryWars.",
}

@inproceedings{louis-sutton-2018-deep,
    title = "Deep Dungeons and Dragons: Learning Character-Action Interactions from Role-Playing Game Transcripts",
    author = "Louis, Annie  and
      Sutton, Charles",
    editor = "Walker, Marilyn  and
      Ji, Heng  and
      Stent, Amanda",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-2111",
    doi = "10.18653/v1/N18-2111",
    pages = "708--713",
    abstract = "An essential aspect to understanding narratives is to grasp the interaction between characters in a story and the actions they take. We examine whether computational models can capture this interaction, when both character attributes and actions are expressed as complex natural language descriptions. We propose role-playing games as a testbed for this problem, and introduce a large corpus of game transcripts collected from online discussion forums. Using neural language models which combine character and action descriptions from these stories, we show that we can learn the latent ties. Action sequences are better predicted when the character performing the action is also taken into account, and vice versa for character attributes.",
}

@article{hong-etal-2023-visual-writing,
    title = "Visual Writing Prompts: Character-Grounded Story Generation with Curated Image Sequences",
    author = "Hong, Xudong  and
      Sayeed, Asad  and
      Mehra, Khushboo  and
      Demberg, Vera  and
      Schiele, Bernt",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "11",
    year = "2023",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2023.tacl-1.33",
    doi = "10.1162/tacl_a_00553",
    pages = "565--581",
    abstract = "Current work on image-based story generation suffers from the fact that the existing image sequence collections do not have coherent plots behind them. We improve visual story generation by producing a new image-grounded dataset, Visual Writing Prompts (VWP). VWP contains almost 2K selected sequences of movie shots, each including 5-10 images. The image sequences are aligned with a total of 12K stories which were collected via crowdsourcing given the image sequences and a set of grounded characters from the corresponding image sequence. Our new image sequence collection and filtering process has allowed us to obtain stories that are more coherent, diverse, and visually grounded compared to previous work. We also propose a character-based story generation model driven by coherence as a strong baseline. Evaluations show that our generated stories are more coherent, visually grounded, and diverse than stories generated with the current state-of-the-art model. Our code, image features, annotations and collected stories are available at \url{https://vwprompt.github.io/}.",
}

@inproceedings{hossain-etal-2019-president,
    title = "{``}President Vows to Cut {\textless}Taxes{\textgreater} Hair{''}: Dataset and Analysis of Creative Text Editing for Humorous Headlines",
    author = "Hossain, Nabil  and
      Krumm, John  and
      Gamon, Michael",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1012",
    doi = "10.18653/v1/N19-1012",
    pages = "133--142",
    abstract = "We introduce, release, and analyze a new dataset, called Humicroedit, for research in computational humor. Our publicly available data consists of regular English news headlines paired with versions of the same headlines that contain simple replacement edits designed to make them funny. We carefully curated crowdsourced editors to create funny headlines and judges to score a to a total of 15,095 edited headlines, with five judges per headline. The simple edits, usually just a single word replacement, mean we can apply straightforward analysis techniques to determine what makes our edited headlines humorous. We show how the data support classic theories of humor, such as incongruity, superiority, and setup/punchline. Finally, we develop baseline classifiers that can predict whether or not an edited headline is funny, which is a first step toward automatically generating humorous headlines as an approach to creating topical humor.",
}

@inproceedings{kayatani2021,
author = {Kayatani, Yuta and Yang, Zekun and Otani, Mayu and Garcia, Noa and Chu, Chenhui and Nakashima, Yuta and Takemura, Haruo},
year = {2021},
month = {01},
pages = {2072-2081},
title = {The Laughing Machine: Predicting Humor in Video},
doi = {10.1109/WACV48630.2021.00212}
}

@inproceedings{stock-strapparava-2005-hahacronym,
    title = "{HAHA}cronym: A Computational Humor System",
    author = "Stock, Oliviero  and
      Strapparava, Carlo",
    editor = "Nagata, Masaaki  and
      Pedersen, Ted",
    booktitle = "Proceedings of the {ACL} Interactive Poster and Demonstration Sessions",
    month = jun,
    year = "2005",
    address = "Ann Arbor, Michigan",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P05-3029",
    doi = "10.3115/1225753.1225782",
    pages = "113--116",
}

@inproceedings{meaney-etal-2021-semeval,
    title = "{S}em{E}val 2021 Task 7: {H}a{H}ackathon, Detecting and Rating Humor and Offense",
    author = "Meaney, J. A.  and
      Wilson, Steven  and
      Chiruzzo, Luis  and
      Lopez, Adam  and
      Magdy, Walid",
    editor = "Palmer, Alexis  and
      Schneider, Nathan  and
      Schluter, Natalie  and
      Emerson, Guy  and
      Herbelot, Aurelie  and
      Zhu, Xiaodan",
    booktitle = "Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.semeval-1.9",
    doi = "10.18653/v1/2021.semeval-1.9",
    pages = "105--119",
    abstract = "SemEval 2021 Task 7, HaHackathon, was the first shared task to combine the previously separate domains of humor detection and offense detection. We collected 10,000 texts from Twitter and the Kaggle Short Jokes dataset, and had each annotated for humor and offense by 20 annotators aged 18-70. Our subtasks were binary humor detection, prediction of humor and offense ratings, and a novel controversy task: to predict if the variance in the humor ratings was higher than a specific threshold. The subtasks attracted 36-58 submissions, with most of the participants choosing to use pre-trained language models. Many of the highest performing teams also implemented additional optimization techniques, including task-adaptive training and adversarial training. The results suggest that the participating systems are well suited to humor detection, but that humor controversy is a more challenging task. We discuss which models excel in this task, which auxiliary techniques boost their performance, and analyze the errors which were not captured by the best systems.",
}

@inproceedings{miller-etal-2017-semeval,
    title = "{S}em{E}val-2017 Task 7: Detection and Interpretation of {E}nglish Puns",
    author = "Miller, Tristan  and
      Hempelmann, Christian  and
      Gurevych, Iryna",
    editor = "Bethard, Steven  and
      Carpuat, Marine  and
      Apidianaki, Marianna  and
      Mohammad, Saif M.  and
      Cer, Daniel  and
      Jurgens, David",
    booktitle = "Proceedings of the 11th International Workshop on Semantic Evaluation ({S}em{E}val-2017)",
    month = aug,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/S17-2005",
    doi = "10.18653/v1/S17-2005",
    pages = "58--68",
    abstract = "A pun is a form of wordplay in which a word suggests two or more meanings by exploiting polysemy, homonymy, or phonological similarity to another word, for an intended humorous or rhetorical effect. Though a recurrent and expected feature in many discourse types, puns stymie traditional approaches to computational lexical semantics because they violate their one-sense-per-context assumption. This paper describes the first competitive evaluation for the automatic detection, location, and interpretation of puns. We describe the motivation for these tasks, the evaluation methods, and the manually annotated data set. Finally, we present an overview and discussion of the participating systems{'} methodologies, resources, and results.",
}

@article{Mitchell2023ComparingHG,
  title={Comparing Humans, GPT-4, and GPT-4V On Abstraction and Reasoning Tasks},
  author={Melanie Mitchell and Alessandro B. Palmarini and Arseny Moskvichev},
  journal={ArXiv},
  year={2023},
  volume={abs/2311.09247},
  url={https://api.semanticscholar.org/CorpusID:265220802}
}

@article{Chollet2019OnTM,
  title={On the Measure of Intelligence},
  author={François Chollet},
  journal={ArXiv},
  year={2019},
  volume={abs/1911.01547},
  url={https://api.semanticscholar.org/CorpusID:207870692}
}

@article{Moskvichev2023TheCB,
  title={The ConceptARC Benchmark: Evaluating Understanding and Generalization in the ARC Domain},
  author={Arseny Moskvichev and Victor Vikram Odouard and Melanie Mitchell},
  journal={ArXiv},
  year={2023},
  volume={abs/2305.07141},
  url={https://api.semanticscholar.org/CorpusID:258676355}
}

@article{Xu2023LLMsAT,
  title={LLMs and the Abstraction and Reasoning Corpus: Successes, Failures, and the Importance of Object-based Representations},
  author={Yudong Xu and Wenhao Li and Pashootan Vaezipoor and Scott Sanner and Elias Boutros Khalil},
  journal={ArXiv},
  year={2023},
  volume={abs/2305.18354},
  url={https://api.semanticscholar.org/CorpusID:258968016}
}

@inproceedings{Xu2022GraphsCA,
  title={Graphs, Constraints, and Search for the Abstraction and Reasoning Corpus},
  author={Yudong Xu and Elias Boutros Khalil and Scott Sanner},
  booktitle={AAAI Conference on Artificial Intelligence},
  year={2022},
  url={https://api.semanticscholar.org/CorpusID:252968055}
}

@article{Odouard2022EvaluatingUO,
  title={Evaluating Understanding on Conceptual Abstraction Benchmarks},
  author={Victor Vikram Odouard and Melanie Mitchell},
  journal={ArXiv},
  year={2022},
  volume={abs/2206.14187},
  url={https://api.semanticscholar.org/CorpusID:250089291}
}

@article{Mirchandani2023LargeLM,
  title={Large Language Models as General Pattern Machines},
  author={Suvir Mirchandani and F. Xia and Peter R. Florence and Brian Ichter and Danny Driess and Montse Gonzalez Arenas and Kanishka Rao and Dorsa Sadigh and Andy Zeng},
  journal={ArXiv},
  year={2023},
  volume={abs/2307.04721},
  url={https://api.semanticscholar.org/CorpusID:259501163}
}

@inproceedings{John2003RavenPM,
  title={Raven Progressive Matrices},
  author={John Carlyle Raven},
  year={1938},
  url={https://api.semanticscholar.org/CorpusID:61756432}
}

@article{Ahrabian2024TheCC,
  title={The Curious Case of Nonverbal Abstract Reasoning with Multi-Modal Large Language Models},
  author={Kian Ahrabian and Zhivar Sourati and Kexuan Sun and Jiarui Zhang and Yifan Jiang and Fred Morstatter and Jay Pujara},
  journal={ArXiv},
  year={2024},
  volume={abs/2401.12117},
  url={https://api.semanticscholar.org/CorpusID:267068363}
}

@inproceedings{Gendron2023LargeLM,
  title={Large Language Models Are Not Strong Abstract Reasoners},
  author={Ga{\"e}l Gendron and Qiming Bao and M. Witbrock and Gillian Dobbie},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:258988045}
}

@inproceedings{Sourati2023ARNAR,
  title={ARN: Analogical Reasoning on Narratives},
  author={Zhivar Sourati and Filip Ilievski and Pia Sommerauer},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:263605669}
}

@article{Mitchell_2021,
   title={Abstraction and analogy‐making in artificial intelligence},
   volume={1505},
   ISSN={1749-6632},
   url={http://dx.doi.org/10.1111/nyas.14619},
   DOI={10.1111/nyas.14619},
   number={1},
   journal={Annals of the New York Academy of Sciences},
   publisher={Wiley},
   author={Mitchell, Melanie},
   year={2021},
   month=jun, pages={79–101} 
}

@article{hofstadter2001,
    author = {Douglas Hofstadter},
    title = {Analogy as the Core of Cognition},
    journal = {MIT Press},
    year = {2001}
}

@inproceedings{Bongard1970PatternR,
  title={Pattern recognition},
  author={M. M. Bongard},
  year={1970},
  url={https://api.semanticscholar.org/CorpusID:267827262}
}

@inproceedings{Raven2016RavenSP,
  title={Raven Standard Progressive Matrices},
  author={John Carlyle Raven},
  year={2016},
  url={https://api.semanticscholar.org/CorpusID:149749549}
}

@article{Santoro2018MeasuringAR,
  title={Measuring abstract reasoning in neural networks},
  author={Adam Santoro and Felix Hill and David G. T. Barrett and Ari S. Morcos and Timothy P. Lillicrap},
  journal={ArXiv},
  year={2018},
  volume={abs/1807.04225},
  url={https://api.semanticscholar.org/CorpusID:49665167}
}

@article{Zhang2019RAVENAD,
  title={RAVEN: A Dataset for Relational and Analogical Visual REasoNing},
  author={Chi Zhang and Feng Gao and Baoxiong Jia and Yixin Zhu and Song-Chun Zhu},
  journal={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2019},
  pages={5312-5322},
  url={https://api.semanticscholar.org/CorpusID:71148268}
}

@article{Kumar2024ComparingAI,
  title={Comparing Abstraction in Humans and Large Language Models Using Multimodal Serial Reproduction},
  author={Sreejan Kumar and Raja Marjieh and Byron Zhang and Declan Campbell and Michael Y. Hu and Umang Bhatt and Brenden Lake and Thomas L. Griffiths},
  journal={ArXiv},
  year={2024},
  volume={abs/2402.03618},
  url={https://api.semanticscholar.org/CorpusID:267499989}
}

@inproceedings{Holzinger2019KANDINSKYPA,
  title={KANDINSKY Patterns as IQ-Test for Machine Learning},
  author={Andreas Holzinger and Michael D. Kickmeier-Rust and Heimo M{\"u}ller},
  booktitle={International Cross-Domain Conference on Machine Learning and Knowledge Extraction},
  year={2019},
  url={https://api.semanticscholar.org/CorpusID:201616604}
}

@article{Lorello2024TheKB,
  title={The KANDY Benchmark: Incremental Neuro-Symbolic Learning and Reasoning with Kandinsky Patterns},
  author={Luca Salvatore Lorello and Marco Lippi and Stefano Melacci},
  journal={ArXiv},
  year={2024},
  volume={abs/2402.17431},
  url={https://api.semanticscholar.org/CorpusID:268033655}
}

@article{Zhang2021ACREAC,
  title={ACRE: Abstract Causal REasoning Beyond Covariation},
  author={Chi Zhang and Baoxiong Jia and Mark Edmonds and Song-Chun Zhu and Yixin Zhu},
  journal={2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2021},
  pages={10638-10648},
  url={https://api.semanticscholar.org/CorpusID:232380074}
}

@article{Langlois2021SerialRR,
  title={Serial reproduction reveals the geometry of visuospatial representations},
  author={Thomas A. Langlois and Nori Jacoby and Jordan W. Suchow and Thomas L. Griffiths},
  journal={Proceedings of the National Academy of Sciences of the United States of America},
  year={2021},
  volume={118},
  url={https://api.semanticscholar.org/CorpusID:232376763}
}

@inproceedings{Jacob2023FAMEFS,
  title={FAME: Flexible, Scalable Analogy Mappings Engine},
  author={Shahar Jacob and Chen Shani and Dafna Shahaf},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:259267034}
}

@article{Gentner1983StructureMappingAT,
  title={Structure-Mapping: A Theoretical Framework for Analogy},
  author={Dedre Gentner},
  journal={Cogn. Sci.},
  year={1983},
  volume={7},
  pages={155-170},
  url={https://api.semanticscholar.org/CorpusID:5371492}
}

@article{Falkenhainer1989TheSE,
  title={The Structure-Mapping Engine: Algorithm and Examples},
  author={Brian Falkenhainer and Kenneth D. Forbus and Dedre Gentner},
  journal={Artif. Intell.},
  year={1989},
  volume={41},
  pages={1-63},
  url={https://api.semanticscholar.org/CorpusID:8751960}
}

@article{Turney2008TheLR,
  title={The Latent Relation Mapping Engine: Algorithm and Experiments},
  author={Peter D. Turney},
  journal={J. Artif. Intell. Res.},
  year={2008},
  volume={33},
  pages={615-655},
  url={https://api.semanticscholar.org/CorpusID:7112602}
}

@article{kaufman2009fourc,
author = {James C. Kaufman and Ronald A. Beghetto},
title ={Beyond Big and Little: The Four C Model of Creativity},
journal = {Review of General Psychology},
volume = {13},
number = {1},
pages = {1-12},
year = {2009},
doi = {10.1037/a0013688},
URL = {https://doi.org/10.1037/a0013688},
eprint = { https://doi.org/10.1037/a0013688},
abstract = { Most investigations of creativity tend to take one of two directions: everyday creativity (also called “little-c”), which can be found in nearly all people, and eminent creativity (also called “Big-C”), which is reserved for the great. In this paper, the authors propose a Four C model of creativity that expands this dichotomy. Specifically, the authors add the idea of “mini-c,” creativity inherent in the learning process, and Pro-c, the developmental and effortful progression beyond little-c that represents professional-level expertise in any creative area. The authors include different transitions and gradations of these four dimensions of creativity, and then discuss advantages and examples of the Four C Model. }
}

@book{wallach1965modes,
  title={Modes of Thinking in Young Children: A Study of the Creativity-intelligence Distinction},
  author={Wallach, M.A. and Kogan, N.},
  isbn={9780030525957},
  lccn={65021085},
  series={Modes of Thinking in Young Children: A Study of the Creativity-intelligence Distinction},
  url={https://books.google.ch/books?id=rAoNAAAAIAAJ},
  year={1965},
  publisher={Holt, Rinehart and Winston}
}

@article{pease2002,
author = {Pease, Alison and Winterstein, Daniel and Colton, Simon},
year = {2002},
month = {10},
pages = {},
title = {Evaluating Machine Creativity}
}

@article{Ritchie2007SomeEC,
  title={Some Empirical Criteria for Attributing Creativity to a Computer Program},
  author={Graeme D. Ritchie},
  journal={Minds and Machines},
  year={2007},
  volume={17},
  pages={67-99},
  url={https://api.semanticscholar.org/CorpusID:7947377}
}

@misc{nath2024characterisingcreativeprocesshumans,
      title={Characterising the Creative Process in Humans and Large Language Models}, 
      author={Surabhi S. Nath and Peter Dayan and Claire Stevenson},
      year={2024},
      eprint={2405.00899},
      archivePrefix={arXiv},
      primaryClass={cs.HC},
      url={https://arxiv.org/abs/2405.00899}, 
}

@article{baas2013,
author = {Baas, Matthijs and Roskes, Marieke and Sligte, Daniel and Nijstad, Bernard and De Dreu, Carsten},
year = {2013},
month = {10},
pages = {},
title = {Personality and Creativity: The Dual Pathway to Creativity Model and a Research Agenda},
volume = {7},
journal = {Social and Personality Psychology Compass},
doi = {10.1111/spc3.12062}
}

@article{Nijstad2010TheDP,
  title={The dual pathway to creativity model: Creative ideation as a function of flexibility and persistence},
  author={Bernard A. Nijstad and C. D. De Dreu and Eric F. Rietzschel and Matthijs Baas},
  journal={European Review of Social Psychology},
  year={2010},
  volume={21},
  pages={34 - 77},
  url={https://api.semanticscholar.org/CorpusID:145290967}
}

@article{Yuan2022WordcraftSW,
  title={Wordcraft: Story Writing With Large Language Models},
  author={Ann Yuan and Andy Coenen and Emily Reif and Daphne Ippolito},
  journal={Proceedings of the 27th International Conference on Intelligent User Interfaces},
  year={2022},
  url={https://api.semanticscholar.org/CorpusID:247585187}
}

@article{Qiao2022ReasoningWL,
  title={Reasoning with Language Model Prompting: A Survey},
  author={Shuofei Qiao and Yixin Ou and Ningyu Zhang and Xiang Chen and Yunzhi Yao and Shumin Deng and Chuanqi Tan and Fei Huang and Huajun Chen},
  journal={ArXiv},
  year={2022},
  volume={abs/2212.09597},
  url={https://api.semanticscholar.org/CorpusID:254854219}
}

@inproceedings{meister-etal-2022-high,
    title = "On the probability{--}quality paradox in language generation",
    author = "Meister, Clara  and
      Wiher, Gian  and
      Pimentel, Tiago  and
      Cotterell, Ryan",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-short.5",
    doi = "10.18653/v1/2022.acl-short.5",
    pages = "36--45",
    abstract = "When generating natural language from neural probabilistic models, high probability does not always coincide with high quality: It has often been observed that mode-seeking decoding methods, i.e., those that produce high-probability text under the model, lead to unnatural language. On the other hand, the lower-probability text generated by stochastic methods is perceived as more human-like. In this note, we offer an explanation for this phenomenon by analyzing language generation through an information-theoretic lens. Specifically, we posit that human-like language should contain an amount of information (quantified as negative log-probability) that is close to the entropy of the distribution over natural strings. Further, we posit that language with substantially more (or less) information is undesirable. We provide preliminary empirical evidence in favor of this hypothesis; quality ratings of both human and machine-generated text{---}covering multiple tasks and common decoding strategies{---}suggest high-quality text has an information content significantly closer to the entropy than we would expect by chance.",
}

@article{meister-etal-2023-locally,
    title = "Locally Typical Sampling",
    author = "Meister, Clara  and
      Pimentel, Tiago  and
      Wiher, Gian  and
      Cotterell, Ryan",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "11",
    year = "2023",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2023.tacl-1.7",
    doi = "10.1162/tacl_a_00536",
    pages = "102--121",
    abstract = "Today{'}s probabilistic language generators fall short when it comes to producing coherent and fluent text despite the fact that the underlying models perform well under standard metrics (e.g., perplexity). This discrepancy has puzzled the language generation community for the last few years. In this work, we posit that the abstraction of natural language generation as a discrete stochastic process{---}which allows for an information-theoretic analysis{---}can provide new insights into the behavior of probabilistic language generators, for example, why high-probability texts can be dull or repetitive. Humans use language as a means of communicating information, aiming to do so in a simultaneously efficient and error-minimizing manner; in fact, psycholinguistics research suggests humans choose each word in a string with this subconscious goal in mind. We formally define the set of strings that meet this criterion: Those for which each word has an information content close to the expected information content, namely, the conditional entropy of our model. We then propose a simple and efficient procedure for enforcing this criterion when generating from probabilistic models, which we call locally typical sampling. Automatic and human evaluations show that, in comparison to nucleus and top-k sampling, locally typical sampling offers competitive performance (in both abstractive summarization and story generation) in terms of quality while consistently reducing degenerate repetitions.",
}

@article{Raskin1994NonliteralnessAN,
  title={Non-literalness and non-bona-f{\^i}de in language: An approach to formal and computational treatments of humor},
  author={Jonathan D. Raskin and Salvatore Attardo},
  journal={Pragmatics \& Cognition},
  year={1994},
  volume={2},
  pages={31-69},
  url={https://api.semanticscholar.org/CorpusID:62211347}
}

@inproceedings{Jain2024IsAF,
  title={Is AI fun? HumorDB: a curated dataset and benchmark to investigate graphical humor},
  author={Veedant Jain and Felipe dos Santos Alves Feitosa and Gabriel Kreiman},
  year={2024},
  url={https://api.semanticscholar.org/CorpusID:270619380}
}

@inproceedings{Yang2015HumorRA,
  title={Humor Recognition and Humor Anchor Extraction},
  author={Diyi Yang and Alon Lavie and Chris Dyer and Eduard H. Hovy},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2015},
  url={https://api.semanticscholar.org/CorpusID:11128248}
}

@inproceedings{Chen2018HumorRU,
  title={Humor Recognition Using Deep Learning},
  author={Peng-Yu Chen and V. Soo},
  booktitle={North American Chapter of the Association for Computational Linguistics},
  year={2018},
  url={https://api.semanticscholar.org/CorpusID:44158782}
}

@article{Tian2021HypoGenHG,
  title={HypoGen: Hyperbole Generation with Commonsense and Counterfactual Knowledge},
  author={Yufei Tian and Arvind krishna Sridhar and Nanyun Peng},
  journal={ArXiv},
  year={2021},
  volume={abs/2109.05097},
  url={https://api.semanticscholar.org/CorpusID:237491428}
}

@article{Lai2024ASO,
  title={A Survey on Automatic Generation of Figurative Language: From Rule-based Systems to Large Language Models},
  author={Huiyuan Lai and Malvina Nissim},
  journal={ACM Computing Surveys},
  year={2024},
  volume={56},
  pages={1 - 34},
  url={https://api.semanticscholar.org/CorpusID:268804894}
}

@inproceedings{Yu2018ANA,
  title={A Neural Approach to Pun Generation},
  author={Zhiwei Yu and Jiwei Tan and Xiaojun Wan},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2018},
  url={https://api.semanticscholar.org/CorpusID:51874992}
}

@inproceedings{Colton2012ComputationalCT,
  title={Computational Creativity: The Final Frontier?},
  author={Simon Colton and Geraint A. Wiggins},
  booktitle={European Conference on Artificial Intelligence},
  year={2012},
  url={https://api.semanticscholar.org/CorpusID:5880786}
}

@article{PopescuBelis2023GPoeTAL,
  title={GPoeT: a Language Model Trained for Rhyme Generation on Synthetic Data},
  author={Andrei Popescu-Belis and {\`A}lex R. Atrio and Bastien Bernath and Etienne Boisson and Teo Ferrari and Xavier Theimer-lienhard and Giorgos Vernikos},
  journal={Proceedings of the 7th Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:258486928}
}

@article{Belouadi2022ByGPT5ES,
  title={ByGPT5: End-to-End Style-conditioned Poetry Generation with Token-free Language Models},
  author={Jonas Belouadi and Steffen Eger},
  journal={ArXiv},
  year={2022},
  volume={abs/2212.10474},
  url={https://api.semanticscholar.org/CorpusID:254877406}
}

@inproceedings{Milic1970ThePU,
  title={The Possible Usefulness of Poetry Generation.},
  author={Louis T. Milic},
  year={1970},
  url={https://api.semanticscholar.org/CorpusID:59924092}
}

@inproceedings{Manurung2004AnEA,
  title={An evolutionary algorithm approach to poetry generation},
  author={Hisar Maruli Manurung},
  year={2004},
  url={https://api.semanticscholar.org/CorpusID:170162691}
}

@article{Manurung2012UsingGA,
  title={Using genetic algorithms to create meaningful poetic text},
  author={Ruli Manurung and Graeme D. Ritchie and Henry S. Thompson},
  journal={Journal of Experimental \& Theoretical Artificial Intelligence},
  year={2012},
  volume={24},
  pages={43 - 64},
  url={https://api.semanticscholar.org/CorpusID:4528781}
}

@inproceedings{Oliveira2012PoeTryMeA,
  title={PoeTryMe : a versatile platform for poetry generation},
  author={Hugo Gonçalo Oliveira},
  year={2012},
  url={https://api.semanticscholar.org/CorpusID:51964516}
}

@inproceedings{Colton2012FullFACEPG,
  title={Full-FACE Poetry Generation},
  author={Simon Colton and Jacob Goodwin and Tony Veale},
  booktitle={International Conference on Innovative Computing and Cloud Computing},
  year={2012},
  url={https://api.semanticscholar.org/CorpusID:2218980}
}

@inproceedings{Zhang2014ChinesePG,
  title={Chinese Poetry Generation with Recurrent Neural Networks},
  author={Xingxing Zhang and Mirella Lapata},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2014},
  url={https://api.semanticscholar.org/CorpusID:12964363}
}

@inproceedings{Lau2018DeepspeareAJ,
  title={Deep-speare: A joint neural model of poetic language, meter and rhyme},
  author={Jey Han Lau and Trevor Cohn and Timothy Baldwin and Julian Brooke and Adam Hammond},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2018},
  url={https://api.semanticscholar.org/CorpusID:49671404}
}

@inproceedings{Oliveira2017ASO,
  title={A Survey on Intelligent Poetry Generation: Languages, Features, Techniques, Reutilisation and Evaluation},
  author={Hugo Gonçalo Oliveira},
  booktitle={International Conference on Natural Language Generation},
  year={2017},
  url={https://api.semanticscholar.org/CorpusID:32461868}
}

@article{Elzohbi2023CreativeDG,
  title={Creative Data Generation: A Review Focusing on Text and Poetry},
  author={Mohamad Elzohbi and Richard Zhao},
  journal={ArXiv},
  year={2023},
  volume={abs/2305.08493},
  url={https://api.semanticscholar.org/CorpusID:258686736}
}

@misc{wang2024aicreativehumans,
      title={Can AI Be as Creative as Humans?}, 
      author={Haonan Wang and James Zou and Michael Mozer and Anirudh Goyal and Alex Lamb and Linjun Zhang and Weijie J Su and Zhun Deng and Michael Qizhe Xie and Hannah Brown and Kenji Kawaguchi},
      year={2024},
      eprint={2401.01623},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2401.01623}, 
}

@article{Hubert2024TheCS,
  title={The current state of artificial intelligence generative language models is more creative than humans on divergent thinking tasks},
  author={Kent F Hubert and Kim N. Awa and Darya L. Zabelina},
  journal={Scientific Reports},
  year={2024},
  volume={14},
  url={https://api.semanticscholar.org/CorpusID:267616181}
}

@article{Koivisto2023BestHS,
  title={Best humans still outperform artificial intelligence in a creative divergent thinking task},
  author={Mika Koivisto and Simone Grassini},
  journal={Scientific Reports},
  year={2023},
  volume={13},
  url={https://api.semanticscholar.org/CorpusID:261883326}
}

@inproceedings{GesPushingGC,
  title={Pushing GPT’s Creativity to Its Limits: Alternative Uses and Torrance Tests},
  author={Fabr{\'i}cio G{\'o}es and Marco Volpe and Piotr Sawicki and Marek Grze´s and Jacob Watson},
  url={https://api.semanticscholar.org/CorpusID:260844252},
  year={2023}
}

@article{Guzik2023TheOO,
  title={The Originality of Machines: AI Takes the Torrance Test.},
  author={Erik E. Guzik and Christian Byrge and Christian Gilde},
  journal={Journal of Creativity},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:261087185}
}

@article{Vinchon2023ArtificialI,
  title={Artificial Intelligence \& Creativity: A Manifesto for Collaboration},
  author={Florent Vinchon and Todd Lubart and Sabrina Bartolotta and Valentin Gironnay and Marion Botella and Samira Bourgeois-Bougrine and Jean-Marie Burkhardt and Nathalie Bonnardel and Giovanni Emanuele Corazza and Vlad Petre Glăveanu and Michael Hanchett Hanson and Zorana Ivcevic and Maciej Karwowski and J. Kaufman and Takeshi Okada and Roni Reiter‐Palmon and Andrea Gaggioli},
  journal={The Journal of Creative Behavior},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:259480496}
}

@article{Nair2024CreativePS,
  title={Creative Problem Solving in Large Language and Vision Models - What Would it Take?},
  author={Lakshmi Nair and Evana Gizzi and Jivko Sinapov},
  journal={ArXiv},
  year={2024},
  volume={abs/2405.01453},
  url={https://api.semanticscholar.org/CorpusID:269502505}
}

@article{Zhao2024AssessingAU,
  title={Assessing and Understanding Creativity in Large Language Models},
  author={Yunpu Zhao and Rui Zhang and Wenyi Li and Di Huang and Jiaming Guo and Shaohui Peng and Yifan Hao and Yuanbo Wen and Xingui Hu and Zidong Du and Qi Guo and Ling Li and Yunji Chen},
  journal={ArXiv},
  year={2024},
  volume={abs/2401.12491},
  url={https://api.semanticscholar.org/CorpusID:267094860}
}

@misc{mohammadi2024creativityleftchatprice,
      title={Creativity Has Left the Chat: The Price of Debiasing Language Models}, 
      author={Behnam Mohammadi},
      year={2024},
      eprint={2406.05587},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.05587}, 
}

@misc{franceschelli2023creativitylargelanguagemodels,
      title={On the Creativity of Large Language Models}, 
      author={Giorgio Franceschelli and Mirco Musolesi},
      year={2023},
      eprint={2304.00008},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2304.00008}, 
}

@inproceedings{Osborn1957AppliedI,
  title={Applied imagination : principles and procedures of creative problem-solving},
  author={Alexander Faickney Osborn},
  year={1957},
  url={https://api.semanticscholar.org/CorpusID:142858886}
}

@misc{riley2019evidencethreateningsituationsenhance,
      title={Evidence that Threatening Situations Enhance Creativity}, 
      author={Sean N. Riley and Liane Gabora},
      year={2019},
      eprint={1308.4245},
      archivePrefix={arXiv},
      primaryClass={q-bio.NC},
      url={https://arxiv.org/abs/1308.4245}, 
}

@article{weisberg2015,
author = {Robert W. Weisberg},
title = {On the Usefulness of “Value” in the Definition of Creativity},
journal = {Creativity Research Journal},
volume = {27},
number = {2},
pages = {111--124},
year = {2015},
publisher = {Routledge},
doi = {10.1080/10400419.2015.1030320},
URL = { https://doi.org/10.1080/10400419.2015.1030320},
eprint = { https://doi.org/10.1080/10400419.2015.1030320}
}

@article{brandt2021,
author = {Anthony Brandt},
title = {Defining Creativity: A View from the Arts},
journal = {Creativity Research Journal},
volume = {33},
number = {2},
pages = {81--95},
year = {2021},
publisher = {Routledge},
doi = {10.1080/10400419.2020.1855905},
URL = {https://doi.org/10.1080/10400419.2020.1855905},
eprint = { https://doi.org/10.1080/10400419.2020.1855905}
}

@book{perkins2001eureka,
  title={The Eureka Effect: The Art and Logic of Breakthrough Thinking},
  author={Perkins, D.N.},
  isbn={9780393322552},
  lccn={00020012},
  url={https://books.google.ch/books?id=84HVnfUywZoC},
  year={2001},
  publisher={W. W. Norton, Incorporated}
}


@article{simonton2012,
author = {Dean Keith Simonton},
title = {Taking the U.S. Patent Office Criteria Seriously: A Quantitative Three-Criterion Creativity Definition and Its Implications},
journal = {Creativity Research Journal},
volume = {24},
number = {2-3},
pages = {97--106},
year = {2012},
publisher = {Routledge},
doi = {10.1080/10400419.2012.676974},
URL = {https://doi.org/10.1080/10400419.2012.676974},
eprint = { https://doi.org/10.1080/10400419.2012.676974}
}

@book{weisberg1986creativity,
  title={Creativity: Genius and Other Myths},
  author={Weisberg, R.W.},
  isbn={9780716717690},
  lccn={86004263},
  series={Books in psychology},
  url={https://books.google.ch/books?id=xiCLQgAACAAJ},
  year={1986},
  publisher={W.H. Freeman}
}

@article{Goodfellow2014GenerativeAN,
  title={Generative adversarial networks},
  author={Ian J. Goodfellow and Jean Pouget-Abadie and Mehdi Mirza and Bing Xu and David Warde-Farley and Sherjil Ozair and Aaron C. Courville and Yoshua Bengio},
  journal={Communications of the ACM},
  year={2014},
  volume={63},
  pages={139 - 144},
  url={https://api.semanticscholar.org/CorpusID:1033682}
}

@article{lecun1989,
  author={LeCun, Y. and Boser, B. and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
  journal={Neural Computation}, 
  title={Backpropagation Applied to Handwritten Zip Code Recognition}, 
  year={1989},
  volume={1},
  number={4},
  pages={541-551},
  keywords={},
  doi={10.1162/neco.1989.1.4.541}
}

@article{Gatys2015ANA,
  title={A Neural Algorithm of Artistic Style},
  author={Leon A. Gatys and Alexander S. Ecker and Matthias Bethge},
  journal={ArXiv},
  year={2015},
  volume={abs/1508.06576},
  url={https://api.semanticscholar.org/CorpusID:13914930}
}

@article{Oord2016ConditionalIG,
  title={Conditional Image Generation with PixelCNN Decoders},
  author={A{\"a}ron van den Oord and Nal Kalchbrenner and Lasse Espeholt and Koray Kavukcuoglu and Oriol Vinyals and Alex Graves},
  journal={ArXiv},
  year={2016},
  volume={abs/1606.05328},
  url={https://api.semanticscholar.org/CorpusID:14989939}
}

@inproceedings{Oord2016PixelRN,
  title={Pixel Recurrent Neural Networks},
  author={A{\"a}ron van den Oord and Nal Kalchbrenner and Koray Kavukcuoglu},
  booktitle={International Conference on Machine Learning},
  year={2016},
  url={https://api.semanticscholar.org/CorpusID:8142135}
}

@article{Dumoulin2016ALR,
  title={A Learned Representation For Artistic Style},
  author={Vincent Dumoulin and Jonathon Shlens and Manjunath Kudlur},
  journal={ArXiv},
  year={2016},
  volume={abs/1610.07629},
  url={https://api.semanticscholar.org/CorpusID:5687613}
}

@article{Li2016CombiningMR,
  title={Combining Markov Random Fields and Convolutional Neural Networks for Image Synthesis},
  author={Chuan Li and Michael Wand},
  journal={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2016},
  pages={2479-2486},
  url={https://api.semanticscholar.org/CorpusID:6635779}
}

@article{Ledig2016PhotoRealisticSI,
  title={Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network},
  author={Christian Ledig and Lucas Theis and Ferenc Husz{\'a}r and Jose Caballero and Andrew P. Aitken and Alykhan Tejani and Johannes Totz and Zehan Wang and Wenzhe Shi},
  journal={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2016},
  pages={105-114},
  url={https://api.semanticscholar.org/CorpusID:211227}
}

@article{Karras2018ASG,
  title={A Style-Based Generator Architecture for Generative Adversarial Networks},
  author={Tero Karras and Samuli Laine and Timo Aila},
  journal={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2018},
  pages={4396-4405},
  url={https://api.semanticscholar.org/CorpusID:54482423}
}

@article{Karras2019AnalyzingAI,
  title={Analyzing and Improving the Image Quality of StyleGAN},
  author={Tero Karras and Samuli Laine and Miika Aittala and Janne Hellsten and Jaakko Lehtinen and Timo Aila},
  journal={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2019},
  pages={8107-8116},
  url={https://api.semanticscholar.org/CorpusID:209202273}
}

@article{Radford2015UnsupervisedRL,
  title={Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks},
  author={Alec Radford and Luke Metz and Soumith Chintala},
  journal={CoRR},
  year={2015},
  volume={abs/1511.06434},
  url={https://api.semanticscholar.org/CorpusID:11758569}
}

@inproceedings{Dong2014LearningAD,
  title={Learning a Deep Convolutional Network for Image Super-Resolution},
  author={Chao Dong and Chen Change Loy and Kaiming He and Xiaoou Tang},
  booktitle={European Conference on Computer Vision},
  year={2014},
  url={https://api.semanticscholar.org/CorpusID:18874645}
}

@inproceedings{Zhang2016ColorfulIC,
  title={Colorful Image Colorization},
  author={Richard Zhang and Phillip Isola and Alexei A. Efros},
  booktitle={European Conference on Computer Vision},
  year={2016},
  url={https://api.semanticscholar.org/CorpusID:50698}
}

@article{Pathak2016ContextEF,
  title={Context Encoders: Feature Learning by Inpainting},
  author={Deepak Pathak and Philipp Kr{\"a}henb{\"u}hl and Jeff Donahue and Trevor Darrell and Alexei A. Efros},
  journal={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2016},
  pages={2536-2544},
  url={https://api.semanticscholar.org/CorpusID:2202933}
}

@inproceedings{Huang2018MultimodalUI,
  title={Multimodal Unsupervised Image-to-Image Translation},
  author={Xun Huang and Ming-Yu Liu and Serge J. Belongie and Jan Kautz},
  booktitle={European Conference on Computer Vision},
  year={2018},
  url={https://api.semanticscholar.org/CorpusID:4883312}
}

@article{Abdal2019Image2StyleGANHT,
  title={Image2StyleGAN: How to Embed Images Into the StyleGAN Latent Space?},
  author={Rameen Abdal and Yipeng Qin and Peter Wonka},
  journal={2019 IEEE/CVF International Conference on Computer Vision (ICCV)},
  year={2019},
  pages={4431-4440},
  url={https://api.semanticscholar.org/CorpusID:102350964}
}

@article{Isola2016ImagetoImageTW,
  title={Image-to-Image Translation with Conditional Adversarial Networks},
  author={Phillip Isola and Jun-Yan Zhu and Tinghui Zhou and Alexei A. Efros},
  journal={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2016},
  pages={5967-5976},
  url={https://api.semanticscholar.org/CorpusID:6200260}
}

@article{Richardson2020EncodingIS,
  title={Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation},
  author={Elad Richardson and Yuval Alaluf and Or Patashnik and Yotam Nitzan and Yaniv Azar and Stav Shapiro and Daniel Cohen-Or},
  journal={2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2020},
  pages={2287-2296},
  url={https://api.semanticscholar.org/CorpusID:220936362}
}

@article{Johnson2016PerceptualLF,
  title={Perceptual Losses for Real-Time Style Transfer and Super-Resolution},
  author={Justin Johnson and Alexandre Alahi and Li Fei-Fei},
  journal={ArXiv},
  year={2016},
  volume={abs/1603.08155},
  url={https://api.semanticscholar.org/CorpusID:980236}
}

@article{Zhang2016StackGANTT,
  title={StackGAN: Text to Photo-Realistic Image Synthesis with Stacked Generative Adversarial Networks},
  author={Han Zhang and Tao Xu and Hongsheng Li and Shaoting Zhang and Xiaogang Wang and Xiaolei Huang and Dimitris N. Metaxas},
  journal={2017 IEEE International Conference on Computer Vision (ICCV)},
  year={2016},
  pages={5908-5916},
  url={https://api.semanticscholar.org/CorpusID:1277217}
}

@inproceedings{Reed2016GenerativeAT,
  title={Generative Adversarial Text to Image Synthesis},
  author={Scott E. Reed and Zeynep Akata and Xinchen Yan and Lajanugen Logeswaran and Bernt Schiele and Honglak Lee},
  booktitle={International Conference on Machine Learning},
  year={2016},
  url={https://api.semanticscholar.org/CorpusID:1563370}
}

@article{Reed2016LearningWA,
  title={Learning What and Where to Draw},
  author={Scott E. Reed and Zeynep Akata and Santosh Mohan and Samuel Tenka and Bernt Schiele and Honglak Lee},
  journal={ArXiv},
  year={2016},
  volume={abs/1610.02454},
  url={https://api.semanticscholar.org/CorpusID:1515901}
}

@article{Mansimov2015GeneratingIF,
  title={Generating Images from Captions with Attention},
  author={Elman Mansimov and Emilio Parisotto and Jimmy Ba and Ruslan Salakhutdinov},
  journal={CoRR},
  year={2015},
  volume={abs/1511.02793},
  url={https://api.semanticscholar.org/CorpusID:9996719}
}

@inproceedings{Yan2015Attribute2ImageCI,
  title={Attribute2Image: Conditional Image Generation from Visual Attributes},
  author={Xinchen Yan and Jimei Yang and Kihyuk Sohn and Honglak Lee},
  booktitle={European Conference on Computer Vision},
  year={2015},
  url={https://api.semanticscholar.org/CorpusID:7577075}
}

@article{Mirza2014ConditionalGA,
  title={Conditional Generative Adversarial Nets},
  author={Mehdi Mirza and Simon Osindero},
  journal={ArXiv},
  year={2014},
  volume={abs/1411.1784},
  url={https://api.semanticscholar.org/CorpusID:12803511}
}

@article{Shen2024EmpoweringVC,
  title={Empowering Visual Creativity: A Vision-Language Assistant to Image Editing Recommendations},
  author={Tiancheng Shen and Jun Hao Liew and Long Mai and Lu Qi and Jiashi Feng and Jiaya Jia},
  journal={ArXiv},
  year={2024},
  volume={abs/2406.00121},
  url={https://api.semanticscholar.org/CorpusID:270218211}
}

@article{Ramesh2021ZeroShotTG,
  title={Zero-Shot Text-to-Image Generation},
  author={Aditya Ramesh and Mikhail Pavlov and Gabriel Goh and Scott Gray and Chelsea Voss and Alec Radford and Mark Chen and Ilya Sutskever},
  journal={ArXiv},
  year={2021},
  volume={abs/2102.12092},
  url={https://api.semanticscholar.org/CorpusID:232035663}
}

@article{Geng2023InstructDiffusionAG,
  title={InstructDiffusion: A Generalist Modeling Interface for Vision Tasks},
  author={Zigang Geng and Binxin Yang and Tiankai Hang and Chen Li and Shuyang Gu and Ting Zhang and Jianmin Bao and Zheng Zhang and Han Hu and Dongdong Chen and Baining Guo},
  journal={2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2023},
  pages={12709-12720},
  url={https://api.semanticscholar.org/CorpusID:261582721}
}

@article{Ramesh2022HierarchicalTI,
  title={Hierarchical Text-Conditional Image Generation with CLIP Latents},
  author={Aditya Ramesh and Prafulla Dhariwal and Alex Nichol and Casey Chu and Mark Chen},
  journal={ArXiv},
  year={2022},
  volume={abs/2204.06125},
  url={https://api.semanticscholar.org/CorpusID:248097655}
}

@article{Saharia2022PhotorealisticTD,
  title={Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding},
  author={Chitwan Saharia and William Chan and Saurabh Saxena and Lala Li and Jay Whang and Emily L. Denton and Seyed Kamyar Seyed Ghasemipour and Burcu Karagol Ayan and Seyedeh Sara Mahdavi and Raphael Gontijo Lopes and Tim Salimans and Jonathan Ho and David J. Fleet and Mohammad Norouzi},
  journal={ArXiv},
  year={2022},
  volume={abs/2205.11487},
  url={https://api.semanticscholar.org/CorpusID:248986576}
}

@article{Rombach2021HighResolutionIS,
  title={High-Resolution Image Synthesis with Latent Diffusion Models},
  author={Robin Rombach and A. Blattmann and Dominik Lorenz and Patrick Esser and Bj{\"o}rn Ommer},
  journal={2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2021},
  pages={10674-10685},
  url={https://api.semanticscholar.org/CorpusID:245335280}
}

@inproceedings{Nichol2021GLIDETP,
  title={GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models},
  author={Alex Nichol and Prafulla Dhariwal and Aditya Ramesh and Pranav Shyam and Pamela Mishkin and Bob McGrew and Ilya Sutskever and Mark Chen},
  booktitle={International Conference on Machine Learning},
  year={2021},
  url={https://api.semanticscholar.org/CorpusID:245335086}
}

@article{Gafni2022MakeASceneST,
  title={Make-A-Scene: Scene-Based Text-to-Image Generation with Human Priors},
  author={Oran Gafni and Adam Polyak and Oron Ashual and Shelly Sheynin and Devi Parikh and Yaniv Taigman},
  journal={ArXiv},
  year={2022},
  volume={abs/2203.13131},
  url={https://api.semanticscholar.org/CorpusID:247628171}
}

@article{Hertz2022PrompttoPromptIE,
  title={Prompt-to-Prompt Image Editing with Cross Attention Control},
  author={Amir Hertz and Ron Mokady and Jay M. Tenenbaum and Kfir Aberman and Yael Pritch and Daniel Cohen-Or},
  journal={ArXiv},
  year={2022},
  volume={abs/2208.01626},
  url={https://api.semanticscholar.org/CorpusID:251252882}
}

@article{Huang2024CreativeSynthCB,
  title={CreativeSynth: Creative Blending and Synthesis of Visual Arts based on Multimodal Diffusion},
  author={Nisha Huang and Weiming Dong and Yuxin Zhang and Fan Tang and Ronghui Li and Chongyang Ma and Xiu Li and Changsheng Xu},
  journal={ArXiv},
  year={2024},
  volume={abs/2401.14066},
  url={https://api.semanticscholar.org/CorpusID:267211817}
}

@article{Ruiz2022DreamBoothFT,
  title={DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation},
  author={Nataniel Ruiz and Yuanzhen Li and Varun Jampani and Yael Pritch and Michael Rubinstein and Kfir Aberman},
  journal={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2022},
  pages={22500-22510},
  url={https://api.semanticscholar.org/CorpusID:251800180}
}

@article{Brooks2022InstructPix2PixLT,
  title={InstructPix2Pix: Learning to Follow Image Editing Instructions},
  author={Tim Brooks and Aleksander Holynski and Alexei A. Efros},
  journal={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2022},
  pages={18392-18402},
  url={https://api.semanticscholar.org/CorpusID:253581213}
}

@article{Patashnik2021StyleCLIPTM,
  title={StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery},
  author={Or Patashnik and Zongze Wu and Eli Shechtman and Daniel Cohen-Or and Dani Lischinski},
  journal={2021 IEEE/CVF International Conference on Computer Vision (ICCV)},
  year={2021},
  pages={2065-2074},
  url={https://api.semanticscholar.org/CorpusID:232428282}
}

@article{Huang2022DrawYA,
  title={Draw Your Art Dream: Diverse Digital Art Synthesis with Multimodal Guided Diffusion},
  author={Nisha Huang and Fan Tang and Weiming Dong and Changsheng Xu},
  journal={Proceedings of the 30th ACM International Conference on Multimedia},
  year={2022},
  url={https://api.semanticscholar.org/CorpusID:252544936}
}

@article{Tulyakov2017MoCoGANDM,
  title={MoCoGAN: Decomposing Motion and Content for Video Generation},
  author={S. Tulyakov and Ming-Yu Liu and Xiaodong Yang and Jan Kautz},
  journal={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2017},
  pages={1526-1535},
  url={https://api.semanticscholar.org/CorpusID:4475365}
}

@inproceedings{Vondrick2016GeneratingVW,
  title={Generating Videos with Scene Dynamics},
  author={Carl Vondrick and Hamed Pirsiavash and Antonio Torralba},
  booktitle={Neural Information Processing Systems},
  year={2016},
  url={https://api.semanticscholar.org/CorpusID:9933254}
}

@article{Singer2022MakeAVideoTG,
  title={Make-A-Video: Text-to-Video Generation without Text-Video Data},
  author={Uriel Singer and Adam Polyak and Thomas Hayes and Xiaoyue Yin and Jie An and Songyang Zhang and Qiyuan Hu and Harry Yang and Oron Ashual and Oran Gafni and Devi Parikh and Sonal Gupta and Yaniv Taigman},
  journal={ArXiv},
  year={2022},
  volume={abs/2209.14792},
  url={https://api.semanticscholar.org/CorpusID:252595919}
}

@article{Luo2023VideoFusionDD,
  title={VideoFusion: Decomposed Diffusion Models for High-Quality Video Generation},
  author={Zhengxiong Luo and Dayou Chen and Yingya Zhang and Yan Huang and Liangsheng Wang and Yujun Shen and Deli Zhao and Jinren Zhou and Tien-Ping Tan},
  journal={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2023},
  pages={10209-10218},
  url={https://api.semanticscholar.org/CorpusID:257532642}
}

@article{Ho2022VideoDM,
  title={Video Diffusion Models},
  author={Jonathan Ho and Tim Salimans and Alexey Gritsenko and William Chan and Mohammad Norouzi and David J. Fleet},
  journal={ArXiv},
  year={2022},
  volume={abs/2204.03458},
  url={https://api.semanticscholar.org/CorpusID:248006185}
}

@article{Ho2022ImagenVH,
  title={Imagen Video: High Definition Video Generation with Diffusion Models},
  author={Jonathan Ho and William Chan and Chitwan Saharia and Jay Whang and Ruiqi Gao and Alexey A. Gritsenko and Diederik P. Kingma and Ben Poole and Mohammad Norouzi and David J. Fleet and Tim Salimans},
  journal={ArXiv},
  year={2022},
  volume={abs/2210.02303},
  url={https://api.semanticscholar.org/CorpusID:252715883}
}

@article{Xing2023ASO,
  title={A Survey on Video Diffusion Models},
  author={Zhen Xing and Qijun Feng and Haoran Chen and Qi Dai and Hang-Rui Hu and Hang Xu and Zuxuan Wu and Yu-Gang Jiang},
  journal={ArXiv},
  year={2023},
  volume={abs/2310.10647},
  url={https://api.semanticscholar.org/CorpusID:264172934}
}

@article{Arnab2021ViViTAV,
  title={ViViT: A Video Vision Transformer},
  author={Anurag Arnab and Mostafa Dehghani and Georg Heigold and Chen Sun and Mario Lucic and Cordelia Schmid},
  journal={2021 IEEE/CVF International Conference on Computer Vision (ICCV)},
  year={2021},
  pages={6816-6826},
  url={https://api.semanticscholar.org/CorpusID:232417054}
}

@article{Yan2021VideoGPTVG,
  title={VideoGPT: Video Generation using VQ-VAE and Transformers},
  author={Wilson Yan and Yunzhi Zhang and P. Abbeel and A. Srinivas},
  journal={ArXiv},
  year={2021},
  volume={abs/2104.10157},
  url={https://api.semanticscholar.org/CorpusID:233307257}
}

@article{Kondratyuk2023VideoPoetAL,
  title={VideoPoet: A Large Language Model for Zero-Shot Video Generation},
  author={D. Kondratyuk and Lijun Yu and Xiuye Gu and Jos{\'e} Lezama and Jonathan Huang and Rachel Hornung and Hartwig Adam and Hassan Akbari and Yair Alon and Vighnesh Birodkar and Yong Cheng and Ming-Chang Chiu and Josh Dillon and Irfan Essa and Agrim Gupta and Meera Hahn and Anja Hauth and David Hendon and Alonso Martinez and David C. Minnen and David A. Ross and Grant Schindler and Mikhail Sirotenko and Kihyuk Sohn and Krishna Somandepalli and Huisheng Wang and Jimmy Yan and Ming Yang and Xuan Yang and Bryan Seybold and Lu Jiang},
  journal={ArXiv},
  year={2023},
  volume={abs/2312.14125},
  url={https://api.semanticscholar.org/CorpusID:266435847}
}

@article{Gupta2023PhotorealisticVG,
  title={Photorealistic Video Generation with Diffusion Models},
  author={Agrim Gupta and Lijun Yu and Kihyuk Sohn and Xiuye Gu and Meera Hahn and Fei-Fei Li and Irfan Essa and Lu Jiang and Jos{\'e} Lezama},
  journal={ArXiv},
  year={2023},
  volume={abs/2312.06662},
  url={https://api.semanticscholar.org/CorpusID:266163109}
}

@article{Huang2023T2ICompBenchAC,
  title={T2I-CompBench: A Comprehensive Benchmark for Open-world Compositional Text-to-image Generation},
  author={Kaiyi Huang and Kaiyue Sun and Enze Xie and Zhenguo Li and Xihui Liu},
  journal={ArXiv},
  year={2023},
  volume={abs/2307.06350},
  url={https://api.semanticscholar.org/CorpusID:259847295}
}

@article{Zarei2024UnderstandingAM,
  title={Understanding and Mitigating Compositional Issues in Text-to-Image Generative Models},
  author={Arman Zarei and Keivan Rezaei and Samyadeep Basu and Mehrdad Saberi and Mazda Moayeri and Priyatham Kattakinda and Soheil Feizi},
  journal={ArXiv},
  year={2024},
  volume={abs/2406.07844},
  url={https://api.semanticscholar.org/CorpusID:270391565}
}

@article{Marcus2022AVP,
  title={A very preliminary analysis of DALL-E 2},
  author={Gary Marcus and Ernest Davis and Scott Aaronson},
  journal={ArXiv},
  year={2022},
  volume={abs/2204.13807},
  url={https://api.semanticscholar.org/CorpusID:248476147}
}

@article{Murphy2024ACI,
  title={A Comparative Investigation of Compositional Syntax and Semantics in DALL-E 2},
  author={Elliot Murphy and Jill de Villiers and Sofia Morales},
  journal={ArXiv},
  year={2024},
  volume={abs/2403.12294},
  url={https://api.semanticscholar.org/CorpusID:268531835}
}

@article{Thrush2022WinogroundPV,
  title={Winoground: Probing Vision and Language Models for Visio-Linguistic Compositionality},
  author={Tristan Thrush and Ryan Jiang and Max Bartolo and Amanpreet Singh and Adina Williams and Douwe Kiela and Candace Ross},
  journal={2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2022},
  pages={5228-5238},
  url={https://api.semanticscholar.org/CorpusID:248006414}
}

@article{Leivada2022DALLE2F,
  title={DALL-E 2 Fails to Reliably Capture Common Syntactic Processes},
  author={Evelina Leivada and Elliot Murphy and Gary Marcus},
  journal={ArXiv},
  year={2022},
  volume={abs/2210.12889},
  url={https://api.semanticscholar.org/CorpusID:253098841}
}

@article{Conwell2022TestingRU,
  title={Testing Relational Understanding in Text-Guided Image Generation},
  author={Colin Conwell and Tomer David Ullman},
  journal={ArXiv},
  year={2022},
  volume={abs/2208.00005},
  url={https://api.semanticscholar.org/CorpusID:251224307}
}

@article{Borji2023QualitativeFO,
  title={Qualitative Failures of Image Generation Models and Their Application in Detecting Deepfakes},
  author={Ali Borji},
  journal={ArXiv},
  year={2023},
  volume={abs/2304.06470},
  url={https://api.semanticscholar.org/CorpusID:257826680}
}

@article{Rassin2022DALLE2IS,
  title={DALLE-2 is Seeing Double: Flaws in Word-to-Concept Mapping in Text2Image Models},
  author={Royi Rassin and Shauli Ravfogel and Yoav Goldberg},
  journal={ArXiv},
  year={2022},
  volume={abs/2210.10606},
  url={https://api.semanticscholar.org/CorpusID:252992545}
}

@article{Lei2024ACS,
  title={A Comprehensive Survey on Human Video Generation: Challenges, Methods, and Insights},
  author={Wen-Ling Lei and Jinting Wang and Fengji Ma and Guanjie Huang and Li Liu},
  journal={ArXiv},
  year={2024},
  volume={abs/2407.08428},
  url={https://api.semanticscholar.org/CorpusID:271097523}
}

@article{shulei2024,
author = {Ji, Shulei and Yang, Xinyu and Luo, Jing},
title = {A Survey on Deep Learning for Symbolic Music Generation: Representations, Algorithms, Evaluations, and Challenges},
year = {2023},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3597493},
doi = {10.1145/3597493},
abstract = {Significant progress has been made in symbolic music generation with the help of deep learning techniques. However, the tasks covered by symbolic music generation have not been well summarized, and the evolution of generative models for the specific music generation task has not been illustrated systematically. This paper attempts to provide a task-oriented survey of symbolic music generation based on deep learning techniques, covering most of the currently popular music generation tasks. The distinct models under the same task are set forth briefly and strung according to their motivations, basically in chronological order. Moreover, we summarize the common datasets suitable for various tasks, discuss the music representations and the evaluation methods, highlight current challenges in symbolic music generation, and finally point out potential future research directions.},
journal = {ACM Comput. Surv.},
month = {aug},
articleno = {7},
numpages = {39},
keywords = {music evaluation methods, symbolic music representations, deep learning, task-oriented survey, Symbolic music generation}
}

@article{hiller1953musical,
  title={Musical composition with a high-speed digital computer},
  author={Hiller, Lejaren and Isaacson, Leonard},
  year={1958}
}

@article{brooks1957experiment,
  title={An experiment in musical composition},
  author={Brooks, Frederick P and Hopkins, AL and Neumann, Peter G and Wright, William V},
  journal={IRE Transactions on Electronic Computers},
  number={3},
  pages={175--182},
  year={1957},
  publisher={IEEE}
}

@article{ames1987automated,
  title={Automated composition in retrospect: 1956-1986},
  author={Ames, Charles},
  journal={Leonardo},
  pages={169--185},
  year={1987},
  publisher={JSTOR}
}

@inproceedings{farbood2001analysis,
  title={Analysis and synthesis of Palestrina-style counterpoint using Markov chains},
  author={Farbood, Mary and Sch{\"o}ner, Bernd},
  booktitle={ICMC},
  year={2001}
}

@book{cope1996experiments,
  title={Experiments in musical intelligence},
  author={Cope, David},
  volume={12},
  year={1996},
  publisher={AR editions Madison, WI}
}

@inproceedings{biles1994genjam,
  title={GenJam: A genetic algorithm for generating jazz solos},
  author={Biles, John and others},
  booktitle={ICMC},
  volume={94},
  pages={131--137},
  year={1994},
  organization={Ann Arbor, MI}
}

@inproceedings{lavrenko2003polyphonic,
  title={Polyphonic music modeling with random fields},
  author={Lavrenko, Victor and Pickens, Jeremy},
  booktitle={Proceedings of the eleventh ACM international conference on Multimedia},
  pages={120--129},
  year={2003}
}

@article{todd1989connectionist,
  title={A connectionist approach to algorithmic composition},
  author={Todd, Peter M},
  journal={Computer Music Journal},
  volume={13},
  number={4},
  pages={27--43},
  year={1989},
  publisher={JSTOR}
}

@inproceedings{eck2002finding,
  title={Finding temporal structure in music: Blues improvisation with LSTM recurrent networks},
  author={Eck, Douglas and Schmidhuber, Juergen},
  booktitle={Proceedings of the 12th IEEE workshop on neural networks for signal processing},
  pages={747--756},
  year={2002},
  organization={IEEE}
}

@article{Roberts2018AHL,
  title={A Hierarchical Latent Vector Model for Learning Long-Term Structure in Music},
  author={Adam Roberts and Jesse Engel and Colin Raffel and Curtis Hawthorne and Douglas Eck},
  journal={ArXiv},
  year={2018},
  volume={abs/1803.05428},
  url={https://api.semanticscholar.org/CorpusID:3891811}
}

@article{Kingma2013AutoEncodingVB,
  title={Auto-Encoding Variational Bayes},
  author={Diederik P. Kingma and Max Welling},
  journal={CoRR},
  year={2013},
  volume={abs/1312.6114},
  url={https://api.semanticscholar.org/CorpusID:216078090}
}

@article{Yang2017MidiNetAC,
  title={MidiNet: A Convolutional Generative Adversarial Network for Symbolic-Domain Music Generation},
  author={Li-Chia Yang and Szu-Yu Chou and Yi-Hsuan Yang},
  journal={ArXiv},
  year={2017},
  volume={abs/1703.10847},
  url={https://api.semanticscholar.org/CorpusID:2002865}
}

@inproceedings{Dong2017MuseGANMS,
  title={MuseGAN: Multi-track Sequential Generative Adversarial Networks for Symbolic Music Generation and Accompaniment},
  author={Hao-Wen Dong and Wen-Yi Hsiao and Li-Chia Yang and Yi-Hsuan Yang},
  booktitle={AAAI Conference on Artificial Intelligence},
  year={2017},
  url={https://api.semanticscholar.org/CorpusID:19098155}
}

@inproceedings{Yu2016SeqGANSG,
  title={SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient},
  author={Lantao Yu and Weinan Zhang and Jun Wang and Yong Yu},
  booktitle={AAAI Conference on Artificial Intelligence},
  year={2016},
  url={https://api.semanticscholar.org/CorpusID:3439214}
}

@inproceedings{Huang2018MusicTG,
  title={Music Transformer: Generating Music with Long-Term Structure},
  author={Cheng-Zhi Anna Huang and Ashish Vaswani and Jakob Uszkoreit and Noam M. Shazeer and Ian Simon and Curtis Hawthorne and Andrew M. Dai and Matthew D. Hoffman and Monica Dinculescu and Douglas Eck},
  booktitle={International Conference on Learning Representations},
  year={2018},
  url={https://api.semanticscholar.org/CorpusID:54477714}
}

@article{Copet2023SimpleAC,
  title={Simple and Controllable Music Generation},
  author={Jade Copet and Felix Kreuk and Itai Gat and Tal Remez and David Kant and Gabriel Synnaeve and Yossi Adi and Alexandre D'efossez},
  journal={ArXiv},
  year={2023},
  volume={abs/2306.05284},
  url={https://api.semanticscholar.org/CorpusID:259108357}
}

@article{Agostinelli2023MusicLMGM,
  title={MusicLM: Generating Music From Text},
  author={Andrea Agostinelli and Timo I. Denk and Zal{\'a}n Borsos and Jesse Engel and Mauro Verzetti and Antoine Caillon and Qingqing Huang and Aren Jansen and Adam Roberts and Marco Tagliasacchi and Matthew Sharifi and Neil Zeghidour and Christian Havn{\o} Frank},
  journal={ArXiv},
  year={2023},
  volume={abs/2301.11325},
  url={https://api.semanticscholar.org/CorpusID:256274504}
}

@article{Donahue2019LakhNESIM,
  title={LakhNES: Improving Multi-instrumental Music Generation with Cross-domain Pre-training},
  author={Chris Donahue and Huanru Henry Mao and Yiting Li and G. Cottrell and Julian McAuley},
  journal={ArXiv},
  year={2019},
  volume={abs/1907.04868},
  url={https://api.semanticscholar.org/CorpusID:195886341}
}

@article{Huang2020PopMT,
  title={Pop Music Transformer: Beat-based Modeling and Generation of Expressive Pop Piano Compositions},
  author={Yu-Siang Huang and Yi-Hsuan Yang},
  journal={Proceedings of the 28th ACM International Conference on Multimedia},
  year={2020},
  url={https://api.semanticscholar.org/CorpusID:220919638}
}

@article{payne2019musenet,
  title={MuseNet, 2019},
  author={Payne, Christine},
  journal={URL https://openai. com/blog/musenet},
  year={2019}
}

@article{Yang2018OnTE,
  title={On the evaluation of generative models in music},
  author={Li-Chia Yang and Alexander Lerch},
  journal={Neural Computing and Applications},
  year={2018},
  volume={32},
  pages={4773 - 4784},
  url={https://api.semanticscholar.org/CorpusID:53258271}
}

@inproceedings{Yin2021AGA,
  title={"A Good Algorithm Does Not Steal - It Imitates": The Originality Report as a Means of Measuring When a Music Generation Algorithm Copies Too Much},
  author={Zong Yin and Federico Reuben and Susan Stepney and Tom Collins},
  booktitle={EvoMUSART},
  year={2021},
  url={https://api.semanticscholar.org/CorpusID:233236232}
}

@article{Sarmento2024BetweenTA,
  title={Between the AI and Me: Analysing Listeners' Perspectives on AI- and Human-Composed Progressive Metal Music},
  author={Pedro Sarmento and John H. Loth and Mathieu Barthet},
  journal={ArXiv},
  year={2024},
  volume={abs/2407.21615},
  url={https://api.semanticscholar.org/CorpusID:271571547}
}

@article{Ji2020ACS,
  title={A Comprehensive Survey on Deep Music Generation: Multi-level Representations, Algorithms, Evaluations, and Future Directions},
  author={Shulei Ji and Jing Luo and Xinyu Yang},
  journal={ArXiv},
  year={2020},
  volume={abs/2011.06801},
  url={https://api.semanticscholar.org/CorpusID:226956064}
}

@article{Kramer2023AutomatedSD,
  title={Automated Scientific Discovery: From Equation Discovery to Autonomous Discovery Systems},
  author={Stefan Kramer and Mattia Cerrato and Saso Dzeroski and Ross D. King},
  journal={ArXiv},
  year={2023},
  volume={abs/2305.02251},
  url={https://api.semanticscholar.org/CorpusID:258461620}
}

@inproceedings{Langley1977BACONAP,
  title={BACON: A Production System That Discovers Empirical Laws},
  author={Pat Langley},
  booktitle={International Joint Conference on Artificial Intelligence},
  year={1977},
  url={https://api.semanticscholar.org/CorpusID:2320342}
}

@inproceedings{Rzevski1987ScientificDC,
  title={Scientific discovery: compulalional explorations of the creative process},
  author={George Rzevski and Pat Langley and Herbert A. Simon and Gary L. Bradshaw and Jan M. Zytkow},
  year={1987},
  url={https://api.semanticscholar.org/CorpusID:62762320}
}

@inproceedings{Deroski1993DiscoveringD,
  title={Discovering Dynamics},
  author={Saso Dzeroski and Ljupco Todorovski},
  booktitle={International Conference on Machine Learning},
  year={1993},
  url={https://api.semanticscholar.org/CorpusID:17202248}
}

@article{Koza1994GeneticPA,
  title={Genetic programming as a means for programming computers by natural selection},
  author={John R. Koza},
  journal={Statistics and Computing},
  year={1994},
  volume={4},
  pages={87-112},
  url={https://api.semanticscholar.org/CorpusID:8750149}
}

@inproceedings{CoTodorovski1997DeclarativeBI,
  title={Declarative Bias in Equation Discovery},
  author={Ljup Co Todorovski},
  year={1997},
  url={https://api.semanticscholar.org/CorpusID:7320694}
}

@article{lenat1984and,
  title={Why AM and EURISKO appear to work},
  author={Lenat, Douglas B and Brown, John Seely},
  journal={Artificial intelligence},
  volume={23},
  number={3},
  pages={269--294},
  year={1984},
  publisher={Elsevier}
}


@article{guimera2020bs,
author = {Roger Guimerà  and Ignasi Reichardt  and Antoni Aguilar-Mogas  and Francesco A. Massucci  and Manuel Miranda  and Jordi Pallarès  and Marta Sales-Pardo },
title = {A Bayesian machine scientist to aid in the solution of challenging scientific problems},
journal = {Science Advances},
volume = {6},
number = {5},
pages = {eaav6971},
year = {2020},
doi = {10.1126/sciadv.aav6971},
URL = {https://www.science.org/doi/abs/10.1126/sciadv.aav6971},
eprint = {https://www.science.org/doi/pdf/10.1126/sciadv.aav6971},
abstract = {A Bayesian machine scientist uncovers closed-form mathematical models from data. Closed-form, interpretable mathematical models have been instrumental for advancing our understanding of the world; with the data revolution, we may now be in a position to uncover new such models for many systems from physics to the social sciences. However, to deal with increasing amounts of data, we need “machine scientists” that are able to extract these models automatically from data. Here, we introduce a Bayesian machine scientist, which establishes the plausibility of models using explicit approximations to the exact marginal posterior over models and establishes its prior expectations about models by learning from a large empirical corpus of mathematical expressions. It explores the space of models using Markov chain Monte Carlo. We show that this approach uncovers accurate models for synthetic and real data and provides out-of-sample predictions that are more accurate than those of existing approaches and of other nonparametric methods.}
}

@article{Petersen2019DeepSR,
  title={Deep symbolic regression: Recovering mathematical expressions from data via risk-seeking policy gradients},
  author={Brenden K. Petersen and Mikel Landajuela},
  journal={arXiv: Learning},
  year={2019},
  url={https://api.semanticscholar.org/CorpusID:212956516}
}

@article{Udrescu2020AIF2,
  title={AI Feynman 2.0: Pareto-optimal symbolic regression exploiting graph modularity},
  author={S. M. Udrescu and Andrew Yong-Yi Tan and Jiahai Feng and Orisvaldo Neto and Tailin Wu and Max Tegmark},
  journal={ArXiv},
  year={2020},
  volume={abs/2006.10782},
  url={https://api.semanticscholar.org/CorpusID:219955930}
}

@article{Cranmer2020DiscoveringSM,
  title={Discovering Symbolic Models from Deep Learning with Inductive Biases},
  author={M. Cranmer and Alvaro Sanchez-Gonzalez and Peter W. Battaglia and Rui Xu and Kyle Cranmer and David N. Spergel and Shirley Ho},
  journal={ArXiv},
  year={2020},
  volume={abs/2006.11287},
  url={https://api.semanticscholar.org/CorpusID:219966125}
}

@article{Garcon2021DeepNN,
  title={Deep neural networks to recover unknown physical parameters from oscillating time series},
  author={Antoine Garcon and Julian Vexler and Dmitry Budker and Stefan Kramer},
  journal={PLoS ONE},
  year={2021},
  volume={17},
  url={https://api.semanticscholar.org/CorpusID:231572959}
}

@article{Chen2022AutomatedDO,
  title={Automated discovery of fundamental variables hidden in experimental data},
  author={Boyuan Chen and Kuang Huang and Sunand Raghupathi and Ishaan Preetam Chandratreya and Qi Du and Hod Lipson},
  journal={Nature Computational Science},
  year={2022},
  volume={2},
  pages={433 - 442},
  url={https://api.semanticscholar.org/CorpusID:251087119}
}

@article{Raayoni_2021,
   title={Generating conjectures on fundamental constants with the Ramanujan Machine},
   volume={590},
   ISSN={1476-4687},
   url={http://dx.doi.org/10.1038/s41586-021-03229-4},
   DOI={10.1038/s41586-021-03229-4},
   number={7844},
   journal={Nature},
   publisher={Springer Science and Business Media LLC},
   author={Raayoni, Gal and Gottlieb, Shahar and Manor, Yahel and Pisha, George and Harris, Yoav and Mendlovic, Uri and Haviv, Doron and Hadad, Yaron and Kaminer, Ido},
   year={2021},
   month=feb, pages={67–73} 
}

@article{HAKUK2020101080,
title = {Automated discovery of scientific concepts: Replicating three recent discoveries in mechanics},
journal = {Advanced Engineering Informatics},
volume = {44},
pages = {101080},
year = {2020},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2020.101080},
url = {https://www.sciencedirect.com/science/article/pii/S1474034620300495},
author = {Yaron Hakuk and Yoram Reich},
keywords = {Interdisciplinary engineering knowledge genome, Knowledge discovery, Scientific discovery, Truss mechanism duality, Knowledge-based discovery},
abstract = {Throughout history, humanity cherished new scientific discoveries; many people devoted their lives to find them in laborious manual processes. Since the emergence of artificial intelligence, about 60 years ago, the possibility of automated scientific discovery has become conceivable and has attracted significant and growing interest. Many studies of automated scientific discovery have been presented over the years. This article presents a new automated discovery approach that crosses disciplines and transfers knowledge between them. The approach requires rich and formal background knowledge to find concepts, methods, or laws not known in one discipline by using their counterpart in other disciplines, specifically disciplinary knowledge that is represented in the Interdisciplinary Engineering Knowledge Genome. Three recent discoveries in mechanics were replicated automatically through software execution, demonstrating the validity of the approach. In future studies, the goal is to discover new knowledge in mechanics and/or electronics, as well as venture into other disciplines including outside engineering.}
}

@incollection{FAJTLOWICZ1988113,
title = {On Conjectures of Graffiti},
editor = {J. Akiyama and Y. Egawa and H. Enomoto},
series = {Annals of Discrete Mathematics},
publisher = {Elsevier},
volume = {38},
pages = {113-118},
year = {1988},
booktitle = {Graph Theory and Applications},
issn = {0167-5060},
doi = {https://doi.org/10.1016/S0167-5060(08)70776-3},
url = {https://www.sciencedirect.com/science/article/pii/S0167506008707763},
author = {Siemion Fajtlowicz},
abstract = {Publisher Summary
Graffiti is a computer program that makes graph-theoretical conjectures. The basic idea of Graffiti is that it “knows” certain graphs and it is capable of evaluating certain formulas formed from graph-theoretical invariants. If none of the graphs with which Graffiti is familiar is a counterexample to a formula then the formula is considered to be a conjecture. The number of conjectures, particularly those that are completely trivial, is the main problem and more than half of the program consists of various heuristics whose purpose is deletion of trivial and otherwise noninteresting but true conjectures. All conclusions that Graffiti draws are based exclusively on graphs it knows, so “follows” means of course “follows as far as the graphs in the library of program are concerned.” CNCL deletes those conjectures of the second and third type in which one of the invariants on the left is always smaller than an invariant on the right. This procedure may undoubtedly remove a number of interesting conjectures but it is the only one that seems to operate completely independently from IRIN.}
}

@article{PhysRevLett2020,
  title = {Discovering Physical Concepts with Neural Networks},
  author = {Iten, Raban and Metger, Tony and Wilming, Henrik and del Rio, L\'{\i}dia and Renner, Renato},
  journal = {Phys. Rev. Lett.},
  volume = {124},
  issue = {1},
  pages = {010508},
  numpages = {6},
  year = {2020},
  month = {Jan},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.124.010508},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.124.010508}
}

@article{chen2016automated,
  title={Automated discovery and proof of congruence theorems for partial sums of combinatorial sequences},
  author={Chen, William YC and Hou, Qing-Hu and Zeilberger, Doron},
  journal={Journal of Difference Equations and Applications},
  volume={22},
  number={6},
  pages={780--788},
  year={2016},
  publisher={Taylor \& Francis}
}

@article{wu2019toward,
  title={Toward an artificial intelligence physicist for unsupervised learning},
  author={Wu, Tailin and Tegmark, Max},
  journal={Physical Review E},
  volume={100},
  number={3},
  pages={033311},
  year={2019},
  publisher={APS}
}

@article{BUCHBERGER2006470,
title = {Theorema: Towards computer-aided mathematical theory exploration},
journal = {Journal of Applied Logic},
volume = {4},
number = {4},
pages = {470-504},
year = {2006},
note = {Towards Computer Aided Mathematics},
issn = {1570-8683},
doi = {https://doi.org/10.1016/j.jal.2005.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S1570868305000716},
author = {Bruno Buchberger and Adrian Crǎciun and Tudor Jebelean and Laura Kovács and Temur Kutsia and Koji Nakagawa and Florina Piroi and Nikolaj Popov and Judit Robu and Markus Rosenkranz and Wolfgang Windsteiger},
keywords = {Mathematical assistant, Automated reasoning, Theory exploration, “Lazy Thinking”, Theorema},
abstract = {Theorema is a project that aims at supporting the entire process of mathematical theory exploration within one coherent logic and software system. This survey paper illustrates the style of Theorema-supported mathematical theory exploration by a case study (the automated synthesis of an algorithm for the construction of Gröbner Bases) and gives an overview on some reasoners and organizational tools for theory exploration developed in the Theorema project.}
}

@article{lindsay1980applications,
  title={Applications of artificial intelligence for organic chemistry: the DENDRAL project},
  author={Lindsay, Robert K},
  journal={(No Title)},
  year={1980}
}

@article{schmidt2009distilling,
  title={Distilling free-form natural laws from experimental data},
  author={Schmidt, Michael and Lipson, Hod},
  journal={science},
  volume={324},
  number={5923},
  pages={81--85},
  year={2009},
  publisher={American Association for the Advancement of Science}
}

@article{savage2012automating,
  title={Automating scientific discovery},
  author={Savage, Neil},
  journal={Communications of the ACM},
  volume={55},
  number={5},
  pages={9--11},
  year={2012},
  publisher={ACM New York, NY, USA}
}

@Article{Jumper2021,
author={Jumper, John
and Evans, Richard
and Pritzel, Alexander
and Green, Tim
and Figurnov, Michael
and Ronneberger, Olaf
and Tunyasuvunakool, Kathryn
and Bates, Russ
and {\v{Z}}{\'i}dek, Augustin
and Potapenko, Anna
and Bridgland, Alex
and Meyer, Clemens
and Kohl, Simon A. A.
and Ballard, Andrew J.
and Cowie, Andrew
and Romera-Paredes, Bernardino
and Nikolov, Stanislav
and Jain, Rishub
and Adler, Jonas
and Back, Trevor
and Petersen, Stig
and Reiman, David
and Clancy, Ellen
and Zielinski, Michal
and Steinegger, Martin
and Pacholska, Michalina
and Berghammer, Tamas
and Bodenstein, Sebastian
and Silver, David
and Vinyals, Oriol
and Senior, Andrew W.
and Kavukcuoglu, Koray
and Kohli, Pushmeet
and Hassabis, Demis},
title={Highly accurate protein structure prediction with AlphaFold},
journal={Nature},
year={2021},
month={Aug},
day={01},
volume={596},
number={7873},
pages={583-589},
abstract={Proteins are essential to life, and understanding their structure can facilitate a mechanistic understanding of their function. Through an enormous experimental effort1--4, the structures of around 100,000 unique proteins have been determined5, but this represents a small fraction of the billions of known protein sequences6,7. Structural coverage is bottlenecked by the months to years of painstaking effort required to determine a single protein structure. Accurate computational approaches are needed to address this gap and to enable large-scale structural bioinformatics. Predicting the three-dimensional structure that a protein will adopt based solely on its amino acid sequence---the structure prediction component of the `protein folding problem'8---has been an important open research problem for more than 50 years9. Despite recent progress10--14, existing methods fall far short of atomic accuracy, especially when no homologous structure is available. Here we provide the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure is known. We validated an entirely redesigned version of our neural network-based model, AlphaFold, in the challenging 14th Critical Assessment of protein Structure Prediction (CASP14)15, demonstrating accuracy competitive with experimental structures in a majority of cases and greatly outperforming other methods. Underpinning the latest version of AlphaFold is a novel machine learning approach that incorporates physical and biological knowledge about protein structure, leveraging multi-sequence alignments, into the design of the deep learning algorithm.},
issn={1476-4687},
doi={10.1038/s41586-021-03819-2},
url={https://doi.org/10.1038/s41586-021-03819-2}
}

@misc{zambaldi2024novodesignhighaffinityprotein,
      title={De novo design of high-affinity protein binders with AlphaProteo}, 
      author={Vinicius Zambaldi and David La and Alexander E. Chu and Harshnira Patani and Amy E. Danson and Tristan O. C. Kwan and Thomas Frerix and Rosalia G. Schneider and David Saxton and Ashok Thillaisundaram and Zachary Wu and Isabel Moraes and Oskar Lange and Eliseo Papa and Gabriella Stanton and Victor Martin and Sukhdeep Singh and Lai H. Wong and Russ Bates and Simon A. Kohl and Josh Abramson and Andrew W. Senior and Yilmaz Alguel and Mary Y. Wu and Irene M. Aspalter and Katie Bentley and David L. V. Bauer and Peter Cherepanov and Demis Hassabis and Pushmeet Kohli and Rob Fergus and Jue Wang},
      year={2024},
      eprint={2409.08022},
      archivePrefix={arXiv},
      primaryClass={q-bio.BM},
      url={https://arxiv.org/abs/2409.08022}, 
}

@Article{Trinh2024,
author={Trinh, Trieu H.
and Wu, Yuhuai
and Le, Quoc V.
and He, He
and Luong, Thang},
title={Solving olympiad geometry without human demonstrations},
journal={Nature},
year={2024},
month={Jan},
day={01},
volume={625},
number={7995},
pages={476-482},
abstract={Proving mathematical theorems at the olympiad level represents a notable milestone in human-level automated reasoning1--4, owing to their reputed difficulty among the world's best talents in pre-university mathematics. Current machine-learning approaches, however, are not applicable to most mathematical domains owing to the high cost of translating human proofs into machine-verifiable format. The problem is even worse for geometry because of its unique translation challenges1,5, resulting in severe scarcity of training data. We propose AlphaGeometry, a theorem prover for Euclidean plane geometry that sidesteps the need for human demonstrations by synthesizing millions of theorems and proofs across different levels of complexity. AlphaGeometry is a neuro-symbolic system that uses a neural language model, trained from scratch on our large-scale synthetic data, to guide a symbolic deduction engine through infinite branching points in challenging problems. On a test set of 30 latest olympiad-level problems, AlphaGeometry solves 25, outperforming the previous best method that only solves ten problems and approaching the performance of an average International Mathematical Olympiad (IMO) gold medallist. Notably, AlphaGeometry produces human-readable proofs, solves all geometry problems in the IMO 2000 and 2015 under human expert evaluation and discovers a generalized version of a translated IMO theorem in 2004.},
issn={1476-4687},
doi={10.1038/s41586-023-06747-5},
url={https://doi.org/10.1038/s41586-023-06747-5}
}

@misc{hubert2024,
      title={AlphaProof}, 
      author={Thomas Hubert and Rishi Mehta and Laurent Sartran},
      year={2024},
      url={https://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level/}, 
}

@Article{Abramson2024,
author={Abramson, Josh
and Adler, Jonas
and Dunger, Jack
and Evans, Richard
and Green, Tim
and Pritzel, Alexander
and Ronneberger, Olaf
and Willmore, Lindsay
and Ballard, Andrew J.
and Bambrick, Joshua
and Bodenstein, Sebastian W.
and Evans, David A.
and Hung, Chia-Chun
and O'Neill, Michael
and Reiman, David
and Tunyasuvunakool, Kathryn
and Wu, Zachary
and {\v{Z}}emgulyt{\.{e}}, Akvil{\.{e}}
and Arvaniti, Eirini
and Beattie, Charles
and Bertolli, Ottavia
and Bridgland, Alex
and Cherepanov, Alexey
and Congreve, Miles
and Cowen-Rivers, Alexander I.
and Cowie, Andrew
and Figurnov, Michael
and Fuchs, Fabian B.
and Gladman, Hannah
and Jain, Rishub
and Khan, Yousuf A.
and Low, Caroline M. R.
and Perlin, Kuba
and Potapenko, Anna
and Savy, Pascal
and Singh, Sukhdeep
and Stecula, Adrian
and Thillaisundaram, Ashok
and Tong, Catherine
and Yakneen, Sergei
and Zhong, Ellen D.
and Zielinski, Michal
and {\v{Z}}{\'i}dek, Augustin
and Bapst, Victor
and Kohli, Pushmeet
and Jaderberg, Max
and Hassabis, Demis
and Jumper, John M.},
title={Accurate structure prediction of biomolecular interactions with AlphaFold 3},
journal={Nature},
year={2024},
month={Jun},
day={01},
volume={630},
number={8016},
pages={493-500},
abstract={The introduction of AlphaFold{\thinspace}21 has spurred a revolution in modelling the structure of proteins and their interactions, enabling a huge range of applications in protein modelling and design2--6. Here we describe our AlphaFold{\thinspace}3 model with a substantially updated diffusion-based architecture that is capable of predicting the joint structure of complexes including proteins, nucleic acids, small molecules, ions and modified residues. The new AlphaFold model demonstrates substantially improved accuracy over many previous specialized tools: far greater accuracy for protein--ligand interactions compared with state-of-the-art docking tools, much higher accuracy for protein--nucleic acid interactions compared with nucleic-acid-specific predictors and substantially higher antibody--antigen prediction accuracy compared with AlphaFold-Multimer v.2.37,8. Together, these results show that high-accuracy modelling across biomolecular space is possible within a single unified deep-learning framework.},
issn={1476-4687},
doi={10.1038/s41586-024-07487-w},
url={https://doi.org/10.1038/s41586-024-07487-w}
}

@misc{si2024llmsgeneratenovelresearch,
      title={Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers}, 
      author={Chenglei Si and Diyi Yang and Tatsunori Hashimoto},
      year={2024},
      eprint={2409.04109},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2409.04109}, 
}

@article{Baek2024ResearchAgentIR,
  title={ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models},
  author={Jinheon Baek and Sujay Kumar Jauhar and Silviu Cucerzan and Sung Ju Hwang},
  journal={ArXiv},
  year={2024},
  volume={abs/2404.07738},
  url={https://api.semanticscholar.org/CorpusID:269042844}
}

@article{skarlinski2024language,
  title={Language agents achieve superhuman synthesis of scientific knowledge},
  author={
    Michael D. Skarlinski and
    Sam Cox and
    Jon M. Laurent and
    James D. Braza and
    Michaela Hinks and
    Michael J. Hammerling and
    Manvitha Ponnapati and
    Samuel G. Rodriques and
    Andrew D. White},
  year={2024},
  journal={preprint},
  url={https://paper.wikicrow.ai}
}

@misc{wang2024scimonscientificinspirationmachines,
      title={SciMON: Scientific Inspiration Machines Optimized for Novelty}, 
      author={Qingyun Wang and Doug Downey and Heng Ji and Tom Hope},
      year={2024},
      eprint={2305.14259},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.14259}, 
}

@misc{qi2023largelanguagemodelszero,
      title={Large Language Models are Zero Shot Hypothesis Proposers}, 
      author={Biqing Qi and Kaiyan Zhang and Haoxiang Li and Kai Tian and Sihang Zeng and Zhang-Ren Chen and Bowen Zhou},
      year={2023},
      eprint={2311.05965},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2311.05965}, 
}

@inproceedings{Yang2023LargeLM,
  title={Large Language Models for Automated Open-domain Scientific Hypotheses Discovery},
  author={Zonglin Yang and Xinya Du and Junxian Li and Jie Zheng and Soujanya Poria and E. Cambria},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:261557055}
}

@inproceedings{Wang2023SciMONSI,
  title={SciMON: Scientific Inspiration Machines Optimized for Novelty},
  author={Qingyun Wang and Doug Downey and Heng Ji and Tom Hope},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:258841365}
}

@inproceedings{Ghafarollahi2024SciAgentsAS,
  title={SciAgents: Automating scientific discovery through multi-agent intelligent graph reasoning},
  author={Alireza Ghafarollahi and Markus J. Buehler},
  year={2024},
  url={https://api.semanticscholar.org/CorpusID:272524655}
}

@inproceedings{Li2024MLRCopilotAM,
  title={MLR-Copilot: Autonomous Machine Learning Research based on Large Language Models Agents},
  author={Ruochen Li and Teerth Patel and Qingyun Wang and Qingyun Wang and Xinya Du},
  year={2024},
  url={https://api.semanticscholar.org/CorpusID:271957477}
}

@misc{lu2024aiscientistfullyautomated,
      title={The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery}, 
      author={Chris Lu and Cong Lu and Robert Tjarko Lange and Jakob Foerster and Jeff Clune and David Ha},
      year={2024},
      eprint={2408.06292},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2408.06292}, 
}

@inproceedings{liu2024coquest,
author = {Liu, Yiren and Chen, Si and Cheng, Haocong and Yu, Mengxia and Ran, Xiao and Mo, Andrew and Tang, Yiliu and Huang, Yun},
title = {How AI Processing Delays Foster Creativity: Exploring Research Question Co-Creation with an LLM-based Agent},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642698},
doi = {10.1145/3613904.3642698},
abstract = {Developing novel research questions (RQs) often requires extensive literature reviews, especially in interdisciplinary fields. To support RQ development through human-AI co-creation, we leveraged Large Language Models (LLMs) to build an LLM-based agent system named CoQuest. We conducted an experiment with 20 HCI researchers to examine the impact of two interaction designs: breadth-first and depth-first RQ generation. The findings revealed that participants perceived the breadth-first approach as more creative and trustworthy upon task completion. Conversely, during the task, participants considered the depth-first generated RQs as more creative. Additionally, we discovered that AI processing delays allowed users to reflect on multiple RQs simultaneously, leading to a higher quantity of generated RQs and an enhanced sense of control. Our work makes both theoretical and practical contributions by proposing and evaluating a mental model for human-AI co-creation of RQs. We also address potential ethical issues, such as biases and over-reliance on AI, advocating for using the system to improve human research creativity rather than automating scientific inquiry. The system’s source is available at: https://github.com/yiren-liu/coquest.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {17},
numpages = {25},
keywords = {Co-creation Systems, Large Language Models, Mixed-initiative Design, Scientifc Discovery},
location = {Honolulu, HI, USA},
series = {CHI '24}
}


@article{king2009automate,
author = {Ross D. King  and Jem Rowland  and Stephen G. Oliver  and Michael Young  and Wayne Aubrey  and Emma Byrne  and Maria Liakata  and Magdalena Markham  and Pinar Pir  and Larisa N. Soldatova  and Andrew Sparkes  and Kenneth E. Whelan  and Amanda Clare },
title = {The Automation of Science},
journal = {Science},
volume = {324},
number = {5923},
pages = {85-89},
year = {2009},
doi = {10.1126/science.1165620},
URL = {https://www.science.org/doi/abs/10.1126/science.1165620},
eprint = {https://www.science.org/doi/pdf/10.1126/science.1165620},
abstract = {The basis of science is the hypothetico-deductive method and the recording of experiments in sufficient detail to enable reproducibility. We report the development of Robot Scientist “Adam,” which advances the automation of both. Adam has autonomously generated functional genomics hypotheses about the yeast Saccharomyces cerevisiae and experimentally tested these hypotheses by using laboratory automation. We have confirmed Adam's conclusions through manual experiments. To describe Adam's research, we have developed an ontology and logical language. The resulting formalization involves over 10,000 different research units in a nested treelike structure, 10 levels deep, that relates the 6.6 million biomass measurements to their logical description. This formalization describes how a machine contributed to scientific knowledge.}
}

@article{waltz2009automate,
author = {David Waltz  and Bruce G. Buchanan },
title = {Automating Science},
journal = {Science},
volume = {324},
number = {5923},
pages = {43-44},
year = {2009},
doi = {10.1126/science.1172781},
URL = {https://www.science.org/doi/abs/10.1126/science.1172781},
eprint = {https://www.science.org/doi/pdf/10.1126/science.1172781}
}

@misc{ifargan2024autonomousllmdrivenresearchdata,
      title={Autonomous LLM-driven research from data to human-verifiable research papers}, 
      author={Tal Ifargan and Lukas Hafner and Maor Kern and Ori Alcalay and Roy Kishony},
      year={2024},
      eprint={2404.17605},
      archivePrefix={arXiv},
      primaryClass={q-bio.OT},
      url={https://arxiv.org/abs/2404.17605}, 
}

@misc{majumder2024discoverybenchdatadrivendiscoverylarge,
      title={DiscoveryBench: Towards Data-Driven Discovery with Large Language Models}, 
      author={Bodhisattwa Prasad Majumder and Harshit Surana and Dhruv Agarwal and Bhavana Dalvi Mishra and Abhijeetsingh Meena and Aryan Prakhar and Tirth Vora and Tushar Khot and Ashish Sabharwal and Peter Clark},
      year={2024},
      eprint={2407.01725},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.01725}, 
}

@article{Girotra2023IdeasAD,
  title={Ideas are Dimes a Dozen: Large Language Models for Idea Generation in Innovation},
  author={Karan Girotra and Lennart Meincke and Christian Terwiesch and Karl T. Ulrich},
  journal={SSRN Electronic Journal},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:260467886}
}

@misc{boiko2023emergentautonomousscientificresearch,
      title={Emergent autonomous scientific research capabilities of large language models}, 
      author={Daniil A. Boiko and Robert MacKnight and Gabe Gomes},
      year={2023},
      eprint={2304.05332},
      archivePrefix={arXiv},
      primaryClass={physics.chem-ph},
      url={https://arxiv.org/abs/2304.05332}, 
}

@article{Wang2019PaperRobotID,
  title={PaperRobot: Incremental Draft Generation of Scientific Ideas},
  author={Qingyun Wang and Lifu Huang and Zhiying Jiang and Kevin Knight and Heng Ji and Mohit Bansal and Yi Luan},
  journal={ArXiv},
  year={2019},
  volume={abs/1905.07870},
  url={https://api.semanticscholar.org/CorpusID:159040684}
}

@article{altmae2023,
author = {Altmäe, Signe and Sola-Leyva, Alberto and Salumets, Andres},
year = {2023},
month = {04},
pages = {},
title = {Artificial intelligence in scientific writing: a friend or a foe?},
volume = {47},
journal = {Reproductive BioMedicine Online},
doi = {10.1016/j.rbmo.2023.04.009}
}

@inproceedings{Sybrandt2020,
author = {Sybrandt, Justin and Tyagin, Ilya and Shtutman, Michael and Safro, Ilya},
title = {AGATHA: Automatic Graph Mining And Transformer based Hypothesis Generation Approach},
year = {2020},
isbn = {9781450368599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340531.3412684},
doi = {10.1145/3340531.3412684},
abstract = {Medical research is risky and expensive. Drug discovery requires researchers to efficiently winnow thousands of potential targets to a small candidate set. However, scientists spend significant time and money long before seeing the intermediate results that ultimately determine this smaller set. Hypothesis generation systems address this challenge by mining the wealth of publicly available scientific information to predict plausible research directions. We present AGATHA, a deep-learning hypothesis generation system that learns a data-driven ranking criteria to recommend new biomedical connections. We massively validate our system with a temporal holdout wherein we predict connections first introduced after 2015 using data published beforehand. We additionally explore biomedical sub-domains, and demonstrate AGATHA's predictive capacity across the twenty most popular relationship types. Furthermore, we perform an ablation study to examine the aspects of our semantic network that most contribute to recommendation quality. Overall, AGATHA achieves best-in-class recommendation quality when compared to other hypothesis generation systems built to predict across all available biomedical literature. Reproducibility: All code, experimental data, and pre-trained models are available online: sybrandt.com/2020/agatha.},
booktitle = {Proceedings of the 29th ACM International Conference on Information \& Knowledge Management},
pages = {2757–2764},
numpages = {8},
keywords = {biomedical recommendation, hypothesis generation, literature-based discovery, semantic networks, transformer models},
location = {Virtual Event, Ireland},
series = {CIKM '20}
}

@article{Li2021FromKT,
  title={From Kepler to Newton: Explainable AI for Science Discovery},
  author={Zelong Li and Jianchao Ji and Yongfeng Zhang},
  journal={ArXiv},
  year={2021},
  volume={abs/2111.12210},
  url={https://api.semanticscholar.org/CorpusID:244728403}
}

@article{Ulam1958JohnVN,
  title={John von Neumann 1903-1957},
  author={Stanisław Ulam},
  journal={Bulletin of the American Mathematical Society},
  year={1958},
  volume={64},
  pages={1-49},
  url={https://api.semanticscholar.org/CorpusID:122374268}
}

@article{Good1965SpeculationsCT,
  title={Speculations Concerning the First Ultraintelligent Machine},
  author={I. J. Good},
  journal={Adv. Comput.},
  year={1965},
  volume={6},
  pages={31-88},
  url={https://api.semanticscholar.org/CorpusID:17886872}
}

@misc{huang2024mlagentbenchevaluatinglanguageagents,
      title={MLAgentBench: Evaluating Language Agents on Machine Learning Experimentation}, 
      author={Qian Huang and Jian Vora and Percy Liang and Jure Leskovec},
      year={2024},
      eprint={2310.03302},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2310.03302}, 
}

@inproceedings{Colton2008CreativityVT,
  title={Creativity Versus the Perception of Creativity in Computational Systems},
  author={Simon Colton},
  booktitle={AAAI Spring Symposium: Creative Intelligent Systems},
  year={2008},
  url={https://api.semanticscholar.org/CorpusID:2876679}
}

@article{lamb2018eval,
author = {Lamb, Carolyn and Brown, Daniel G. and Clarke, Charles L. A.},
title = {Evaluating Computational Creativity: An Interdisciplinary Tutorial},
year = {2018},
issue_date = {March 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3167476},
doi = {10.1145/3167476},
abstract = {This article is a tutorial for researchers who are designing software to perform a creative task and want to evaluate their system using interdisciplinary theories of creativity. Researchers who study human creativity have a great deal to offer computational creativity. We summarize perspectives from psychology, philosophy, cognitive science, and computer science as to how creativity can be measured both in humans and in computers. We survey how these perspectives have been used in computational creativity research and make recommendations for how they should be used.},
journal = {ACM Comput. Surv.},
month = feb,
articleno = {28},
numpages = {34},
keywords = {digital art, computational creativity, Computational aesthetics}
}

@inbook{gervas2019,
author = {Gervás, Pablo},
year = {2019},
month = {07},
pages = {275-304},
title = {Exploring Quantitative Evaluations of the Creativity of Automatic Poets},
isbn = {978-3-319-43608-1},
doi = {10.1007/978-3-319-43610-4_13}
}

@inproceedings{Veale2015GameOT,
  title={Game of Tropes: Exploring the Placebo Effect in Computational Creativity},
  author={Tony Veale},
  booktitle={International Conference on Innovative Computing and Cloud Computing},
  year={2015},
  url={https://api.semanticscholar.org/CorpusID:3966712}
}

@article{Baer2012DomainSA,
  title={Domain Specificity and the Limits of Creativity Theory},
  author={John Baer},
  journal={Journal of Creative Behavior},
  year={2012},
  volume={46},
  pages={16-29},
  url={https://api.semanticscholar.org/CorpusID:45983030}
}

@article{Minsky1982WhyPT,
  title={Why People Think Computers Can't},
  author={Marvin Minsky},
  journal={AI Mag.},
  year={1982},
  volume={3},
  pages={3-15},
  url={https://api.semanticscholar.org/CorpusID:42565554}
}

@inproceedings{Boden1991TheCM,
  title={The creative mind : myths \& mechanisms},
  author={Margaret A. Boden},
  year={1991},
  url={https://api.semanticscholar.org/CorpusID:143261160}
}

@article{Karampiperis2014TowardsMF,
  title={Towards Machines for Measuring Creativity: The Use of Computational Tools in Storytelling Activities},
  author={Pythagoras Karampiperis and Antonis Koukourikos and Evangelia Koliopoulou},
  journal={2014 IEEE 14th International Conference on Advanced Learning Technologies},
  year={2014},
  pages={508-512},
  url={https://api.semanticscholar.org/CorpusID:17903634}
}

@inproceedings{Jordanous2016TheLT,
  title={The longer term value of creativity judgements in computational creativity},
  author={Anna Jordanous},
  year={2016},
  url={https://api.semanticscholar.org/CorpusID:637189}
}

@misc{peeperkorn2024temperaturecreativityparameterlarge,
      title={Is Temperature the Creativity Parameter of Large Language Models?}, 
      author={Max Peeperkorn and Tom Kouwenhoven and Dan Brown and Anna Jordanous},
      year={2024},
      eprint={2405.00492},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.00492}, 
}

@misc{banerjee2024llmshallucinateneedlive,
      title={LLMs Will Always Hallucinate, and We Need to Live With This}, 
      author={Sourav Banerjee and Ayushi Agarwal and Saloni Singla},
      year={2024},
      eprint={2409.05746},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2409.05746}, 
}

@article{Carlini2022QuantifyingMA,
  title={Quantifying Memorization Across Neural Language Models},
  author={Nicholas Carlini and Daphne Ippolito and Matthew Jagielski and Katherine Lee and Florian Tram{\`e}r and Chiyuan Zhang},
  journal={ArXiv},
  year={2022},
  volume={abs/2202.07646},
  url={https://api.semanticscholar.org/CorpusID:246863735}
}

@article{Hupkes2022ATA,
  title={A taxonomy and review of generalization research in NLP},
  author={Dieuwke Hupkes and Mario Giulianelli and Verna Dankers and Mikel Artetxe and Yanai Elazar and Tiago Pimentel and Christos Christodoulopoulos and Karim Lasri and Naomi Saphra and Arabella J. Sinclair and Dennis Ulmer and Florian Schottmann and Khuyagbaatar Batsuren and Kaiser Sun and Koustuv Sinha and Leila Khalatbari and Maria Ryskina and Rita Frieske and Ryan Cotterell and Zhijing Jin},
  journal={Nature Machine Intelligence},
  year={2022},
  volume={5},
  pages={1161 - 1174},
  url={https://api.semanticscholar.org/CorpusID:252735124}
}

@article{Somepalli2022DiffusionAO,
  title={Diffusion Art or Digital Forgery? Investigating Data Replication in Diffusion Models},
  author={Gowthami Somepalli and Vasu Singla and Micah Goldblum and Jonas Geiping and Tom Goldstein},
  journal={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2022},
  pages={6048-6058},
  url={https://api.semanticscholar.org/CorpusID:254366634}
}

@inproceedings{Carlini2018TheSS,
  title={The Secret Sharer: Evaluating and Testing Unintended Memorization in Neural Networks},
  author={Nicholas Carlini and Chang Liu and {\'U}lfar Erlingsson and Jernej Kos and Dawn Xiaodong Song},
  booktitle={USENIX Security Symposium},
  year={2018},
  url={https://api.semanticscholar.org/CorpusID:170076423}
}

@misc{chang2023speakmemoryarchaeologybooks,
      title={Speak, Memory: An Archaeology of Books Known to ChatGPT/GPT-4}, 
      author={Kent K. Chang and Mackenzie Cramer and Sandeep Soni and David Bamman},
      year={2023},
      eprint={2305.00118},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.00118}, 
}

@misc{raffel2023exploringlimitstransferlearning,
      title={Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer}, 
      author={Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
      year={2023},
      eprint={1910.10683},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1910.10683}, 
}

@article{Franceschelli2021CopyrightIG,
  title={Copyright in generative deep learning},
  author={Giorgio Franceschelli and Mirco Musolesi},
  journal={Data \& Policy},
  year={2021},
  volume={4},
  url={https://api.semanticscholar.org/CorpusID:234777751}
}

@misc{duarte2024decopdetectingcopyrightedcontent,
      title={DE-COP: Detecting Copyrighted Content in Language Models Training Data}, 
      author={André V. Duarte and Xuandong Zhao and Arlindo L. Oliveira and Lei Li},
      year={2024},
      eprint={2402.09910},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.09910}, 
}

@misc{zhao2022provablyconfidentiallanguagemodelling,
      title={Provably Confidential Language Modelling}, 
      author={Xuandong Zhao and Lei Li and Yu-Xiang Wang},
      year={2022},
      eprint={2205.01863},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2205.01863}, 
}

@misc{carlini2021extractingtrainingdatalarge,
      title={Extracting Training Data from Large Language Models}, 
      author={Nicholas Carlini and Florian Tramer and Eric Wallace and Matthew Jagielski and Ariel Herbert-Voss and Katherine Lee and Adam Roberts and Tom Brown and Dawn Song and Ulfar Erlingsson and Alina Oprea and Colin Raffel},
      year={2021},
      eprint={2012.07805},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2012.07805}, 
}

@misc{li2024diggerdetectingcopyrightcontent,
      title={Digger: Detecting Copyright Content Mis-usage in Large Language Model Training}, 
      author={Haodong Li and Gelei Deng and Yi Liu and Kailong Wang and Yuekang Li and Tianwei Zhang and Yang Liu and Guoai Xu and Guosheng Xu and Haoyu Wang},
      year={2024},
      eprint={2401.00676},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2401.00676}, 
}

@misc{oren2023provingtestsetcontamination,
      title={Proving Test Set Contamination in Black Box Language Models}, 
      author={Yonatan Oren and Nicole Meister and Niladri Chatterji and Faisal Ladhak and Tatsunori B. Hashimoto},
      year={2023},
      eprint={2310.17623},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.17623}, 
}

@misc{shi2024detectingpretrainingdatalarge,
      title={Detecting Pretraining Data from Large Language Models}, 
      author={Weijia Shi and Anirudh Ajith and Mengzhou Xia and Yangsibo Huang and Daogao Liu and Terra Blevins and Danqi Chen and Luke Zettlemoyer},
      year={2024},
      eprint={2310.16789},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.16789}, 
}

@article{Kandpal2022DeduplicatingTD,
  title={Deduplicating Training Data Mitigates Privacy Risks in Language Models},
  author={Nikhil Kandpal and Eric Wallace and Colin Raffel},
  journal={ArXiv},
  year={2022},
  volume={abs/2202.06539},
  url={https://api.semanticscholar.org/CorpusID:246823128}
}

@article{Ippolito2022PreventingVM,
  title={Preventing Verbatim Memorization in Language Models Gives a False Sense of Privacy},
  author={Daphne Ippolito and Florian Tram{\`e}r and Milad Nasr and Chiyuan Zhang and Matthew Jagielski and Katherine Lee and Christopher A. Choquette-Choo and Nicholas Carlini},
  journal={ArXiv},
  year={2022},
  volume={abs/2210.17546},
  url={https://api.semanticscholar.org/CorpusID:253237404}
}

@article{Hans2024BeLA,
  title={Be like a Goldfish, Don't Memorize! Mitigating Memorization in Generative LLMs},
  author={Abhimanyu Hans and Yuxin Wen and Neel Jain and John Kirchenbauer and Hamid Kazemi and Prajwal Singhania and Siddharth Singh and Gowthami Somepalli and Jonas Geiping and Abhinav Bhatele and Tom Goldstein},
  journal={ArXiv},
  year={2024},
  volume={abs/2406.10209},
  url={https://api.semanticscholar.org/CorpusID:270521774}
}

@article{abbot2023,
    author = {Ryan Abbott and Elizabeth Rothman},
    title = {Disrupting Creativity: Copyright Law in the Age of Generative Artificial Intelligence},
    journal = {Elsevier},
    year = {2023}
}

@misc{lawsuitNYTOpenai,
    author = {Grynbaum, M. M. and Mac, R.},
    title = {The Times Sues OpenAI and Microsoft Over A.I. Use of Copyrighted Work.},
    journal = {New York Times},
    year = {2023},
    url={https://www.nytimes.com/2023/12/27/business/media/new-york-times-open-ai-microsoft-lawsuit.html}
}

@misc{classactionstability,
    author = {Brittain, B.},
    title = {Artists take new shot at Stability, Midjourney in updated copyright lawsuit.},
    journal = {Reuters},
    year = {2023},
    url={https://www.reuters.com/legal/litigation/artists-take-new-shot-stability-midjourney-updated-copyright-lawsuit-2023-11-30/}
}

@inproceedings{Netanel2011MakingSO,
  title={Making Sense of Fair Use},
  author={Neil Weinstock Netanel},
  year={2011},
  url={https://api.semanticscholar.org/CorpusID:152743703}
}

@article{Sobel2017ArtificialIF,
  title={Artificial Intelligence's Fair Use Crisis},
  author={Benjamin Sobel},
  journal={Columbia Journal of Law and the Arts},
  year={2017},
  volume={41},
  pages={45-97},
  url={https://api.semanticscholar.org/CorpusID:115500744}
}

@article{Ginsburg2018PeopleNM,
  title={People Not Machines: Authorship and What It Means in the Berne Convention},
  author={Jane C. Ginsburg},
  journal={IIC - International Review of Intellectual Property and Competition Law},
  year={2018},
  volume={49},
  pages={131-135},
  url={https://api.semanticscholar.org/CorpusID:158580780}
}

@article{Miller1993COPYRIGHTPF,
  title={COPYRIGHT PROTECTION FOR COMPUTER PROGRAMS, DATABASES, AND COMPUTER-GENERATED WORKS: IS ANYTHING NEW SINCE CONTU?},
  author={Arthur R. Miller},
  journal={Harvard Law Review},
  year={1993},
  volume={106},
  pages={978-1073},
  url={https://api.semanticscholar.org/CorpusID:141462591}
}

@article{Samuelson1986AllocatingOR,
  title={Allocating Ownership Rights in Computer-Generated Works},
  author={Pamela Samuelson},
  journal={University of Pittsburgh Law Review},
  year={1986},
  volume={47},
  pages={1185},
  url={https://api.semanticscholar.org/CorpusID:142911255}
}

@article{Denicola2016ExMC,
  title={Ex Machina: Copyright Protection for Computer-Generated Works},
  author={Robert C. Denicola},
  journal={Innovation Law \& Policy eJournal},
  year={2016},
  url={https://api.semanticscholar.org/CorpusID:88486949}
}

@article{ralston2005,
author = {Ralston, W.T.},
year = {2005},
month = {03},
pages = {281-284},
title = {Copyright in computer-composed music: Hal meets Handel},
volume = {52},
journal = {Journal of the Copyright Society of the U.S.A}
}

@article{Santos2020IntellectualPO,
  title={Intellectual Property on Works of Art Made by Artificial Intelligence},
  author={Cl{\'a}udio Lisboa dos Santos and {\^A}ngela Rocha Machado},
  journal={International Journal of Advanced Engineering Research and Science},
  year={2020},
  url={https://api.semanticscholar.org/CorpusID:234541178}
}

@article{Deltorn2018AuthorshipIT,
  title={Authorship in the Age of Machine learning and Artificial Intelligence},
  author={Jean-Marc Deltorn and Franck Macrez},
  journal={Legal Perspectives in Information Systems eJournal},
  year={2018},
  url={https://api.semanticscholar.org/CorpusID:69806575}
}

@article{Guadamuz2017DoAD,
  title={Do Androids Dream of Electric Copyright? Comparative Analysis of Originality in Artificial Intelligence Generated Works},
  author={Andres Guadamuz},
  journal={Econometrics: Computer Programs \& Software eJournal},
  year={2017},
  url={https://api.semanticscholar.org/CorpusID:21670175}
}

@article{McCoy2023EmbersOA,
  title={Embers of Autoregression: Understanding Large Language Models Through the Problem They are Trained to Solve},
  author={R. Thomas McCoy and Shunyu Yao and Dan Friedman and Matthew Hardy and Thomas L. Griffiths},
  journal={ArXiv},
  year={2023},
  volume={abs/2309.13638},
  url={https://api.semanticscholar.org/CorpusID:262464572}
}

@inproceedings{taylor2004computationally,
  title={Computationally recognizing wordplay in jokes},
  author={Taylor, Julia M and Mazlack, Lawrence J},
  booktitle={Proceedings of the Annual Meeting of the Cognitive Science Society},
  volume={26},
  number={26},
  year={2004}
}

@inproceedings{liu-etal-2018-exploiting,
    title = "Exploiting Syntactic Structures for Humor Recognition",
    author = "Liu, Lizhen  and
      Zhang, Donghai  and
      Song, Wei",
    editor = "Bender, Emily M.  and
      Derczynski, Leon  and
      Isabelle, Pierre",
    booktitle = "Proceedings of the 27th International Conference on Computational Linguistics",
    month = aug,
    year = "2018",
    address = "Santa Fe, New Mexico, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/C18-1159",
    pages = "1875--1883",
    abstract = "Humor recognition is an interesting and challenging task in natural language processing. This paper proposes to exploit syntactic structure features to enhance humor recognition. Our method achieves significant improvements compared with humor theory driven baselines. We found that some syntactic structure features consistently correlate with humor, which indicate interesting linguistic phenomena. Both the experimental results and the analysis demonstrate that humor can be viewed as a kind of style and content independent syntactic structures can help identify humor and have good interpretability.",
}

@article{abulaish2020,
author = {Abulaish, Muhammad and Kamal, Ashraf and Zaki, Mohammed J.},
title = {A Survey of Figurative Language and Its Computational Detection in Online Social Networks},
year = {2020},
issue_date = {February 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {1},
issn = {1559-1131},
url = {https://doi.org/10.1145/3375547},
doi = {10.1145/3375547},
abstract = {The frequent usage of figurative language on online social networks, especially on Twitter, has the potential to mislead traditional sentiment analysis and recommender systems. Due to the extensive use of slangs, bashes, flames, and non-literal texts, tweets are a great source of figurative language, such as sarcasm, irony, metaphor, simile, hyperbole, humor, and satire. Starting with a brief introduction of figurative language and its various categories, this article presents an in-depth survey of the state-of-the-art techniques for computational detection of seven different figurative language categories, mainly on Twitter. For each figurative language category, we present details about the characterizing features, datasets, and state-of-the-art computational detection approaches. Finally, we discuss open challenges and future directions of research for each figurative language category.},
journal = {ACM Trans. Web},
month = feb,
articleno = {3},
numpages = {52},
keywords = {Social network analysis, figurative language, humor recognition, hyperbole detection, irony detection, metaphor detection, sarcasm detection, satire detection, simile detection}
}

@inproceedings{zhang-etal-2019-telling,
    title = "Telling the Whole Story: A Manually Annotated {C}hinese Dataset for the Analysis of Humor in Jokes",
    author = "Zhang, Dongyu  and
      Zhang, Heting  and
      Liu, Xikai  and
      Lin, Hongfei  and
      Xia, Feng",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1673",
    doi = "10.18653/v1/D19-1673",
    pages = "6402--6407",
    abstract = "Humor plays important role in human communication, which makes it important problem for natural language processing. Prior work on the analysis of humor focuses on whether text is humorous or not, or the degree of funniness, but this is insufficient to explain why it is funny. We therefore create a dataset on humor with 9,123 manually annotated jokes in Chinese. We propose a novel annotation scheme to give scenarios of how humor arises in text. Specifically, our annotations of linguistic humor not only contain the degree of funniness, like previous work, but they also contain key words that trigger humor as well as character relationship, scene, and humor categories. We report reasonable agreement between annota-tors. We also conduct an analysis and exploration of the dataset. To the best of our knowledge, we are the first to approach humor annotation for exploring the underlying mechanism of the use of humor, which may contribute to a significantly deeper analysis of humor. We also contribute with a scarce and valuable dataset, which we will release publicly.",
}

@inproceedings{ziser2020,
author = {Ziser, Yftah and Kravi, Elad and Carmel, David},
title = {Humor Detection in Product Question Answering Systems},
year = {2020},
isbn = {9781450380164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397271.3401077},
doi = {10.1145/3397271.3401077},
abstract = {Community question-answering (CQA) has been established as a prominent web service enabling users to post questions and get answers from the community. Product Question Answering (PQA) is a special CQA framework where questions are asked (and are answered) in the context of a specific product. Naturally, humorous questions are integral part of such platforms, especially as some products attract humor due to their unreasonable price, their peculiar functionality, or in cases that users emphasize their critical point-of-view through humor. Detecting humorous questions in such systems is important for sellers, to better understand user engagement with their products. It is also important to signal users about flippancy of humorous questions, and that answers for such questions should be taken with a grain of salt.In this study we present a deep-learning framework for detecting humorous questions in PQA systems. Our framework utilizes two properties of the questions - Incongruity and Subjectivity, demonstrating their contribution for humor detection. We evaluate our framework over a real-world dataset, demonstrating an accuracy of 90.8\%, up to 18.3\% relative improvement over baseline methods. We then demonstrate the existence of product bias in PQA platforms, when some products attract more humorous questions than others. A classifier trained over unbiased data is outperformed by the biased classifier, however, it excels in the task of differentiating between humorous and non-humorous questions that are both related to the same product. To the best of our knowledge this work is the first to detect humor in PQA setting.},
booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {519–528},
numpages = {10},
keywords = {humor detection, product question answering},
location = {Virtual Event, China},
series = {SIGIR '20}
}

@inproceedings{zhang2014recognize,
author = {Zhang, Renxian and Liu, Naishi},
title = {Recognizing Humor on Twitter},
year = {2014},
isbn = {9781450325981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2661829.2661997},
doi = {10.1145/2661829.2661997},
abstract = {In this paper, we present our work of humor recognition on Twitter, which will facilitate affect and sentimental analysis in the social network. The central question of what makes a tweet (Twitter post) humorous drives us to design humor-related features, which are derived from influential humor theories, linguistic norms, and affective dimensions. Using machine learning techniques, we are able to recognize humorous tweets with high accuracy and F-measure. More importantly, we single out features that contribute to distinguishing non-humorous tweets from humorous tweets, and humorous tweets from other short humorous texts (non-tweets). This proves that humorous tweets possess discernible characteristics that are neither found in plain tweets nor in humorous non-tweets. We believe our novel findings will inform and inspire the burgeoning field of computational humor research in the social media.},
booktitle = {Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management},
pages = {889–898},
numpages = {10},
keywords = {twitter, machine learning, humor recognition, computational humor},
location = {Shanghai, China},
series = {CIKM '14}
}

@inproceedings{weller-seppi-2019-humor,
    title = "Humor Detection: A Transformer Gets the Last Laugh",
    author = "Weller, Orion  and
      Seppi, Kevin",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1372",
    doi = "10.18653/v1/D19-1372",
    pages = "3621--3625",
    abstract = "Much previous work has been done in attempting to identify humor in text. In this paper we extend that capability by proposing a new task: assessing whether or not a joke is humorous. We present a novel way of approaching this problem by building a model that learns to identify humorous jokes based on ratings gleaned from Reddit pages, consisting of almost 16,000 labeled instances. Using these ratings to determine the level of humor, we then employ a Transformer architecture for its advantages in learning from sentence context. We demonstrate the effectiveness of this approach and show results that are comparable to human performance. We further demonstrate our model{'}s increased capabilities on humor identification problems, such as the previously created datasets for short jokes and puns. These experiments show that this method outperforms all previous work done on these tasks, with an F-measure of 93.1{\%} for the Puns dataset and 98.6{\%} on the Short Jokes dataset.",
}

@inproceedings{xie-etal-2021-uncertainty,
    title = "Uncertainty and Surprisal Jointly Deliver the Punchline: Exploiting Incongruity-Based Features for Humor Recognition",
    author = "Xie, Yubo  and
      Li, Junze  and
      Pu, Pearl",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-short.6",
    doi = "10.18653/v1/2021.acl-short.6",
    pages = "33--39",
    abstract = "Humor recognition has been widely studied as a text classification problem using data-driven approaches. However, most existing work does not examine the actual joke mechanism to understand humor. We break down any joke into two distinct components: the set-up and the punchline, and further explore the special relationship between them. Inspired by the incongruity theory of humor, we model the set-up as the part developing semantic uncertainty, and the punchline disrupting audience expectations. With increasingly powerful language models, we were able to feed the set-up along with the punchline into the GPT-2 language model, and calculate the uncertainty and surprisal values of the jokes. By conducting experiments on the SemEval 2021 Task 7 dataset, we found that these two features have better capabilities of telling jokes from non-jokes, compared with existing baselines.",
}

@inproceedings{Peyrard2021LaughingHC,
  title={Laughing Heads: Can Transformers Detect What Makes a Sentence Funny?},
  author={Maxime Peyrard and Beatriz Borges and Kristina Gligoric and Robert West},
  booktitle={International Joint Conference on Artificial Intelligence},
  year={2021},
  url={https://api.semanticscholar.org/CorpusID:234778118}
}

@article{Annamoradnejad2020ColBERTUB,
  title={ColBERT: Using BERT sentence embedding in parallel neural networks for computational humor},
  author={Issa Annamoradnejad and Gohar Zoghi},
  journal={Expert Syst. Appl.},
  year={2020},
  volume={249},
  pages={123685},
  url={https://api.semanticscholar.org/CorpusID:254125774}
}

@inproceedings{arora-etal-2022-transfer,
    title = "Transfer Learning for Humor Detection by Twin Masked Yellow {M}uppets",
    author = {Arora, Aseem  and
      Dias, Ga{\"e}l  and
      Jatowt, Adam  and
      Ekbal, Asif},
    editor = "He, Yulan  and
      Ji, Heng  and
      Li, Sujian  and
      Liu, Yang  and
      Chang, Chua-Hui",
    booktitle = "Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)",
    month = nov,
    year = "2022",
    address = "Online only",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.aacl-short.1",
    pages = "1--7",
    abstract = "Humorous texts can be of different forms such as punchlines, puns, or funny stories. Existing humor classification systems have been dealing with such diverse forms by treating them independently. In this paper, we argue that different forms of humor share a common background either in terms of vocabulary or constructs. As a consequence, it is likely that classification performance can be improved by jointly tackling different humor types. Hence, we design a shared-private multitask architecture following a transfer learning paradigm and perform experiments over four gold standard datasets. Empirical results steadily confirm our hypothesis by demonstrating statistically-significant improvements over baselines and accounting for new state-of-the-art figures for two datasets.",
}

@inproceedings{potash-etal-2017-semeval,
    title = "{S}em{E}val-2017 Task 6: {\#}{H}ashtag{W}ars: Learning a Sense of Humor",
    author = "Potash, Peter  and
      Romanov, Alexey  and
      Rumshisky, Anna",
    editor = "Bethard, Steven  and
      Carpuat, Marine  and
      Apidianaki, Marianna  and
      Mohammad, Saif M.  and
      Cer, Daniel  and
      Jurgens, David",
    booktitle = "Proceedings of the 11th International Workshop on Semantic Evaluation ({S}em{E}val-2017)",
    month = aug,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/S17-2004",
    doi = "10.18653/v1/S17-2004",
    pages = "49--57",
    abstract = "This paper describes a new shared task for humor understanding that attempts to eschew the ubiquitous binary approach to humor detection and focus on comparative humor ranking instead. The task is based on a new dataset of funny tweets posted in response to shared hashtags, collected from the {`}Hashtag Wars{'} segment of the TV show @midnight. The results are evaluated in two subtasks that require the participants to generate either the correct pairwise comparisons of tweets (subtask A), or the correct ranking of the tweets (subtask B) in terms of how funny they are. 7 teams participated in subtask A, and 5 teams participated in subtask B. The best accuracy in subtask A was 0.675. The best (lowest) rank edit distance for subtask B was 0.872.",
}

@inproceedings{van-hee-etal-2018-semeval,
    title = "{S}em{E}val-2018 Task 3: Irony Detection in {E}nglish Tweets",
    author = "Van Hee, Cynthia  and
      Lefever, Els  and
      Hoste, V{\'e}ronique",
    editor = "Apidianaki, Marianna  and
      Mohammad, Saif M.  and
      May, Jonathan  and
      Shutova, Ekaterina  and
      Bethard, Steven  and
      Carpuat, Marine",
    booktitle = "Proceedings of the 12th International Workshop on Semantic Evaluation",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/S18-1005",
    doi = "10.18653/v1/S18-1005",
    pages = "39--50",
    abstract = "This paper presents the first shared task on irony detection: given a tweet, automatic natural language processing systems should determine whether the tweet is ironic (Task A) and which type of irony (if any) is expressed (Task B). The ironic tweets were collected using irony-related hashtags (i.e. {\#}irony, {\#}sarcasm, {\#}not) and were subsequently manually annotated to minimise the amount of noise in the corpus. Prior to distributing the data, hashtags that were used to collect the tweets were removed from the corpus. For both tasks, a training corpus of 3,834 tweets was provided, as well as a test set containing 784 tweets. Our shared tasks received submissions from 43 teams for the binary classification Task A and from 31 teams for the multiclass Task B. The highest classification scores obtained for both subtasks are respectively F1= 0.71 and F1= 0.51 and demonstrate that fine-grained irony classification is much more challenging than binary irony detection.",
}

@inproceedings{Castro2018OverviewOT,
  title={Overview of the HAHA Task: Humor Analysis Based on Human Annotation at IberEval 2018},
  author={Santiago Castro and Luis Chiruzzo and Aiala Ros{\'a}},
  booktitle={IberEval@SEPLN},
  year={2018},
  url={https://api.semanticscholar.org/CorpusID:51940157}
}

@inproceedings{hossain-etal-2020-semeval,
    title = "{S}em{E}val-2020 Task 7: Assessing Humor in Edited News Headlines",
    author = "Hossain, Nabil  and
      Krumm, John  and
      Gamon, Michael  and
      Kautz, Henry",
    editor = "Herbelot, Aurelie  and
      Zhu, Xiaodan  and
      Palmer, Alexis  and
      Schneider, Nathan  and
      May, Jonathan  and
      Shutova, Ekaterina",
    booktitle = "Proceedings of the Fourteenth Workshop on Semantic Evaluation",
    month = dec,
    year = "2020",
    address = "Barcelona (online)",
    publisher = "International Committee for Computational Linguistics",
    url = "https://aclanthology.org/2020.semeval-1.98",
    doi = "10.18653/v1/2020.semeval-1.98",
    pages = "746--758",
    abstract = "This paper describes the SemEval-2020 shared task {``}Assessing Humor in Edited News Headlines.{''} The task{'}s dataset contains news headlines in which short edits were applied to make them funny, and the funniness of these edited headlines was rated using crowdsourcing. This task includes two subtasks, the first of which is to estimate the funniness of headlines on a humor scale in the interval 0-3. The second subtask is to predict, for a pair of edited versions of the same original headline, which is the funnier version. To date, this task is the most popular shared computational humor task, attracting 48 teams for the first subtask and 31 teams for the second.",
}

@inproceedings{radev-etal-2016-humor,
    title = "Humor in Collective Discourse: Unsupervised Funniness Detection in the New Yorker Cartoon Caption Contest",
    author = "Radev, Dragomir  and
      Stent, Amanda  and
      Tetreault, Joel  and
      Pappu, Aasish  and
      Iliakopoulou, Aikaterini  and
      Chanfreau, Agustin  and
      de Juan, Paloma  and
      Vallmitjana, Jordi  and
      Jaimes, Alejandro  and
      Jha, Rahul  and
      Mankoff, Robert",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Grobelnik, Marko  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, Helene  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16)",
    month = may,
    year = "2016",
    address = "Portoro{\v{z}}, Slovenia",
    publisher = "European Language Resources Association (ELRA)",
    url = "https://aclanthology.org/L16-1076",
    pages = "475--479",
    abstract = "The New Yorker publishes a weekly captionless cartoon. More than 5,000 readers submit captions for it. The editors select three of them and ask the readers to pick the funniest one. We describe an experiment that compares a dozen automatic methods for selecting the funniest caption. We show that negative sentiment, human-centeredness, and lexical centrality most strongly match the funniest captions, followed by positive sentiment. These results are useful for understanding humor and also in the design of more engaging conversational agents in text and multimodal (vision+text) systems. As part of this work, a large set of cartoons and captions is being made available to the community.",
}

@article{Shahaf2015InsideJI,
  title={Inside Jokes: Identifying Humorous Cartoon Captions},
  author={Dafna Shahaf and Eric Horvitz and Robert Mankoff},
  journal={Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  year={2015},
  url={https://api.semanticscholar.org/CorpusID:14570747}
}

@inproceedings{hasan-etal-2019-ur,
    title = "{UR}-{FUNNY}: A Multimodal Language Dataset for Understanding Humor",
    author = "Hasan, Md Kamrul  and
      Rahman, Wasifur  and
      Bagher Zadeh, AmirAli  and
      Zhong, Jianyuan  and
      Tanveer, Md Iftekhar  and
      Morency, Louis-Philippe  and
      Hoque, Mohammed (Ehsan)",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1211",
    doi = "10.18653/v1/D19-1211",
    pages = "2046--2056",
    abstract = "Humor is a unique and creative communicative behavior often displayed during social interactions. It is produced in a multimodal manner, through the usage of words (text), gestures (visual) and prosodic cues (acoustic). Understanding humor from these three modalities falls within boundaries of multimodal language; a recent research trend in natural language processing that models natural language as it happens in face-to-face communication. Although humor detection is an established research area in NLP, in a multimodal context it has been understudied. This paper presents a diverse multimodal dataset, called UR-FUNNY, to open the door to understanding multimodal language used in expressing humor. The dataset and accompanying studies, present a framework in multimodal humor detection for the natural language processing community. UR-FUNNY is publicly available for research.",
}

@inproceedings{bertero-fung-2016-deep,
    title = "Deep Learning of Audio and Language Features for Humor Prediction",
    author = "Bertero, Dario  and
      Fung, Pascale",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Grobelnik, Marko  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, Helene  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16)",
    month = may,
    year = "2016",
    address = "Portoro{\v{z}}, Slovenia",
    publisher = "European Language Resources Association (ELRA)",
    url = "https://aclanthology.org/L16-1079",
    pages = "496--501",
    abstract = "We propose a comparison between various supervised machine learning methods to predict and detect humor in dialogues. We retrieve our humorous dialogues from a very popular TV sitcom: {``}The Big Bang Theory{''}. We build a corpus where punchlines are annotated using the canned laughter embedded in the audio track. Our comparative study involves a linear-chain Conditional Random Field over a Recurrent Neural Network and a Convolutional Neural Network. Using a combination of word-level and audio frame-level features, the CNN outperforms the other methods, obtaining the best F-score of 68.5{\%} over 66.5{\%} by CRF and 52.9{\%} by RNN. Our work is a starting point to developing more effective machine learning and neural network models on the humor prediction task, as well as developing machines capable in understanding humor in general.",
}

@inproceedings{buscaldi2007some,
  title={Some experiments in humour recognition using the italian wikiquote collection},
  author={Buscaldi, Davide and Rosso, Paolo},
  booktitle={International workshop on fuzzy logic and applications},
  pages={464--468},
  year={2007},
  organization={Springer}
}

@article{castro2017crowd,
  title={A crowd-annotated spanish corpus for humor analysis},
  author={Castro, Santiago and Chiruzzo, Luis and Ros{\'a}, Aiala and Garat, Diego and Moncecchi, Guillermo},
  journal={arXiv preprint arXiv:1710.00477},
  year={2017}
}

@article{winters2020dutch,
  title={Dutch humor detection by generating negative examples},
  author={Winters, Thomas and Delobelle, Pieter},
  journal={arXiv preprint arXiv:2010.13652},
  year={2020}
}

@inproceedings{blinov2019large,
  title={Large dataset and language model fun-tuning for humor recognition},
  author={Blinov, Vladislav and Bolotova-Baranova, Valeria and Braslavski, Pavel},
  booktitle={Proceedings of the 57th annual meeting of the association for computational linguistics},
  pages={4027--4032},
  year={2019}
}

@article{kocon2023chatgpt,
  title={ChatGPT: Jack of all trades, master of none},
  author={Koco{\'n}, Jan and Cichecki, Igor and Kaszyca, Oliwier and Kochanek, Mateusz and Szyd{\l}o, Dominika and Baran, Joanna and Bielaniewicz, Julita and Gruza, Marcin and Janz, Arkadiusz and Kanclerz, Kamil and others},
  journal={Information Fusion},
  volume={99},
  pages={101861},
  year={2023},
  publisher={Elsevier}
}

@misc{borji2023categoricalarchivechatgptfailures,
      title={A Categorical Archive of ChatGPT Failures}, 
      author={Ali Borji},
      year={2023},
      eprint={2302.03494},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2302.03494}, 
}

@inproceedings{baranov-etal-2023-told,
    title = "You Told Me That Joke Twice: A Systematic Investigation of Transferability and Robustness of Humor Detection Models",
    author = "Baranov, Alexander  and
      Kniazhevsky, Vladimir  and
      Braslavski, Pavel",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.845",
    doi = "10.18653/v1/2023.emnlp-main.845",
    pages = "13701--13715",
    abstract = {In this study, we focus on automatic humor detection, a highly relevant task for conversational AI. To date, there are several English datasets for this task, but little research on how models trained on them generalize and behave in the wild. To fill this gap, we carefully analyze existing datasets, train RoBERTa-based and Na{\"\i}ve Bayes classifiers on each of them, and test on the rest. Training and testing on the same dataset yields good results, but the transferability of the models varies widely. Models trained on datasets with jokes from different sources show better transferability, while the amount of training data has a smaller impact. The behavior of the models on out-of-domain data is unstable, suggesting that some of the models overfit, while others learn non-specific humor characteristics. An adversarial attack shows that models trained on pun datasets are less robust. We also evaluate the sense of humor of the chatGPT and Flan-UL2 models in a zero-shot scenario. The LLMs demonstrate competitive results on humor datasets and a more stable behavior on out-of-domain data. We believe that the obtained results will facilitate the development of new datasets and evaluation methodologies in the field of computational humor. We{'}ve made all the data from the study and the trained models publicly available at https://github.com/Humor-Research/Humor-detection.},
}

@inproceedings{mihalcea-strapparava-2005-making,
    title = "Making Computers Laugh: Investigations in Automatic Humor Recognition",
    author = "Mihalcea, Rada  and
      Strapparava, Carlo",
    editor = "Mooney, Raymond  and
      Brew, Chris  and
      Chien, Lee-Feng  and
      Kirchhoff, Katrin",
    booktitle = "Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing",
    month = oct,
    year = "2005",
    address = "Vancouver, British Columbia, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/H05-1067",
    pages = "531--538",
}

@misc{tang2022naughtyformertransformerunderstandsoffensive,
      title={The Naughtyformer: A Transformer Understands Offensive Humor}, 
      author={Leonard Tang and Alexander Cai and Steve Li and Jason Wang},
      year={2022},
      eprint={2211.14369},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2211.14369}, 
}

@inproceedings{hossain-etal-2020-stimulating,
    title = "Stimulating Creativity with {F}un{L}ines: A Case Study of Humor Generation in Headlines",
    author = "Hossain, Nabil  and
      Krumm, John  and
      Sajed, Tanvir  and
      Kautz, Henry",
    editor = "Celikyilmaz, Asli  and
      Wen, Tsung-Hsien",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-demos.28",
    doi = "10.18653/v1/2020.acl-demos.28",
    pages = "256--262",
    abstract = "Building datasets of creative text, such as humor, is quite challenging. We introduce FunLines, a competitive game where players edit news headlines to make them funny, and where they rate the funniness of headlines edited by others. FunLines makes the humor generation process fun, interactive, collaborative, rewarding and educational, keeping players engaged and providing humor data at a very low cost compared to traditional crowdsourcing approaches. FunLines offers useful performance feedback, assisting players in getting better over time at generating and assessing humor, as our analysis shows. This helps to further increase the quality of the generated dataset. We show the effectiveness of this data by training humor classification models that outperform a previous benchmark, and we release this dataset to the public.",
}

@article{Xie2023FunQATS,
  title={FunQA: Towards Surprising Video Comprehension},
  author={Binzhu Xie and Sicheng Zhang and Zitang Zhou and Bo Li and Yuanhan Zhang and Jack Hessel and Jingkang Yang and Ziwei Liu},
  journal={ArXiv},
  year={2023},
  volume={abs/2306.14899},
  url={https://api.semanticscholar.org/CorpusID:259262297}
}

@article{Zhong2023LetsTO,
  title={Let's Think Outside the Box: Exploring Leap-of-Thought in Large Language Models with Creative Humor Generation},
  author={Shan Zhong and Zhongzhan Huang and Shanghua Gao and Wushao Wen and Liang Lin and Marinka Zitnik and Pan Zhou},
  journal={2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2023},
  pages={13246-13257},
  url={https://api.semanticscholar.org/CorpusID:265659076}
}

@inproceedings{GesIsGG2023,
  title={Is GPT-4 Good Enough to Evaluate Jokes?},
  author={Fabr{\'i}cio G{\'o}es and Piotr Sawicki and Marek Grze´s and Daniel Brown and Marco Volpe},
  url={https://api.semanticscholar.org/CorpusID:265255804},
  year={2023}
}

@inproceedings{Swanson2021StoryCL,
  title={Story Centaur: Large Language Model Few Shot Learning as a Creative Writing Tool},
  author={Benjamin Swanson and Kory Wallace Mathewson and Ben Pietrzak and Sherol Chen and Monica Dinalescu},
  booktitle={Conference of the European Chapter of the Association for Computational Linguistics},
  year={2021},
  url={https://api.semanticscholar.org/CorpusID:233365238}
}

@inproceedings{Maher2010EvaluatingCI,
  title={Evaluating creativity in humans, computers, and collectively intelligent systems},
  author={Mary Lou Maher},
  booktitle={Network Conference on Creativity and Innovation in Design},
  year={2010},
  url={https://api.semanticscholar.org/CorpusID:4840039}
}

@inproceedings{Gunkle1975AestheticsAP,
  title={Aesthetics and Psychobiology},
  author={George Gunkle and Daniel E. Berlyne},
  year={1975},
  url={https://api.semanticscholar.org/CorpusID:144314694}
}

@article{Elgammal2015QuantifyingCI,
  title={Quantifying Creativity in Art Networks},
  author={A. Elgammal and Babak Saleh},
  journal={ArXiv},
  year={2015},
  volume={abs/1506.00711},
  url={https://api.semanticscholar.org/CorpusID:14607648}
}

@article{Gatt2017SurveyOT,
  title={Survey of the State of the Art in Natural Language Generation: Core tasks, applications and evaluation},
  author={Albert Gatt and Emiel J. Krahmer},
  journal={ArXiv},
  year={2017},
  volume={abs/1703.09902},
  url={https://api.semanticscholar.org/CorpusID:16946362}
}

@inproceedings{zhao-etal-2023-discoscore,
    title = "{D}isco{S}core: Evaluating Text Generation with {BERT} and Discourse Coherence",
    author = "Zhao, Wei  and
      Strube, Michael  and
      Eger, Steffen",
    editor = "Vlachos, Andreas  and
      Augenstein, Isabelle",
    booktitle = "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics",
    month = may,
    year = "2023",
    address = "Dubrovnik, Croatia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.eacl-main.278",
    doi = "10.18653/v1/2023.eacl-main.278",
    pages = "3865--3883",
}

@article{Laban2021CanTM,
  title={Can Transformer Models Measure Coherence In Text: Re-Thinking the Shuffle Test},
  author={Philippe Laban and Luke Dai and Lucas Bandarkar},
  journal={ArXiv},
  year={2021},
  volume={abs/2107.03448},
  url={https://api.semanticscholar.org/CorpusID:235656966}
}

@article{wisniewski1997concepts,
  title={When concepts combine},
  author={Wisniewski, Edward J},
  journal={Psychonomic bulletin \& review},
  volume={4},
  pages={167--183},
  year={1997},
  publisher={Springer}
}

@article{costello2000efficient,
  title={Efficient creativity: Constraint-guided conceptual combination},
  author={Costello, Fintan J and Keane, Mark T},
  journal={Cognitive Science},
  volume={24},
  number={2},
  pages={299--349},
  year={2000},
  publisher={Wiley Online Library}
}

@misc{ismayilzada2024evaluatingmorphologicalcompositionalgeneralization,
      title={Evaluating Morphological Compositional Generalization in Large Language Models}, 
      author={Mete Ismayilzada and Defne Circi and Jonne Sälevä and Hale Sirin and Abdullatif Köksal and Bhuwan Dhingra and Antoine Bosselut and Lonneke van der Plas and Duygu Ataman},
      year={2024},
      eprint={2410.12656},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.12656}, 
}

@inproceedings{anh-etal-2024-morphology,
    title = "Morphology Matters: Probing the Cross-linguistic Morphological Generalization Abilities of Large Language Models through a Wug Test",
    author = "Anh, Dang  and
      Raviv, Limor  and
      Galke, Lukas",
    editor = "Kuribayashi, Tatsuki  and
      Rambelli, Giulia  and
      Takmaz, Ece  and
      Wicke, Philipp  and
      Oseki, Yohei",
    booktitle = "Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.cmcl-1.15",
    doi = "10.18653/v1/2024.cmcl-1.15",
    pages = "177--188",
    abstract = "We develop a multilingual version of the Wug Test, an artificial word completion experiment that is typically used to test the morphological knowledge of children, and apply it to the GPT family of large language models (LLMs). LLMs{'} performance on this test was evaluated by native speakers of six different languages, who judged whether the inflected and derived forms generated by the models conform to the morphological rules of their language. Our results show that LLMs can generalize their morphological knowledge to new, unfamiliar words, but that their success in generating the {``}correct{''} generalization (as judged by native human speakers) is predicted by a language{'}s morphological complexity (specifically, integrative complexity). We further find that the amount of training data has surprisingly little on LLMs{'} morphological generalization abilities within the scope of the analyzed languages. These findings highlight that {``}morphology matters{''}, and have important implications for improving low-resource language modeling.",
}

@misc{lu2024aihumanityssalieriquantifying,
      title={AI as Humanity's Salieri: Quantifying Linguistic Creativity of Language Models via Systematic Attribution of Machine Text against Web Text}, 
      author={Ximing Lu and Melanie Sclar and Skyler Hallinan and Niloofar Mireshghallah and Jiacheng Liu and Seungju Han and Allyson Ettinger and Liwei Jiang and Khyathi Chandu and Nouha Dziri and Yejin Choi},
      year={2024},
      eprint={2410.04265},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.04265}, 
}

@misc{demis-creativity2018,
    title={Creativity and AI},
    year={2018}, 
    author={Demis Hassabis},
    publisher={The Rothschild Foundation Lecture - Royal Academy of Arts},
    url={https://www.youtube.com/watch?v=d-bvsJWmqlc&ab_channel=RoyalAcademyofArts}
}

@inproceedings{Frana2016RegentDependentCA,
  title={Regent-Dependent Creativity: A Domain Independent Metric for the Assessment of Creative Artifacts},
  author={Celso França and Lu{\'i}s Fabr{\'i}cio Wanderley G{\'o}es and Alvaro Amorim and Rodrigo C. O. Rocha and Alysson Ribeiro Da Silva},
  booktitle={International Conference on Innovative Computing and Cloud Computing},
  year={2016},
  url={https://api.semanticscholar.org/CorpusID:7817742}
}

@inproceedings{Lamb2015HumanCI,
  title={Human Competence in Creativity Evaluation},
  author={Carolyn Lamb and Daniel G. Brown and Charles L. A. Clarke},
  booktitle={International Conference on Innovative Computing and Cloud Computing},
  year={2015},
  url={https://api.semanticscholar.org/CorpusID:14806090}
}

@inproceedings{cook2015generating,
  title={Generating Code For Expressing Simple Preferences: Moving On From Hardcoding And Randomness.},
  author={Cook, Michael and Colton, Simon},
  booktitle={ICCC},
  pages={8--16},
  year={2015}
}

@article{jordanous2015measuring,
  title={Measuring cultural value using social network analysis: a case study on valuing electronic musicians},
  author={Jordanous, Anna and Allington, Daniel and Dueck, Byron},
  year={2015},
  publisher={Citeseer}
}

@inproceedings{maher2012using,
  title={Using AI to evaluate creative designs},
  author={Maher, Mary Lou and Fisher, Douglas H},
  booktitle={DS 73-1 Proceedings of the 2nd International Conference on Design Creativity Volume 1},
  year={2012}
}

@misc{saphra2024mechanistic,
      title={Mechanistic?}, 
      author={Naomi Saphra and Sarah Wiegreffe},
      year={2024},
      eprint={2410.09087},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2410.09087}, 
}

@article{Bereska2024MechanisticIF,
  title={Mechanistic Interpretability for AI Safety - A Review},
  author={Leonard Bereska and Efstratios Gavves},
  journal={ArXiv},
  year={2024},
  volume={abs/2404.14082},
  url={https://api.semanticscholar.org/CorpusID:269293418}
}

@article{Marco2024PronVP,
  title={Pron vs Prompt: Can Large Language Models already Challenge a World-Class Fiction Author at Creative Text Writing?},
  author={Guillermo Marco and Julio Gonzalo and Ram'on del Castillo and Mar'ia Teresa Mateo Girona},
  journal={ArXiv},
  year={2024},
  volume={abs/2407.01119},
  url={https://api.semanticscholar.org/CorpusID:270870769}
}

@article{Lu2024BenchmarkingLM,
  title={Benchmarking Language Model Creativity: A Case Study on Code Generation},
  author={Yining Lu and Dixuan Wang and Tianjian Li and Dongwei Jiang and Daniel Khashabi},
  journal={ArXiv},
  year={2024},
  volume={abs/2407.09007},
  url={https://api.semanticscholar.org/CorpusID:271162279}
}

@article{Mehrotra2024EnhancingCI,
  title={Enhancing Creativity in Large Language Models through Associative Thinking Strategies},
  author={Pronita Mehrotra and Aishni Parab and Sumit Gulwani},
  journal={ArXiv},
  year={2024},
  volume={abs/2405.06715},
  url={https://api.semanticscholar.org/CorpusID:269756853}
}

@article{Tian2024AreLL,
  title={Are Large Language Models Capable of Generating Human-Level Narratives?},
  author={Yufei Tian and Tenghao Huang and Miri Liu and Derek Jiang and Alexander Spangher and Muhao Chen and Jonathan May and Nanyun Peng},
  journal={ArXiv},
  year={2024},
  volume={abs/2407.13248},
  url={https://api.semanticscholar.org/CorpusID:271270813}
}

@article{Chhun2024DoLM,
  title={Do Language Models Enjoy Their Own Stories? Prompting Large Language Models for Automatic Story Evaluation},
  author={Cyril Chhun and Fabian M. Suchanek and Chlo{\'e} Clavel},
  journal={Transactions of the Association for Computational Linguistics},
  year={2024},
  volume={12},
  pages={1122-1142},
  url={https://api.semanticscholar.org/CorpusID:269982125}
}

@article{yuan2024chatmusician,
  title={Chatmusician: Understanding and generating music intrinsically with llm},
  author={Yuan, Ruibin and Lin, Hanfeng and Wang, Yi and Tian, Zeyue and Wu, Shangda and Shen, Tianhao and Zhang, Ge and Wu, Yuhang and Liu, Cong and Zhou, Ziya and others},
  journal={arXiv preprint arXiv:2402.16153},
  year={2024}
}

@article{deng2024composerx,
  title={ComposerX: Multi-Agent Symbolic Music Composition with LLMs},
  author={Deng, Qixin and Yang, Qikai and Yuan, Ruibin and Huang, Yipeng and Wang, Yi and Liu, Xubo and Tian, Zeyue and Pan, Jiahao and Zhang, Ge and Lin, Hanfeng and others},
  journal={arXiv preprint arXiv:2404.18081},
  year={2024}
}

@article{qu2024mupt,
  title={Mupt: A generative symbolic music pretrained transformer},
  author={Qu, Xingwei and Bai, Yuelin and Ma, Yinghao and Zhou, Ziya and Lo, Ka Man and Liu, Jiaheng and Yuan, Ruibin and Min, Lejun and Liu, Xueling and Zhang, Tianyu and others},
  journal={arXiv preprint arXiv:2404.06393},
  year={2024}
}

@article{peeperkorn2023characterizations,
  title={On characterizations of large language models and creativity evaluation},
  author={Peeperkorn, Max and Brown, Dan and Jordanous, Anna},
  year={2023},
  publisher={Association for Computational Creativity}
}

@inproceedings{gomez-rodriguez-williams-2023-confederacy,
    title = "A Confederacy of Models: a Comprehensive Evaluation of {LLM}s on Creative Writing",
    author = "G{\'o}mez-Rodr{\'\i}guez, Carlos  and
      Williams, Paul",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.966",
    doi = "10.18653/v1/2023.findings-emnlp.966",
    pages = "14504--14528",
    abstract = "We evaluate a range of recent LLMs on English creative writing, a challenging and complex task that requires imagination, coherence, and style. We use a difficult, open-ended scenario chosen to avoid training data reuse: an epic narration of a single combat between Ignatius J. Reilly, the protagonist of the Pulitzer Prize-winning novel A Confederacy of Dunces (1980), and a pterodactyl, a prehistoric flying reptile. We ask several LLMs and humans to write such a story and conduct a human evalution involving various criteria such as fluency, coherence, originality, humor, and style. Our results show that some state-of-the-art commercial LLMs match or slightly outperform our writers in most dimensions; whereas open-source LLMs lag behind. Humans retain an edge in creativity, while humor shows a binary divide between LLMs that can handle it comparably to humans and those that fail at it. We discuss the implications and limitations of our study and suggest directions for future research.",
}

@inproceedings{xie-etal-2023-next,
    title = "The Next Chapter: A Study of Large Language Models in Storytelling",
    author = "Xie, Zhuohan  and
      Cohn, Trevor  and
      Lau, Jey Han",
    editor = "Keet, C. Maria  and
      Lee, Hung-Yi  and
      Zarrie{\ss}, Sina",
    booktitle = "Proceedings of the 16th International Natural Language Generation Conference",
    month = sep,
    year = "2023",
    address = "Prague, Czechia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.inlg-main.23",
    doi = "10.18653/v1/2023.inlg-main.23",
    pages = "323--351",
    abstract = "To enhance the quality of generated stories, recent story generation models have been investigating the utilization of higher-level attributes like plots or commonsense knowledge. The application of prompt-based learning with large language models (LLMs), exemplified by GPT-3, has exhibited remarkable performance in diverse natural language processing (NLP) tasks. This paper conducts a comprehensive investigation, utilizing both automatic and human evaluation, to compare the story generation capacity of LLMs with recent models across three datasets with variations in style, register, and length of stories. The results demonstrate that LLMs generate stories of significantly higher quality compared to other story generation models. Moreover, they exhibit a level of performance that competes with human authors, albeit with the preliminary observation that they tend to replicate real stories in situations involving world knowledge, resembling a form of plagiarism.",
}

@inproceedings{huang-etal-2024-lateval,
    title = "{L}at{E}val: An Interactive {LLM}s Evaluation Benchmark with Incomplete Information from Lateral Thinking Puzzles",
    author = "Huang, Shulin  and
      Ma, Shirong  and
      Li, Yinghui  and
      Huang, Mengzuo  and
      Zou, Wuhe  and
      Zhang, Weidong  and
      Zheng, Haitao",
    editor = "Calzolari, Nicoletta  and
      Kan, Min-Yen  and
      Hoste, Veronique  and
      Lenci, Alessandro  and
      Sakti, Sakriani  and
      Xue, Nianwen",
    booktitle = "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",
    month = may,
    year = "2024",
    address = "Torino, Italia",
    publisher = "ELRA and ICCL",
    url = "https://aclanthology.org/2024.lrec-main.889",
    pages = "10186--10197",
    abstract = "With the evolution of LLMs, they are endowed with impressive logical reasoning, or vertical thinking capabilities. But can they think out of the box? Do they possess proficient lateral thinking abilities? Following the setup of Lateral Thinking Puzzles, we propose a novel evaluation benchmark, LatEval, which assesses the model{'}s lateral thinking within an interactive framework. In our benchmark, we challenge LLMs with 2 aspects: (1) posing high-quality questions that break out of conventional norms but are beneficial for puzzle-solving. (2) integrating existing information to gradually deduce the truth through reasoning. We observe that it is hard for most LLMs to accomplish lateral thinking during interactions. Even the most powerful LLM, GPT-4, faces challenges in achieving satisfactory performance, and for most open-source models, simply completing this task is quite difficult. This evaluation benchmark provides LLMs with a highly challenging and differentiating task that is crucial to an effective AI assistant. Our dataset and source codes are available at https://github.com/THUKElab/LatEval.",
}

@article{yang2024makes,
  title={What makes a good story and how can we measure it? a comprehensive survey of story evaluation},
  author={Yang, Dingyi and Jin, Qin},
  journal={arXiv preprint arXiv:2408.14622},
  year={2024}
}

@misc{ismayilzada2024evaluatingcreativeshortstory,
      title={Evaluating Creative Short Story Generation in Humans and Large Language Models}, 
      author={Mete Ismayilzada and Claire Stevenson and Lonneke van der Plas},
      year={2024},
      eprint={2411.02316},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2411.02316}, 
}

@article{haase2023artificial,
  title={Artificial muses: Generative artificial intelligence chatbots have risen to human-level creativity},
  author={Haase, Jennifer and Hanel, Paul HP},
  journal={Journal of Creativity},
  volume={33},
  number={3},
  pages={100066},
  year={2023},
  publisher={Elsevier}
}

@article{cropley2023artificial,
  title={Is artificial intelligence more creative than humans?: ChatGPT and the divergent association task},
  author={Cropley, David},
  journal={Learning Letters},
  volume={2},
  pages={13--13},
  year={2023}
}

@article{castelo2024ai,
  title={How AI Outperforms Humans at Creative Idea Generation},
  author={Castelo, Noah and Katona, Zsolt and Li, Peiyao and Sarvary, Miklos},
  journal={Available at SSRN 4751779},
  year={2024}
}

@article{bellemare2024divergent,
  title={Divergent Creativity in Humans and Large Language Models},
  author={Bellemare-Pepin, Antoine and Lespinasse, Fran{\c{c}}ois and Th{\"o}lke, Philipp and Harel, Yann and Mathewson, Kory and Olson, Jay A and Bengio, Yoshua and Jerbi, Karim},
  journal={arXiv preprint arXiv:2405.13012},
  year={2024}
}